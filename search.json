[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/flaskHW/index.html",
    "href": "posts/flaskHW/index.html",
    "title": "Homework 3: Flask",
    "section": "",
    "text": "https://github.com/jakebrowning02/PICHHW3 https://jakebrowning02.github.io/PIC16BHW/posts/flaskHW/\nWe will start the tutorial with the init.py file. The first function we see in this file is create_app(). The function effectively the creation of the app. As we can see, in the first line, we will set the app to be an instance of the Flask class that we imported at the top of the page. Next we begin to configure the app by defining where the database is to be located, by mapping the database to the ‘flaskBlog.sqlite’ file. If you are wishing to use a different file name for your database, that is fine, but just be sure to use that file name here instead of the one that I used. It is important to note, that for this we must import os at the top of the page. The function os.path.join shows the computer how to reach the file we have specified. Continuing downward, we see a hello() function. This function is not critical to this project, but is useful in checking that the app is working correctly and also happens to be an easier wsay to explain how these functions work. Above the function we see the command ‘@app.route(’/hello’)’ which is critical to this function. This ties the url route ‘/hello’ to this function, a practice we will use pervasively throughout this project. This page simply prints out ‘Hello, World!’, but other pages will have more complex applications. The following commands are all imports that we will revisit later after we have defined the functions that they are referring to. Let’s move over to the db.py file. This file is responsible for defining most of the basic commands necessary for setting up our database. The first thing we want to do is to import sqlite3. This allows us to make an sqlite3 database on this page. We will also import click for a later function, but more importantly, we import both current_app and g from the flask library. g is a special object that is able to be accessed by multiple functions, which is also unique for each request. Similarly, current_app is another special object that points to the Flask application handling the request. We will ensure that current_app will be able to be used by importing this file into the other .py files. The first function we see is get_db(). We first want to check that there is no database currently in our special instance g. If there is not, logically we want to begin to access the database. We define the database connection by creating an attribute db of g, and setting it to sqlite3.connect(), effectively making it a cursor we can use to search the database in later functions. We then return this attribute so that when we import it into other files, we can effectively use this as our cursor. The next function close_db() is rather self explanatory. We need a way to close the connection to the database if the database is open. We check if it is open by writing the if statement ‘if db is not None:’. This way, we only attempt to close the connection if it exists, which we do within the if statement by calling db.close(). We now come across our function that is called when we are intiating the database, init_db(). Using the function we just defined, we call get_db() to create a cursor, and using that we open the schematic that we write for the database, defining the tables, rows, columns, and titles to be held within the database. The next function gives us a way to call the function we just created. We call @click.command(‘init-db’) so that we can call the function using a command line command. Now, when we call this function via the command, we can see that it simply calls the init_db() function and recieve a message letting us know that we have successfully initialized the database. Finally, we come to the init_app() function. This function will be used to ensure that the cursor is closed when it needs to be. In order to ensure that the cursor is closed each time after returning a response, we will use the function teardown_appcontext(). This calls whatever function is within the parentheses whenever a response is returned, so we choose to call close_db() whenever a response is returned. Finally, we want to travel back to init.py and look at what we imported from this page. By importing db.init_app(app), we allow ourselves to initiate the database within the init.py file. Let us now take a look at our last .py file, blog.py. This file will contain most of what we see on the web app. We will import several useful methods from flask up top to be able to run our code. Taking a look at the first function index(), we see it appears to have a similar formate to the hello() function we saw before. Using the @bp.route(‘/’), we define the URL extension to be the backslash to access this page. First thing we do in this function is define db to be the cursor we get from the get_db() function we created in the previous page. Now, we will use this cursor to access the database. We now select both the author and body attributes from every post in the database, and set this list to be the list posts. This will be used in the html to show all of the posts that are contained within our database. This is useful for debugging and being able to view the database in a user friendly manner. Now we reach the return line of this function by calling render_template() we tell the computer to run the html file template we have created for this page called ‘index.html’. In my case, this file is contained within a blog folder, hence the ‘/blog’ before the ‘index.html’. We also choose to pass the posts list to the template as it is used to display the posts on this page. Continuing on, we get to the function used to insert new messages into the database. We see the function is associated with the URL extension ‘/submit’, however this time we also define the methods(‘GET’, ‘POST’), as any user request will be considered a ‘POST’. We check to make sure that this is the case via the if statement at the beginning of the function. Now we need to get input from the user so that we can input their name and their message. We do so by using the request.form[] function, allowing us to use the input to define the variables. We then check that the boxes have been filled, and send an error if they have not been. If everything is working well, we then use the get_db() function to access the cursor again and use it to insert the new message with its author into the database. We ensure to call db.commit() to save the new information into the database and call for a redirect. This function sends us back to the main page by calling the index function in the blog.py file, or in other words the function above this one. We also make sure to render the template for this page so that the html file we write will appear on this page. The random_messages function again uses the cursor we acquire from get_db(). We then use a special command for the cursor ‘RANDOM()’ to select the rows from the database in a random order. We use the variable n to determine how many rows are to be selected, and then set this to the list messages to be returned by this function. The view function is associated with its own URL and is the page used to determine how many random posts the user withes to see. We use request.form.get() to take in the user input. Then again, assuming no errors, we call random_messages function with the user inputted amount of function and pass it to our final function randomview(). This final function is associated with its own URL and gets random messages from the random_messages function. We call int(request.args.get(‘num_messages’)) to get the amount of messages desired by the user, a value passed by the previous page. Finally, we will take a look at our template for the new post page. First, we extend ‘base.html’ so that we can keep what we coded in that. We now define our header under the {%block header%} and set the block title to ‘New Messages’. Next, we get to the block content. We create a label titled ‘Author’ which is used for our input. This title is placed with the input text box that sends its input to the value request.form[‘author’] which we use to input the value into the database. We then do the same for the message, however this time we define a textarea to create a bigger box for the user to input their message. Finally, we create a submit button that says ‘Submit message’ for the user to click after they input their desired information."
  },
  {
    "objectID": "posts/HW6/index.html",
    "href": "posts/HW6/index.html",
    "title": "HW 6: Fake News Detector",
    "section": "",
    "text": "Today we will be be creating a fake news classifier utilizing Keras. We will assess several different models, evaluating each to determine which method yields the highest accuracy rate."
  },
  {
    "objectID": "posts/HW6/index.html#introduction",
    "href": "posts/HW6/index.html#introduction",
    "title": "HW 6: Fake News Detector",
    "section": "",
    "text": "Today we will be be creating a fake news classifier utilizing Keras. We will assess several different models, evaluating each to determine which method yields the highest accuracy rate."
  },
  {
    "objectID": "posts/HW6/index.html#imports",
    "href": "posts/HW6/index.html#imports",
    "title": "HW 6: Fake News Detector",
    "section": "Imports",
    "text": "Imports\n\n!pip install keras --upgrade\n\nRequirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (3.0.5)\nRequirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from keras) (1.4.0)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from keras) (1.25.2)\nRequirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras) (13.7.1)\nRequirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras) (0.0.7)\nRequirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from keras) (3.9.0)\nRequirement already satisfied: dm-tree in /usr/local/lib/python3.10/dist-packages (from keras) (0.1.8)\nRequirement already satisfied: ml-dtypes in /usr/local/lib/python3.10/dist-packages (from keras) (0.2.0)\nRequirement already satisfied: markdown-it-py&gt;=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich-&gt;keras) (3.0.0)\nRequirement already satisfied: pygments&lt;3.0.0,&gt;=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich-&gt;keras) (2.16.1)\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py&gt;=2.2.0-&gt;rich-&gt;keras) (0.1.2)\n\n\nHere we are insalling an upgrade to keras to ensure we are using keras3.\n\nimport os\nimport re\nimport string\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.feature_extraction import text\n\nos.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n\nWe will start with some of the imports we will need for this project, such as pandas, tensorflow, numpy, etc. One import that we do not normally see is text from sklearn.feature_extraction. This library will help us to remove stop words from our articles. Note that we have set the backend of keras to be tensorflow using the os library. This tries to ensure that keras runs as fast as we would like it to.\n\nimport keras\nfrom keras import layers, losses\nfrom keras import utils\nimport tensorflow as tf\n\n\nprint(keras.__version__)\n\n3.0.5\n\n\nNow that we have set the backend of keras to be tensorflow, we can import keras itself, as well as layers and losses from keras that will be used within our models.We also check our version of keras to make sure we are using the third version."
  },
  {
    "objectID": "posts/HW6/index.html#data-configuration",
    "href": "posts/HW6/index.html#data-configuration",
    "title": "HW 6: Fake News Detector",
    "section": "Data Configuration",
    "text": "Data Configuration\n\n# setting train_url to our dataset\ntrain_url = \"https://github.com/PhilChodrow/PIC16b/blob/master/datasets/fake_news_train.csv?raw=true\"\n# reading in dataset\ntrain_data = pd.read_csv(train_url)\n\nHere we are reading in data from a github site. We use the read_csv function from pandas to do so, passing in the url for the site with the data.\n\ndef make_dataset(df):\n  '''\n  Function takes in a dataframe, formats the information within, and returns a tf.data.Dataframe\n  '''\n  # make text lowercase\n  df['title'] = df['title'].apply(lambda x: x.lower())\n  df['text'] = df['text'].apply(lambda x: x.lower())\n\n\n  # create list with stopwords\n  stop = text.ENGLISH_STOP_WORDS\n\n  # remove stopwords from title and text columns\n  df['title'] = df['title'].apply(lambda x:' '.join([word for word in x.split() if word not in (stop)]))\n  df['text'] = df['text'].apply(lambda x:' '.join([word for word in x.split() if word not in (stop)]))\n\n  # construct tf.data.Dataset with inputs of form (title, text) and output being fake column\n  retDataset = tf.data.Dataset.from_tensor_slices(({'title': df[['title']], 'text':df[['text']]}, {'fake': df[['fake']]} ))\n\n  # batch dataset before returning\n  retDataset = retDataset.batch(100)\n\n  return retDataset\n\nThis function will take in a pandas dataframe, do some data processing, and return a tf.data.Dataset. For our data processing, the first thing we want to do is make all of our text and titles lowercase. We accomplish this by utilizing the lower function, however we need to apply this to the entire ‘text’ and ‘title’ columns within our dataframe. By calling the apply attribute of our columns, we can then use a lambda function that takes each input and calls the lower function, effectively making all of the article titles and text lower case. The next step is to remove all stopwords from our title and text columns. We first want to get a list of all the words we wish to remove, which can be accomplished by using the text library we imported. This library has an attribute ‘ENGLISH_STOP_WORDS’, which returns a list of, as you can probably guess, all english stop words. We set this equal to stop, to allow for easier use throughout the rest of the function. We will use a similar method for removing stopwords as we did for lowering all of our text and titles. Again, we call the apply function for both columns, with the only difference this time being within the lambda function. The stop words are removed within this lambda function by first joining all words in the lambda input into a single string via the join function. Note that the words will be separated by spaces due to the space placed in single quotation marks before join is called. The second part of the lambda function creates a list of words from the string where words are only included if they are not in our list stop, effectively removing all stopwords. Finally, we are ready to create the dataset that we will be returning. We create such a dataset using the tf.data.Dataset.from_tensor_slices command, passing in both our inputs and our outputs. For our dataset, we want inputs of both title and text while having a single output of the fake column. To accomplish this, our first argument enclosed in curly braces, which will be taken in as our inputs, includes both the ‘title’ and ‘text’ columns from our dataframe, while the second set of curly braces, our output, only contains the ‘fake’ column from our dataframe. Our dataframe has been created, but for training purposes it is useful to batch it so that we do not train over the entire set each epoch. I chose 100 for the batch amount, and after batching we can simply return our newly created dataframe, concluding the implementation of this function.\n\n# creating our primary dataset from our dataframe\nprimaryDataset = make_dataset(train_data)\n\nAfter doing all the work initializing the previous function, we can now utilize it to create our dataset by passing in the dataframe we read in earlier to the function, and initializing it as our primaryDataset.\n\n# assigning training and validation size\nval_size   = int(0.2*len(primaryDataset))\n\n# assigning portions of datset to train and val\ntrain = primaryDataset.skip(val_size)\nval   = primaryDataset.take(val_size)\n\nWe would like a validation dataset for training purposes, so we will set aside a portion of our dataset as a validation set. In this case, we chose to set aside twenty percent of our dataset for validation. In order to implement this, we first have to set the desired size for our validation. As we want twenty percent of our datset, we set val_size to 0.2 times the length of our datset. Note that we cast it as an int, allowing us to use it in the following lines. We now set our training dataset to everything in our original dataset except for our validation section, accomplished by using the skip attribute, letting us hop over the size of the validation dataset while grabbing everything else. We then intitialize our validation datset by using the take attribute of our original dataset, while using the size we specified earlier as an argument. We have now concluded construction of the datasets that we will train our models on.\n\n# checking the size of our training and validation datasets\nprint(\"Size of training dataset:\", len(train))\nprint(\"Size of validation dataset:\", len(val))\n\nSize of training dataset: 180\nSize of validation dataset: 45\n\n\nHere we are simply checking the sizes of our training and validation datasets. This is useful for making sure that our previous code block worked properly. While you may be concerned that there is only a combined size of 225, we must remember that we batched the data by 100 when we created the dataset, meaning that there is much more than just 225 articles in the combined sets.\n\n# defining variables for calculating baseline model\ntotSum = 0\nelNum = 0\n# iterating through train to check all labels\nfor article, fake in train:\n  # adding labels to totSum and length of each array to elNum\n  addVal =  np.sum(fake['fake'].numpy().flatten())\n  elNum = elNum + len(fake['fake'].numpy().flatten())\n  totSum = totSum + addVal\n\n# displaying results\nprint(\"Number of true articles is:\" , totSum)\nprint(\"Number of total articles is:\", elNum)\nprint(\"Baseline estimate is: \", max(totSum/elNum,((elNum-totSum)/elNum)))\n\nNumber of true articles is: 9412\nNumber of total articles is: 17949\nBaseline estimate is:  0.5243746169703047\n\n\nIt is useful to know the accuracy that a baseline model would achieve on a given datset. Recall that a baseline model simply guesses the most common label each time. To find the accuracy, we simply need to find whether the percentage of true or false articles is higher. We start by initializing two variables, totSum and elNum, which will be used to count the number of true articles and the total number of articles respectively. We will now iterate through our train dataset, calling the articles and their fake values. For each value, we want to add the number of true articles to our variable totSum. However, as true articles have a ‘fake’ value of one, and false articles have a ‘fake’ value of zero, we can simply add up all of the ‘fake’ values and we will be left with the total number of true articles. To accomplish this, for each ‘fake’ in train, we call the ‘fake’ attribue and call the numpy() attribute of it. This yields the ones and zeros referring to falsehood of each article. We must remember that we batched our data, making it appear in chunks as opposing to each article individually, hence we call the ‘flatten’ attribute to flatten the array before using the sum function from numpy to get the total of all the ‘false’ values from that array, setting that equal to the value we will add to our total sum of true articles. For each ‘fake’, we also call the length of the flattened array and add it to elNum in order to get an idea of how many total articles are contained in our dataset. Finally, within the for loop we add our value addVal to totSum to increment it by the number of true articles in the most recent batch, and then we are ready to display our data. We display the number of true articles by printing totSum, as well as the number of total articles by printing out elNum. Finally, to calculate our baseline accuracy, we use the max function to find whether there are more true or false articles, and then divide by the total number of articles, yielding an accuracy of around 52.437%. Note that we find the number of false articles by subtracting the number of true articles from the total number of articles as there are only two categories."
  },
  {
    "objectID": "posts/HW6/index.html#model-layer-preparation",
    "href": "posts/HW6/index.html#model-layer-preparation",
    "title": "HW 6: Fake News Detector",
    "section": "Model Layer Preparation",
    "text": "Model Layer Preparation\n\n# only top 2000 words will be tracked\nsize_vocabulary = 2000\n\n# converts input to lowercase and strips it of all punctuation\ndef standardization(input_data):\n    lowercase = tf.strings.lower(input_data)\n    no_punctuation = tf.strings.regex_replace(lowercase,\n                                  '[%s]' % re.escape(string.punctuation),'')\n    return no_punctuation\n\n\ntitle_vectorize_layer = tf.keras.layers.TextVectorization(\n    standardize=standardization,\n    max_tokens=size_vocabulary, # only consider this many words\n    output_mode='int',\n    output_sequence_length=500)\n\ntitle_vectorize_layer.adapt(train.map(lambda x, y: x[\"title\"]))\ntitle_vectorize_layer.adapt(train.map(lambda x, y: x[\"text\"]))\n\nWe start our model preparation by creating our text vectorization layer. The first thing we do is set the size of our vocabulary we wish to consider, in this case we chose 2000. Therefore, only the 2000 most popular words will be considered. Next, we create a standardization function which takes in an input, converts it all to lowercase, and removes all punctuation before returning the modified input. Finally, we create our title_vectorize_layer that we will be using within our model. We set it as an instance of a TextVectorization from keras and pass in a few arguments. As the method of standardization, we pass in our standardization function we just created, as well as setting max_tokens equal to the size of our vocabulary we defined at the top of this block of code. Finally, we set the layer to ouput_mode an integer while also setting the output_sequence_length equal to 500, meaning the returned vector will be of size 500. Finally, we adapt the layer to our headlines, allowing for the layer to learn what words are common. We accomplish this by calling the ‘adapt’ attribute of the function, and map it to to the ‘title’ and ‘text’ columns of our train dataset. Our text vectorization layer is now prepared for use.\n\n# creating embedding layer to be used in multiple models\nembedLayer = layers.Embedding(size_vocabulary, 3, name = 'embedding')\n\nHere we are creating an embedding layer that we will use throughout multiple models. We give it a name so that it is easier to reuse.\n\n# inputs\ntitleInput = keras.Input(shape = (1,), name = 'title', dtype = 'string')\ntextInput = keras.Input(shape = (1,), name = 'text', dtype = 'string')\n\nHere we are defining the different kinds of inputs we will pass into our models using the ‘Input’ attribute from keras. As our inputs are of the same shape, we can set the shape for both titleInput and textInput to be (1,). We give them each their respective names, as well as specifying they are both of the string datatype, and our inputs are now ready to be introduced into our model.\n\n# layers for processing the titles\n\ntitleFeatures = title_vectorize_layer(titleInput)\ntitleFeatures = embedLayer(titleFeatures)\ntitleFeatures = layers.Dropout(0.2)(titleFeatures)\ntitleFeatures = layers.GlobalAveragePooling1D()(titleFeatures)\ntitleFeatures = layers.Dropout(0.2)(titleFeatures)\ntitleFeatures = layers.Dense(32, activation='relu')(titleFeatures)\n\nHere we are beginning to define our model by specifying the layers that should be used to process our titles input. Note that we begin with the vectorization layer we defined earlier, followed by our embedding layer ‘embedLayer’ we defined previously. Note that for the first layer we pass in titleInput, but for each following layer we simply pass in the output of the previous layer. We are utilizing a variety of GlobalAveragePooling1D layers, Dense layers with ‘relu’ activation to introduce the property of nonlinearity, and Dropout layers to avoid overfitting.\n\n# layers for processing the text\n\ntextFeatures = title_vectorize_layer(textInput)\ntextFeatures = embedLayer(textFeatures)\ntextFeatures = layers.Dropout(0.2)(textFeatures)\ntextFeatures = layers.GlobalAveragePooling1D()(textFeatures)\ntextFeatures = layers.Dropout(0.2)(textFeatures)\ntextFeatures = layers.Dense(32, activation = 'relu')(textFeatures)\n\nIn this block we are doing the same thing as above, however for our textInputs. Note that the layers are the same, however we use our named layers we previously defined as the first two layers. These layers will be the exact same as the ones in the title process, while the other layers will be different instances of layers with the same parameters.\n\ndef trainingVis(history):\n  # display training history visualization\n  plt.plot(history.history[\"accuracy\"], label = \"training\")\n  plt.plot(history.history[\"val_accuracy\"], label = \"validation\")\n  plt.gca().set(xlabel = \"epoch\", ylabel = \"accuracy\")\n  plt.legend()\n  return\n\nHere we define a visualization function for the training history of each model. It takes in the history of a trained model, and displays a graph plotting training and validation accuracy across each epoch. We will see its use later after we train a model."
  },
  {
    "objectID": "posts/HW6/index.html#title-model",
    "href": "posts/HW6/index.html#title-model",
    "title": "HW 6: Fake News Detector",
    "section": "Title Model",
    "text": "Title Model\n\n# adding dense layer as well as specifying output\ntitleMain = layers.Dense(32, activation = 'relu')(titleFeatures)\ntitleOutput = layers.Dense(2, name = 'fake')(titleMain)\n\nHere we are adding an extra dense layer to be applied after the processing we previously defined for our title input. Additionally, we specify our output layer, a dense layer with two neurons. We specify the name ‘fake’ so that the layer knows which layer it is trying to predict\n\n# creating model\ntitleModel = keras.Model(inputs = titleInput, outputs = titleOutput)\n\nWe can now create our model, which we call titleModel, by specifying the inputs to just be titleInput, and specifying the output to be the titleOutput we defined in the previous block which contains all of our layer modificatins.\n\n#providing visualization for model\nutils.plot_model(titleModel, \"output_filename.png\",\n                      show_shapes=True,\n                      show_layer_names=True)\n\n\n\n\n\n\n\n\nHere we are able to provide a nice visualization of what our model is doing layer by layer. This is possible by use of the utils library. We call the plot_model function from this library and pass in our model as an argument to yield the plot above. We can see that each layer displays their name and output shape.\n\n# compiling titleModel\ntitleModel.compile(optimizer = \"adam\",\n              loss = losses.SparseCategoricalCrossentropy(from_logits=True),\n              metrics=['accuracy']\n)\n\nHere we compile our model to prepare for training. We are using the optimizer ‘adam’ as well as SparseCategoricalCrossentropy for our loss. We make sure to include ‘accuracy’ in our metrics so that we can view training history after our model has been trained.\n\ncallback = keras.callbacks.EarlyStopping(monitor='val_loss', patience = 5)\n\nHere we are creating an instance of the EarlyStopping callback. This will help us to prevent overfitting as it will stop the training process should certain criteria be met. We tell the callback to monitor the validation set loss by setting monitor equal to ‘val_loss’. This means that if ‘val_loss’ plateus or stops improving, the training will stop. We have also set patience to five, meaning that the validation will have to stop improving for five epochs before the callback will end the training. Now that we have defined our callback, we are ready to begin training our model.\n\n# training titleModel\ntitleHistory = titleModel.fit(train,\n                         validation_data = val,\n                         epochs = 50,\n                         callbacks=[callback],\n                         verbose = True)\n\nEpoch 1/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 9s 7ms/step - accuracy: 0.5218 - loss: 0.6924 - val_accuracy: 0.5173 - val_loss: 0.6925\nEpoch 2/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 1s 6ms/step - accuracy: 0.5233 - loss: 0.6915 - val_accuracy: 0.5173 - val_loss: 0.6886\nEpoch 3/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 1s 6ms/step - accuracy: 0.5977 - loss: 0.6630 - val_accuracy: 0.6213 - val_loss: 0.5933\nEpoch 4/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 1s 6ms/step - accuracy: 0.7174 - loss: 0.5419 - val_accuracy: 0.7851 - val_loss: 0.4722\nEpoch 5/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 1s 6ms/step - accuracy: 0.7541 - loss: 0.4983 - val_accuracy: 0.7936 - val_loss: 0.4496\nEpoch 6/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 1s 6ms/step - accuracy: 0.7657 - loss: 0.4780 - val_accuracy: 0.8069 - val_loss: 0.4245\nEpoch 7/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 1s 6ms/step - accuracy: 0.7844 - loss: 0.4572 - val_accuracy: 0.8018 - val_loss: 0.4216\nEpoch 8/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 1s 7ms/step - accuracy: 0.7834 - loss: 0.4549 - val_accuracy: 0.8258 - val_loss: 0.4015\nEpoch 9/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 9ms/step - accuracy: 0.7988 - loss: 0.4318 - val_accuracy: 0.8276 - val_loss: 0.3956\nEpoch 10/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 1s 6ms/step - accuracy: 0.8093 - loss: 0.4177 - val_accuracy: 0.8207 - val_loss: 0.3930\nEpoch 11/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 1s 6ms/step - accuracy: 0.8061 - loss: 0.4200 - val_accuracy: 0.8169 - val_loss: 0.4071\nEpoch 12/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 1s 6ms/step - accuracy: 0.8149 - loss: 0.4042 - val_accuracy: 0.8489 - val_loss: 0.3555\nEpoch 13/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 1s 6ms/step - accuracy: 0.8159 - loss: 0.4032 - val_accuracy: 0.8276 - val_loss: 0.3619\nEpoch 14/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 1s 6ms/step - accuracy: 0.8152 - loss: 0.3995 - val_accuracy: 0.8556 - val_loss: 0.3282\nEpoch 15/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 1s 6ms/step - accuracy: 0.8302 - loss: 0.3800 - val_accuracy: 0.8658 - val_loss: 0.3137\nEpoch 16/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 1s 6ms/step - accuracy: 0.8354 - loss: 0.3711 - val_accuracy: 0.8296 - val_loss: 0.3624\nEpoch 17/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 1s 6ms/step - accuracy: 0.8382 - loss: 0.3586 - val_accuracy: 0.8538 - val_loss: 0.3165\nEpoch 18/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 1s 7ms/step - accuracy: 0.8498 - loss: 0.3470 - val_accuracy: 0.8404 - val_loss: 0.3399\nEpoch 19/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 9ms/step - accuracy: 0.8493 - loss: 0.3356 - val_accuracy: 0.8513 - val_loss: 0.3256\nEpoch 20/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 1s 6ms/step - accuracy: 0.8506 - loss: 0.3348 - val_accuracy: 0.8418 - val_loss: 0.3332\n\n\nWe can finally train our model! We do so by using the ‘fit’ attribute of our model, passing in a few important arguments. We pass in the train dataset to tell the function what to train the model on, as well as giving the val dataset to be used as the validation_data. Additionally, we set the number of epochs to be fifty, which may seem large, however we also passed in the callback we just defined which will stop the training once it deems it necessary. As we can see from the output above, our model was relatively successful, settling at validation accuracies of between 83% and 86%. We will visualize below to make it easier to interpret the results.\n\n# displaying training history\ntrainingVis(titleHistory)\n\n\n\n\n\n\n\n\nUtilizing the visualization function trainingVis we defined earlier, we are able to plot our training and validation accuracy against the epochs. As we can see, there was a drastic increase in accuracy accross the first couple epochs, before the training and validation accuracies settled at above 80%, approximately 85% to be precise. These are pretty good results, however we only used titles as input. We can now use a model using article text as an input to check if we are able to obtain better results."
  },
  {
    "objectID": "posts/HW6/index.html#text-model",
    "href": "posts/HW6/index.html#text-model",
    "title": "HW 6: Fake News Detector",
    "section": "Text Model",
    "text": "Text Model\n\n# adding dense layer as well as specifying output\ntextMain = layers.Dense(32, activation = 'relu')(textFeatures)\ntextOutput = layers.Dense(2, name = 'fake')(textMain)\n\nHere we are adding an additional dense layer after the layers implemented by textFeatures, as well as defining our output layer, specifying the name ‘fake’ so that the layer knows what to predict.\n\n# creating model\ntextModel = keras.Model(inputs = textInput, outputs = textOutput)\n\nHere we can define our model as we did for the titleModel, however this time we pass in only the textInput for our inputs and define our output to be the textOutput we defined above.\n\n# providing visualization for textModel\nutils.plot_model(textModel, \"output_filename.png\",\n                       show_shapes=True,\n                       show_layer_names=True)\n\n\n\n\n\n\n\n\nHere we have a plot displaying the process an input goes through within our model. Note how it is nearly identical to our previous model. This is due to the fact that the layers used in each model are different instances of layers passed the same parametrs, with the text vectorization layer and the embedding layer being the exact same layers.\n\n# compiling textModel\ntextModel.compile(optimizer = \"adam\",\n              loss = losses.SparseCategoricalCrossentropy(from_logits=True),\n              metrics=['accuracy']\n)\n\nWe can now compile our textModel, again using the optimizer ‘adam’ as well as using SparseCategoricalCrossentropy as our loss function. Note that it is critical for visualization of training data that we include ‘accuracy’ within our metrics argument.\n\ntextHistory = textModel.fit(train,\n                         validation_data = val,\n                         epochs = 50,\n                         callbacks=[callback],\n                         verbose = True)\n\nEpoch 1/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 12ms/step - accuracy: 0.6574 - loss: 0.6223 - val_accuracy: 0.8469 - val_loss: 0.3582\nEpoch 2/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 11ms/step - accuracy: 0.8766 - loss: 0.3087 - val_accuracy: 0.9127 - val_loss: 0.2105\nEpoch 3/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 12ms/step - accuracy: 0.9217 - loss: 0.2090 - val_accuracy: 0.9160 - val_loss: 0.1883\nEpoch 4/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 15ms/step - accuracy: 0.9365 - loss: 0.1730 - val_accuracy: 0.9329 - val_loss: 0.1645\nEpoch 5/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 4s 11ms/step - accuracy: 0.9419 - loss: 0.1582 - val_accuracy: 0.9420 - val_loss: 0.1425\nEpoch 6/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 11ms/step - accuracy: 0.9531 - loss: 0.1410 - val_accuracy: 0.9416 - val_loss: 0.1417\nEpoch 7/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 12ms/step - accuracy: 0.9567 - loss: 0.1293 - val_accuracy: 0.9371 - val_loss: 0.1505\nEpoch 8/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 14ms/step - accuracy: 0.9537 - loss: 0.1286 - val_accuracy: 0.9411 - val_loss: 0.1379\nEpoch 9/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 5s 11ms/step - accuracy: 0.9584 - loss: 0.1211 - val_accuracy: 0.9318 - val_loss: 0.1617\nEpoch 10/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 11ms/step - accuracy: 0.9571 - loss: 0.1186 - val_accuracy: 0.9413 - val_loss: 0.1426\nEpoch 11/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 13ms/step - accuracy: 0.9622 - loss: 0.1096 - val_accuracy: 0.9438 - val_loss: 0.1369\nEpoch 12/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 13ms/step - accuracy: 0.9634 - loss: 0.1086 - val_accuracy: 0.9378 - val_loss: 0.1563\nEpoch 13/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 11ms/step - accuracy: 0.9655 - loss: 0.0998 - val_accuracy: 0.9431 - val_loss: 0.1375\nEpoch 14/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 11ms/step - accuracy: 0.9647 - loss: 0.1009 - val_accuracy: 0.9440 - val_loss: 0.1365\nEpoch 15/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 11ms/step - accuracy: 0.9621 - loss: 0.0997 - val_accuracy: 0.9469 - val_loss: 0.1305\nEpoch 16/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 13ms/step - accuracy: 0.9678 - loss: 0.0908 - val_accuracy: 0.9438 - val_loss: 0.1381\nEpoch 17/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 13ms/step - accuracy: 0.9677 - loss: 0.0879 - val_accuracy: 0.9509 - val_loss: 0.1176\nEpoch 18/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 11ms/step - accuracy: 0.9703 - loss: 0.0842 - val_accuracy: 0.9322 - val_loss: 0.1812\nEpoch 19/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 11ms/step - accuracy: 0.9688 - loss: 0.0845 - val_accuracy: 0.9509 - val_loss: 0.1164\nEpoch 20/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 11ms/step - accuracy: 0.9703 - loss: 0.0823 - val_accuracy: 0.9456 - val_loss: 0.1399\nEpoch 21/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 11ms/step - accuracy: 0.9704 - loss: 0.0825 - val_accuracy: 0.9358 - val_loss: 0.1686\nEpoch 22/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 16ms/step - accuracy: 0.9700 - loss: 0.0832 - val_accuracy: 0.9460 - val_loss: 0.1303\nEpoch 23/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 4s 11ms/step - accuracy: 0.9748 - loss: 0.0748 - val_accuracy: 0.9469 - val_loss: 0.1307\nEpoch 24/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 11ms/step - accuracy: 0.9741 - loss: 0.0726 - val_accuracy: 0.9504 - val_loss: 0.1232\n\n\nWhen training this function, we pass in all the same arguments as we did when we passed in our titleModel training. We will save it to a different variable, textHistory, to display later. We can see that this model performed slightly bette, as the training accuracy settled around 97% while the validation accuracies settled at around 95%. We can take a closer look in the following visualization.\n\n# display training history for textModel\ntrainingVis(textHistory)\n\n\n\n\n\n\n\n\nWe have a similar graph to the titleModel training history, however note that in this graph the validation accuracy started much higher. While both graphs settled at a reasonable accuracy for both validation and training accuracy, the textModel achieved training accuracies of 97% and validation accuracies of 95%, representing more than a 10% increase from the previous model. In our next model we will test using both article title and article text as inputs to see if that can even further increase our accuracy."
  },
  {
    "objectID": "posts/HW6/index.html#model-using-title-and-text",
    "href": "posts/HW6/index.html#model-using-title-and-text",
    "title": "HW 6: Fake News Detector",
    "section": "Model Using Title and Text",
    "text": "Model Using Title and Text\n\n# concatenating title and text features\nfinMain = layers.concatenate([titleFeatures, textFeatures], axis = 1)\n\nWhile somewhat unassuming, this is one of the most critical lines in this model. We are concatenating the output of the pipelines titleFeatures and textFeatures. This will allow these inputs to work in unison in our model.\n\nfinMain = layers.Dense(32, activation = 'relu')(finMain)\nfinOutput = layers.Dense(2, name = 'fake')(finMain)\n\nWe are again adding two dense layers, with one used for the output, as we did in the previous models. Note that we are now setting the first layer equal to finMain which we defined with the concatenated layer that we defined above.\n\n# creating model\nfinModel = keras.Model(inputs = [titleInput, textInput], outputs = finOutput)\n\nHere we are creating our model, this time passing in a list with titleInput and textInput as our list of inputs. Our output is finOutput which we defined in the previous block of code.\n\n# displaying visualization for finModel\nutils.plot_model(finModel,\n                 show_shapes=True,\n                 show_layer_names=True)\n\n\n\n\n\n\n\n\nThe visualization of this model appears more complex than the previous ones. We can see the two inputs that are received from the model are at first joined in passing through the text vectorization layer followed by the embedding layer. This is due to the fact that these are the exact saem layers as we defined them prior to the creation of the pipelines and passed them in by name. After these two layers, we see the a split into two tracks, representing the different paths taken by the two inputs. However if we look closely, despite the names being different, the type of layers, as well as their inputs and outputs are identical. The layers are concatenated again before we reach our final dense layers, which are identical to the previous models.\n\n# compiling finModel\nfinModel.compile(optimizer = \"adam\",\n              loss = losses.SparseCategoricalCrossentropy(from_logits=True),\n              metrics=['accuracy']\n)\n\nThe comilation of this model remains identical to the previous models despite having multiple inputs. The optimizer and loss functions remain the same. We can now test the model to check for an improvement in our accuracy scores.\n\nfinHistory = finModel.fit(train,\n                         validation_data = val,\n                         epochs = 50,\n                         callbacks=[callback],\n                         verbose = True)\n\nEpoch 1/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 5s 15ms/step - accuracy: 0.8738 - loss: 0.3823 - val_accuracy: 0.9787 - val_loss: 0.0905\nEpoch 2/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 5s 17ms/step - accuracy: 0.9752 - loss: 0.0873 - val_accuracy: 0.9791 - val_loss: 0.0794\nEpoch 3/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 13ms/step - accuracy: 0.9778 - loss: 0.0706 - val_accuracy: 0.9782 - val_loss: 0.0786\nEpoch 4/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 13ms/step - accuracy: 0.9804 - loss: 0.0585 - val_accuracy: 0.9787 - val_loss: 0.0789\nEpoch 5/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 13ms/step - accuracy: 0.9813 - loss: 0.0622 - val_accuracy: 0.9793 - val_loss: 0.0784\nEpoch 6/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 13ms/step - accuracy: 0.9825 - loss: 0.0566 - val_accuracy: 0.9787 - val_loss: 0.0826\nEpoch 7/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 18ms/step - accuracy: 0.9817 - loss: 0.0560 - val_accuracy: 0.9782 - val_loss: 0.0822\nEpoch 8/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 13ms/step - accuracy: 0.9785 - loss: 0.0618 - val_accuracy: 0.9804 - val_loss: 0.0750\nEpoch 9/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 13ms/step - accuracy: 0.9824 - loss: 0.0530 - val_accuracy: 0.9800 - val_loss: 0.0770\nEpoch 10/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 13ms/step - accuracy: 0.9810 - loss: 0.0562 - val_accuracy: 0.9793 - val_loss: 0.0764\nEpoch 11/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 15ms/step - accuracy: 0.9815 - loss: 0.0546 - val_accuracy: 0.9593 - val_loss: 0.0964\nEpoch 12/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 5s 13ms/step - accuracy: 0.9827 - loss: 0.0483 - val_accuracy: 0.9809 - val_loss: 0.0819\nEpoch 13/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 13ms/step - accuracy: 0.9847 - loss: 0.0494 - val_accuracy: 0.9804 - val_loss: 0.0785\n\n\nUsing the same training call, we observe another improvement in accuracy. Our validation accuracy settles at around 98%, while the training accuracy settles even higher, at just under 99%. This is yet another improvement in accuracy, which we can take another look at in our visualization below.\n\n# display training history for finModel\ntrainingVis(finHistory)\n\n\n\n\n\n\n\n\nWe again see a rapid increase in training accuracy, however if we look carefully, it also starts much higher, at just under 95%. While it appears that this training resulted in much more variable results due to the apparent ups and downs in our accuracies, we must take into account the fact that this plot is much more zoomed in, just showing accuracies starting at 94%. We can confidently state that this was our most successful model with an increase of 1% accuracy over our previous model using just the article text.\nBased on our results, it seems evident that algorithms should use both title and text when trying to detect fake news. While using the text gave us very good results at approximately 95% validation accuracy, including the title increased our accuracy by about 1%, which is not a massive improvement, but nevertheless is worth it."
  },
  {
    "objectID": "posts/HW6/index.html#evaluating-model-on-test-set",
    "href": "posts/HW6/index.html#evaluating-model-on-test-set",
    "title": "HW 6: Fake News Detector",
    "section": "Evaluating Model on Test Set",
    "text": "Evaluating Model on Test Set\n\n# reading in test dataset\ntest_url = \"https://github.com/PhilChodrow/PIC16b/blob/master/datasets/fake_news_test.csv?raw=true\"\ntestDf = pd.read_csv(test_url)\n\nHere we are reading in a new dataframe that we will use to test our highest performing model against.\n\n# creating a Dataset from testDf\ntest = make_dataset(testDf)\n\nWe are converting the test dataframe we just read in into a dastaset using the make_dataset function we created earlier.\n\n# testing most accurate model on test set\nfinModel.evaluate(test)\n\n225/225 ━━━━━━━━━━━━━━━━━━━━ 2s 8ms/step - accuracy: 0.9803 - loss: 0.0699\n\n\n[0.06710163503885269, 0.980756402015686]\n\n\nUsing the evaluation function, we test our highest performing model, the one utilizing multiple inputs, on our new dataset test. As we can see, we achieved very good accuracy at 98%. This means that in theory if we were to utilize our model as a fake news detector we would be right approximately 98% of the time."
  },
  {
    "objectID": "posts/HW6/index.html#embedding-visualization",
    "href": "posts/HW6/index.html#embedding-visualization",
    "title": "HW 6: Fake News Detector",
    "section": "Embedding Visualization",
    "text": "Embedding Visualization\n\nweights = finModel.get_layer('embedding').get_weights()[0] # get the weights from the embedding layer\nvocab = title_vectorize_layer.get_vocabulary()                # get the vocabulary from our data prep for later\n\nfrom sklearn.decomposition import PCA\npca = PCA(n_components=2)\nweights = pca.fit_transform(weights)\n\nembedding_df = pd.DataFrame({\n    'word' : vocab,\n    'x0'   : weights[:,0],\n    'x1'   : weights[:,1]\n})\n\nHere we are setting up a visualization of our embedding layer. We use the PCA library to reduce the dimensions to a visualizable number, and we can see where these variables are used in the following block.\n\n#use iframe\nimport plotly.io as pio\niframe_renderer = pio.renderers['iframe_connected']\niframe_renderer.html_directory='notebooks/iframe_figures'\npio.renderers.default = \"iframe_connected\"\n\n\nimport plotly.express as px\nfig = px.scatter(embedding_df,\n                 x = \"x0\",\n                 y = \"x1\",\n                 size = list(np.ones(len(embedding_df))),\n                 size_max = 5,\n                 hover_name = \"word\")\n\nfig.show()\n\n\n\n\nHere we create a visualization for the embedding layer using the weights and variables we found in the previous code block. Certain locations on the plot, as well as proximity to other words can tell us a significant amount about the given word. For example, in the bottom left-hand side, approximately at coordinates roughly (-3.5,0), we see two points that are very close together. These words are ‘myanmar’ and ‘rohingya’. Due to their close proximity, we can assume that they are related in some way, which in this case is due to the significant conflict between the Rohingya people and the Myanmar military. Similarly, at around the point (3,0.05), we see that the words ‘racist’ and ‘terror’ are essentially on top of each other, which makes sense as these words are very closely tied together. An interesting point is at (-5,-0.025), with the word being ‘trumps’. This word is clearly isolated from the main bunch as there are no words in its immediate vicinity. Some possible reasons as to why this word is so isolated are that it is so specific that it does now have many synonyms, hence its isolation. Additionally, it may be a result of political bias within the articles, which may lead the embedding to differentiate the word based on certain political connotations."
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code."
  },
  {
    "objectID": "posts/HW5/index.html",
    "href": "posts/HW5/index.html",
    "title": "HW 5: Image Classification with Keras",
    "section": "",
    "text": "Today we will be creating various keras models to help us classify images containing either cats or dogs. We will evaluate each model on a dataset to see which of our models has the highest accuracy."
  },
  {
    "objectID": "posts/HW5/index.html#introduction",
    "href": "posts/HW5/index.html#introduction",
    "title": "HW 5: Image Classification with Keras",
    "section": "",
    "text": "Today we will be creating various keras models to help us classify images containing either cats or dogs. We will evaluate each model on a dataset to see which of our models has the highest accuracy."
  },
  {
    "objectID": "posts/HW5/index.html#imports",
    "href": "posts/HW5/index.html#imports",
    "title": "HW 5: Image Classification with Keras",
    "section": "Imports",
    "text": "Imports\n\nimport os\nfrom keras import utils\nimport tensorflow_datasets as tfds\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nos.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n\nThis block is where we will begin our imports to be used for this project. We then change the backend of keras to be tensorflow as it will allow for our image augmentation layers to run faster.\n\nimport keras\nfrom keras import layers\n\nWe have another import block in which we import keras and layers from keras. It is important that keras is imported after we change the backend to tensorflow, as that cannot be changed once keras has been imported.\n\ntrain_ds, validation_ds, test_ds = tfds.load(\n    \"cats_vs_dogs\",\n    #40% for training, 10% for validation, and 10% for test (the rest unused)\n    split=[\"train[:40%]\", \"train[40%:50%]\", \"train[50%:60%]\"],\n    as_supervised=True, #Include labels\n)\n\nprint(f\"Number of training samples: {train_ds.cardinality()}\")\nprint(f\"Number of validation samples: {validation_ds.cardinality()}\")\nprint(f\"Number of test samples: {test_ds.cardinality()}\")\n\nDownloading and preparing dataset 786.67 MiB (download: 786.67 MiB, generated: 1.04 GiB, total: 1.81 GiB) to /root/tensorflow_datasets/cats_vs_dogs/4.0.1...\nDataset cats_vs_dogs downloaded and prepared to /root/tensorflow_datasets/cats_vs_dogs/4.0.1. Subsequent calls will reuse this data.\nNumber of training samples: 9305\nNumber of validation samples: 2326\nNumber of test samples: 2326\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWARNING:absl:1738 images were corrupted and were skipped\n\n\n\n\n\nThis block is used to access our data, as well as organizing it into datasets that will be used later. As we define our dataset names in the beginning as train_ds, validation_ds, and test_ds. We then call tfds.load, which loads datasets from the tensorflow_datasets library, which we then specify to be the “cats_vs_dogs” dataset. We then define splits for how much data will be implemented into each dataset. As you can see above, forty percent is dedicated to training, while only ten percent are allocated to both the validation and test datasets. Finally, we print out how many samples are in each dataset by calling the cardinality of each dataset. As expected, the number of training samples is approximately four times larger than that of the validation or testing datasets, which are themselves equal in size.\n\nresize_fn = keras.layers.Resizing(150,150)\n\ntrain_ds = train_ds.map(lambda x,y: (resize_fn(x),y))\nvalidation_ds = validation_ds.map(lambda x,y : (resize_fn(x),y))\ntest_ds = test_ds.map(lambda x,y:(resize_fn(x), y))\n\nWe now clean up our data a bit by creating a constant size for all samples. We can see that resize_fn calls a keras layer that resizes its input to be of size 150 x 150. We then use this layer to create lambda functions to be applied to all three of our datasets. We can now be sure of the dimensions of all of our samples in each of our datasets.\n\nfrom tensorflow import data as tf_data\nfrom tensorflow.keras.optimizers import Adam\nbatch_size = 64\n\ntrain_ds = train_ds.batch(batch_size).prefetch(tf_data.AUTOTUNE).cache()\nvalidation_ds = validation_ds.batch(batch_size).prefetch(tf_data.AUTOTUNE).cache()\ntest_ds = test_ds.batch(batch_size).prefetch(tf_data.AUTOTUNE).cache()\n\nThis block is a bit technical. We import tf_data to be used in this block, as well as an optimizer called Adam that we will not use until later. The batch_size variable we define determines how many data points are gathered from the directory at once."
  },
  {
    "objectID": "posts/HW5/index.html#visualization",
    "href": "posts/HW5/index.html#visualization",
    "title": "HW 5: Image Classification with Keras",
    "section": "Visualization",
    "text": "Visualization\n\ndef twoRowVis():\n    plt.figure(figsize=(10, 10))\n    for images, labels in train_ds.take(1):\n        dogNum = 0\n        catNum = 0\n\n        for i in range(32):\n            if labels[i].numpy() == 1 and dogNum &lt; 3:\n                ax = plt.subplot(2, 3, dogNum + 1)\n                plt.imshow(images[i].numpy().astype(\"uint8\"))\n                plt.title(\"dog\")\n                plt.axis(\"off\")\n                dogNum += 1\n\n            elif labels[i].numpy() == 0 and catNum &lt; 3:\n                ax = plt.subplot(2, 3, 3 + catNum + 1)\n                plt.imshow(images[i].numpy().astype(\"uint8\"))\n                plt.title(\"cat\")\n                plt.axis(\"off\")\n                catNum += 1\n\n            if dogNum == 3 and catNum == 3:\n                break\n\n    plt.show()\n    return\n\n\n\nAbove we have created a function for visualizing some of our images. We first set the size of the images by setting our figure size to (10,10). Now we need to access our images to be able to fill our plot. The method used to do this is take(1), which will retrieve one batch of images with labels, which we defined to be of size 64 in the previous code block. We will begin a for loop using these labels and images by calling for images and labels returned by the take method we call upon train_ds. It is critical to note that images containing dogs are labeled with a 1 while images with cats are labeled with a 0. Now that we have accessed our images, we wish to display a row of three images of dogs, followed by a row of three images with cats in them. In order to keep track of how many images of each have been displayed, we will create two variables titled numCats and numDogs respectively. We can now loop through and add images to a plot should they satisfy our criteria. We will first check if the image is a dog, by checking if the numpy attribute of the current label we are on is equal to 1. Additionally, we must check that we have not already entered the images for three dogs, by checking if numDog is less than three. Should this be true, we create a plot of dimensions (2,3) as we wish to have our plot contain two rows of three. We will then input this image into the plot at the first position. However as this is a loop, we cannot simply place 1, but rather we have to consider how the loop will deal with future images, which is where our numDogs variable will come in handy. We can send each image to the position 1+numDog in the plot, as they will each be placed one after another as we continually increment numDog. We can then use plt.imshow() to display our image, which can be called by using images[i].numpy(), as this image corresponds to the label we checked at the beginning of the if statement. Finally, we can set the title of our image to be label, turn off the axis, and increment our numDog and we are done with the first portion of our function. Placing our cats in the bottom row is very similar to what we just did, but with a few minor tweaks. First off, in our if statement condition, we want to ensure that the label is equal to 0, not 1. Secondly, as we want the cats to be on the bottom, we will add an extra three to their position in the plot, as they will then begin being placed in the bottom row. Other than that, as long as we make sure to use numCat in our position and incrementation instead of numDog, our cat placement should be complete. Finally, to ensure we finish as we place our final image in, we call a final if statement, where if both numCat and numDog are equal to or greater than three, we break out of our loop. After our loop has concluded, we can call plt.show() and our function is complete.\n\n# Calling visualization function defined above\ntwoRowVis()\n\n\n\n\n\n\n\n\nAs you can see, we call the function twoRowVis() that we created in the above code block, and it displays a row of three images with dogs followed by a row of three cat images. Notice how the labels above all the dogs are 1 and above all the cats are 0.\n\n# Creating a label iterator\nlabels_iterator=train_ds.unbatch().map(lambda image, label: label).as_numpy_iterator()\n\nWe now will create a label iterator so that we can check the number of cats and dogs in our training dataset. To do so, we unbatch train_ds to get all of our information, and then use a lambda function to extract just the labels. Finally we use the as_numpy_iterator() function to transform it into an iterator, yielding the iterator we title labels_iterator.\n\n# Compute the number of cats (labelled 0) and dogs (labelled 1) in training data\ncatCounter = 0\ndogCounter = 0\nfor label in labels_iterator:\n  if label==0:\n    catCounter = catCounter+1\n  elif label==1:\n    dogCounter = dogCounter+1\n\nprint(\"Number of cats is: \", catCounter)\nprint(\"Number of dogs is: \", dogCounter)\n\nNumber of cats is:  4637\nNumber of dogs is:  4668\n\n\nHere we use our label iterator that we made prviously to caluclate the number of cats and dogs in our training dataset. We will create two variables that will be used to count the number of cats and dogs in our dataset, titled catCounter and dogCounter. Recall that cats are distinguished with a label of 0 while dogs have a label of 1. We use this information to run through the iterator, incrementing the catCounter when we find a label equal to zero, and incrementing dogCounter when we encounter a label equal to one. Finally, we print out our results and find that there is a similar number of cat and dog images, 4,637 and 4,668 respectively.\nThis information can be used to estimate the accuracy of a baseline learning model, a model that simply guesses the most frequent label each time. As we can see, there are more dogs in this dataset, therefore the baseline would guess dog every time. We can calculate the accuracy by taking how many times it would be correct over the total samples, yielding 4,668/9,305= 0.50166, or approximately 50.166% accuracy."
  },
  {
    "objectID": "posts/HW5/index.html#first-model",
    "href": "posts/HW5/index.html#first-model",
    "title": "HW 5: Image Classification with Keras",
    "section": "First Model",
    "text": "First Model\n\n# Create a sequential model\nmodel = keras.Sequential()\n\nmodel.add(layers.Input((150,150,3)))\n\n# Convolutional layers\nmodel.add(layers.Conv2D(32, (3, 3), activation='relu'))\nmodel.add(layers.MaxPooling2D(pool_size=(2, 2)))\n\nmodel.add(layers.Conv2D(64, (3, 3), activation='relu'))\nmodel.add(layers.MaxPooling2D(pool_size=(2, 2)))\n\n# Flatten layer to convert 2D feature maps to a vector\nmodel.add(layers.Flatten())\n\n# Fully connected layers\nmodel.add(layers.Dense(128, activation='relu'))\nmodel.add(layers.Dropout(0.5))  # Dropout layer to prevent overfitting\n\n# Output layer with binary classification (sigmoid activation for binary classification)\nmodel.add(layers.Dense(2))\n\n# Compile the model\nmodel.compile(optimizer='adam', loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])\n\n# Print the model summary\nmodel.summary()\n\n\nModel: \"sequential\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n conv2d (Conv2D)             (None, 148, 148, 32)      896       \n                                                                 \n max_pooling2d (MaxPooling2  (None, 74, 74, 32)        0         \n D)                                                              \n                                                                 \n conv2d_1 (Conv2D)           (None, 72, 72, 64)        18496     \n                                                                 \n max_pooling2d_1 (MaxPoolin  (None, 36, 36, 64)        0         \n g2D)                                                            \n                                                                 \n flatten (Flatten)           (None, 82944)             0         \n                                                                 \n dense (Dense)               (None, 128)               10616960  \n                                                                 \n dropout (Dropout)           (None, 128)               0         \n                                                                 \n dense_1 (Dense)             (None, 2)                 258       \n                                                                 \n=================================================================\nTotal params: 10636610 (40.58 MB)\nTrainable params: 10636610 (40.58 MB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\n\n\nWe will now create our first keras sequential model. We start by initializing our model, titled model, to be an instance of keras.Sequential(). Now we can begin introducing our layers. Firstly, as we know the dimensions of each of our inputs, we can give this information to the model by adding an Input layer, specifying that each input will be of size (150,150,3). We know the dimensions as we resized each image earlier to be of size (150,150), and then each image has rgb coloring, representing the final dimension of size 3. We can now begin adding our convolutional layers to our model. We start by adding a layer called Conv2D. These layers effectively sharpen the image passed through them by increasing the contrast between pixels by passing them through a kernel. They slightly change the size of the image, as we can see in our summary above by looking at the input it is passed in and observing the difference between its output. We then implement several MaxPooling2D layers. Put somewhat simply, these down-sample the spatial dimensions of the input, while retaining most of the important information. After two iterations of both of those layers, we call a Flatten layer, which converts its input into a one dimension. This then allows us to call Dense layers, which require a flattened input. Our first dense layer has an argument of 128 passed to it, meaning it is functioning with 128 neurons. After our first dense layer, we implement a dropout layer which excludes various nodes with a probability passed into it, with the intention to combat overfitting. Finally, we call a dense layer with only two categories so that the model can determine if the test image is either a cat or dog. You may notice that several of the layers have an activation argument introduced. These activations are mathematical functions applied to the output of a layer which serve as a way to introduce non-linearity into our model. Additionally, in our final dense layer, we see an optimizer and loss argument. These serve to help the network know what kind of loss it should be trying to avoid, as well as adjust weights and learning rates in order to get a more accurate final result. At the end of this layer, we call metrics = [‘accuracy’] to ensure that we can access information on the accuracy when we train the model. Finally, we call model.summary() to get the representation of the model we see outputted above.\n\n# Train model1 here\nhistory = model.fit(train_ds,\n                     epochs=20,\n                     validation_data=validation_ds)\n\nEpoch 1/20\n146/146 [==============================] - 17s 73ms/step - loss: 19.3372 - accuracy: 0.5457 - val_loss: 0.6769 - val_accuracy: 0.5464\nEpoch 2/20\n146/146 [==============================] - 5s 34ms/step - loss: 0.6161 - accuracy: 0.6516 - val_loss: 0.6674 - val_accuracy: 0.6032\nEpoch 3/20\n146/146 [==============================] - 5s 34ms/step - loss: 0.5227 - accuracy: 0.7246 - val_loss: 0.7363 - val_accuracy: 0.5860\nEpoch 4/20\n146/146 [==============================] - 5s 33ms/step - loss: 0.4344 - accuracy: 0.7883 - val_loss: 0.7968 - val_accuracy: 0.5817\nEpoch 5/20\n146/146 [==============================] - 5s 34ms/step - loss: 0.3732 - accuracy: 0.8245 - val_loss: 0.8865 - val_accuracy: 0.5886\nEpoch 6/20\n146/146 [==============================] - 5s 34ms/step - loss: 0.3202 - accuracy: 0.8604 - val_loss: 0.9987 - val_accuracy: 0.5817\nEpoch 7/20\n146/146 [==============================] - 5s 36ms/step - loss: 0.3381 - accuracy: 0.8528 - val_loss: 0.9540 - val_accuracy: 0.5911\nEpoch 8/20\n146/146 [==============================] - 5s 34ms/step - loss: 0.2380 - accuracy: 0.9017 - val_loss: 1.1495 - val_accuracy: 0.5877\nEpoch 9/20\n146/146 [==============================] - 5s 34ms/step - loss: 0.2813 - accuracy: 0.8816 - val_loss: 1.1250 - val_accuracy: 0.6015\nEpoch 10/20\n146/146 [==============================] - 5s 34ms/step - loss: 0.2122 - accuracy: 0.9159 - val_loss: 1.4047 - val_accuracy: 0.5916\nEpoch 11/20\n146/146 [==============================] - 5s 34ms/step - loss: 0.1932 - accuracy: 0.9244 - val_loss: 1.4502 - val_accuracy: 0.6019\nEpoch 12/20\n146/146 [==============================] - 5s 34ms/step - loss: 0.1561 - accuracy: 0.9461 - val_loss: 1.5631 - val_accuracy: 0.5972\nEpoch 13/20\n146/146 [==============================] - 5s 34ms/step - loss: 0.1751 - accuracy: 0.9371 - val_loss: 1.6239 - val_accuracy: 0.5976\nEpoch 14/20\n146/146 [==============================] - 5s 35ms/step - loss: 0.1599 - accuracy: 0.9416 - val_loss: 1.6674 - val_accuracy: 0.6122\nEpoch 15/20\n146/146 [==============================] - 5s 34ms/step - loss: 0.1325 - accuracy: 0.9520 - val_loss: 1.7750 - val_accuracy: 0.5907\nEpoch 16/20\n146/146 [==============================] - 5s 36ms/step - loss: 0.1415 - accuracy: 0.9498 - val_loss: 1.7944 - val_accuracy: 0.6045\nEpoch 17/20\n146/146 [==============================] - 5s 37ms/step - loss: 0.2000 - accuracy: 0.9327 - val_loss: 1.6671 - val_accuracy: 0.6148\nEpoch 18/20\n146/146 [==============================] - 5s 34ms/step - loss: 0.1824 - accuracy: 0.9341 - val_loss: 1.7361 - val_accuracy: 0.6066\nEpoch 19/20\n146/146 [==============================] - 5s 34ms/step - loss: 0.1369 - accuracy: 0.9522 - val_loss: 1.5825 - val_accuracy: 0.6032\nEpoch 20/20\n146/146 [==============================] - 5s 34ms/step - loss: 0.1059 - accuracy: 0.9636 - val_loss: 1.9182 - val_accuracy: 0.6234\n\n\nWe now will train our newly created model on our training dataset. We want to save this training so that we access the information later to get more information on how our training evolved over time. We initiate training with the fit() method, passing train_ds as an argument representing the dataset to train over, as well as the number of epochs we wish to run, and the validation dataset to test on after each dataset. We can note that our validation accuracy settled between 59% and 63% by the final epochs. This represents an improvement over our baseline which we calculated to have an accuracy of just over 50%. With regards to overfitting, we can observe there is definitely a prominent presence as our validation accuracy began to stagnate at around 60% while the training accuracy continued to improve, reaching above 95% accuracy. Note that the graph being referred to is displayed just below.\n\nplt.plot(history.history[\"accuracy\"], label=\"training\")\nplt.plot(history.history['val_accuracy'], label=\"validation\")\nplt.gca().set(xlabel=\"epoch\", ylabel=\"accuracy\")\nplt.legend()\n\n\n\n\n\n\n\n\nIn order to visualize how our model did, we plot the training accuracy as well as the validation accuracy accross each epoch. We can see in this case that our validation accuracy remained somewhat stagnant while our training accuracy continued to increase, a sign that overfitting became a significant factor."
  },
  {
    "objectID": "posts/HW5/index.html#second-model",
    "href": "posts/HW5/index.html#second-model",
    "title": "HW 5: Image Classification with Keras",
    "section": "Second Model",
    "text": "Second Model\n\n#Demonstrate flipping\nmodelFlip = keras.Sequential()\nmodelFlip.add(layers.RandomFlip())\n\n\nfor images, labels in train_ds.take(1):\n    plt.subplot(1, 3, 1)\n    plt.imshow(images[0].numpy().astype(\"uint8\"))\n    plt.title(\"Original Image\")\n    plt.axis(\"off\")\n    plt.subplot(1, 3, 2)\n    plt.imshow(modelFlip(images[0]).numpy().astype(\"uint8\"))\n    plt.title(\"Flipped Image\")\n    plt.axis(\"off\")\n    plt.subplot(1, 3, 3)\n    plt.imshow(modelFlip(images[0]).numpy().astype(\"uint8\"))\n    plt.title(\"Flipped Image\")\n    plt.axis(\"off\")\n    plt.show()\n\n\n\n\n\n\n\n\n\nIn this section we are experimenting with data augmentation layers, more specifically RandomFlip() and RandomRotation(). Starting with RandomFlip() we start of by making a model with just a RandomFlip() layer in it. We can demonstrate what this layer does by passing images through it and displaying what is outputted. On the left we see the first image from our train_ds dataset, shown using pyplot. To find out what happens when using the flipped layer, we display the same image, however when passing the image to imshow(), we apply the model to the image. What comes out is a reflected version of our original image, as you can see in the middle. We can do this same process one more time, and as we can see from the third image, we get another flipped version of our original image, however this one is flipped horizontally instead of vertically. If we do not pass any arguments to RandomFlip() we can see from the above images that it can flip the images both vertically and horizontally. Should we want it to only be able to flip images vertically we can pass the argument ‘vertical’ into the layer, and we will only get vertical flips.\n\n#Demonstrate rotating\nmodelRotate = keras.Sequential()\nmodelRotate.add(layers.RandomRotation(factor = 0.2))\n\n#plt.figure(figsize=(10,10))\n\nfor images, labels in train_ds.take(1):\n    ax = plt.subplot(1, 3, 1)\n    plt.imshow(images[0].numpy().astype(\"uint8\"))\n    plt.title(\"Original Image\")\n    plt.axis(\"off\")\n    #plt.show()\n    ax = plt.subplot(1,3,2)\n    plt.imshow(modelRotate(images[0]).numpy().astype(\"uint8\"))\n    plt.title(\"Rotated Image\")\n    plt.axis(\"off\")\n    #plt.show()\n    ax = plt.subplot(1, 3, 3)\n    plt.imshow(modelRotate(images[0]).numpy().astype(\"uint8\"))\n    plt.title(\"Rotated Image\")\n    plt.axis(\"off\")\n    plt.show()\n\n\n\n\n\n\n\n\nSimilar to our RandomFlip() demonstration above, here we are showing what a RandomRotation() layer does to an image. Again, we have our original image on the left. We can see from the second image that when we apply a RandomRotation() layer to our image is slightly rotated, in this case clockwise. Running the image through the layer again results in a different rotation as seen in image three. The amount of rotation can be controlled by passing a rotation factor, in this case we used 0.2. This factor represents a factor of 2 Pi, which becomes the maximum roatation this layer will provide, given that 2 Pi is a complete spin.\n\n# Create a sequential model\nmodel2 = keras.Sequential()\nmodel2.add(layers.Input((150,150,3)))\n\n# Augmentation Layers\nmodel2.add(layers.RandomFlip('vertical'))\nmodel2.add(layers.RandomRotation(factor=0.2))\n\n\n# Convolutional layers\nmodel2.add(layers.Conv2D(32, (3, 3), activation='relu'))\nmodel2.add(layers.MaxPooling2D(pool_size=(3, 3)))\n\nmodel2.add(layers.Conv2D(32, (3, 3), activation='relu'))\nmodel2.add(layers.MaxPooling2D(pool_size=(3, 3)))\n\nmodel2.add(layers.Conv2D(32, (3, 3), activation='relu'))\nmodel2.add(layers.MaxPooling2D(pool_size=(3, 3)))\n\nmodel2.add(layers.Conv2D(32,(3,3), activation = 'relu'))\n# Dropout layer to prevent overfitting\nmodel2.add(layers.Dropout(0.1))\n\n# Flatten layer to convert 2D feature maps to a vector\nmodel2.add(layers.Flatten())\n\n# Fully connected layers\nmodel2.add(layers.Dense(64, activation='relu'))\n\n# Output layer\nmodel2.add(layers.Dense(2))\n\n# Compile the model\nmodel2.compile(optimizer=Adam(learning_rate=0.0001), loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])\n\n# Print the model summary\nmodel2.summary()\n\n\nModel: \"sequential_3\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n random_flip_1 (RandomFlip)  (None, 150, 150, 3)       0         \n                                                                 \n random_rotation_1 (RandomR  (None, 150, 150, 3)       0         \n otation)                                                        \n                                                                 \n conv2d_2 (Conv2D)           (None, 148, 148, 32)      896       \n                                                                 \n max_pooling2d_2 (MaxPoolin  (None, 49, 49, 32)        0         \n g2D)                                                            \n                                                                 \n conv2d_3 (Conv2D)           (None, 47, 47, 32)        9248      \n                                                                 \n max_pooling2d_3 (MaxPoolin  (None, 15, 15, 32)        0         \n g2D)                                                            \n                                                                 \n conv2d_4 (Conv2D)           (None, 13, 13, 32)        9248      \n                                                                 \n max_pooling2d_4 (MaxPoolin  (None, 4, 4, 32)          0         \n g2D)                                                            \n                                                                 \n conv2d_5 (Conv2D)           (None, 2, 2, 32)          9248      \n                                                                 \n dropout_1 (Dropout)         (None, 2, 2, 32)          0         \n                                                                 \n flatten_1 (Flatten)         (None, 128)               0         \n                                                                 \n dense_2 (Dense)             (None, 64)                8256      \n                                                                 \n dense_3 (Dense)             (None, 2)                 130       \n                                                                 \n=================================================================\nTotal params: 37026 (144.63 KB)\nTrainable params: 37026 (144.63 KB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\n\n\nThe main difference in between this model and the first one we created is in the augmentation layers we have been experimenting with. These are introduced at the beginning of the model, just after we define the input shape. Other than that we are essentially just building upon our previous model, with a couple key exceptions. When looking at our output Dense layer, our optimizer is slightly different than before, as we have Adam(learning_rate=0.0001) instead of just ‘adam’. As one may expect, Adam and ‘adam’ refer to the same optimizer, however we are now customizing it a bit by passing a specific learning_rate. A lower learning rate puts less emphasis on each sample and epoch, although too small a learning rate would lead to a model that learns very slowly. On the other hand, should we have a learning rate that is too high, we can experience severe oscillations within different epochs as there is a larger emphasis placed on each sample. It is important to find a good medium, and in our case, 0.0001 worked well for us to achieve our desired accuracy.\n\n# Train model2 here\nhistory2 = model2.fit(train_ds,\n                     epochs=20,\n                     validation_data=validation_ds)\n\nEpoch 1/20\n146/146 [==============================] - 9s 36ms/step - loss: 1.5543 - accuracy: 0.5122 - val_loss: 0.7863 - val_accuracy: 0.5606\nEpoch 2/20\n146/146 [==============================] - 5s 32ms/step - loss: 0.7831 - accuracy: 0.5365 - val_loss: 0.6995 - val_accuracy: 0.5675\nEpoch 3/20\n146/146 [==============================] - 5s 32ms/step - loss: 0.7079 - accuracy: 0.5623 - val_loss: 0.6832 - val_accuracy: 0.5821\nEpoch 4/20\n146/146 [==============================] - 5s 31ms/step - loss: 0.6893 - accuracy: 0.5797 - val_loss: 0.6741 - val_accuracy: 0.5890\nEpoch 5/20\n146/146 [==============================] - 5s 33ms/step - loss: 0.6779 - accuracy: 0.5850 - val_loss: 0.6700 - val_accuracy: 0.5894\nEpoch 6/20\n146/146 [==============================] - 5s 32ms/step - loss: 0.6693 - accuracy: 0.5944 - val_loss: 0.6624 - val_accuracy: 0.5899\nEpoch 7/20\n146/146 [==============================] - 5s 32ms/step - loss: 0.6560 - accuracy: 0.6096 - val_loss: 0.6529 - val_accuracy: 0.6083\nEpoch 8/20\n146/146 [==============================] - 5s 32ms/step - loss: 0.6540 - accuracy: 0.6174 - val_loss: 0.6365 - val_accuracy: 0.6285\nEpoch 9/20\n146/146 [==============================] - 5s 32ms/step - loss: 0.6468 - accuracy: 0.6308 - val_loss: 0.6240 - val_accuracy: 0.6470\nEpoch 10/20\n146/146 [==============================] - 5s 32ms/step - loss: 0.6400 - accuracy: 0.6368 - val_loss: 0.6088 - val_accuracy: 0.6578\nEpoch 11/20\n146/146 [==============================] - 5s 32ms/step - loss: 0.6341 - accuracy: 0.6440 - val_loss: 0.5981 - val_accuracy: 0.6819\nEpoch 12/20\n146/146 [==============================] - 5s 32ms/step - loss: 0.6229 - accuracy: 0.6545 - val_loss: 0.5960 - val_accuracy: 0.6844\nEpoch 13/20\n146/146 [==============================] - 5s 33ms/step - loss: 0.6114 - accuracy: 0.6688 - val_loss: 0.5796 - val_accuracy: 0.6935\nEpoch 14/20\n146/146 [==============================] - 5s 32ms/step - loss: 0.6046 - accuracy: 0.6777 - val_loss: 0.5797 - val_accuracy: 0.6995\nEpoch 15/20\n146/146 [==============================] - 5s 33ms/step - loss: 0.5983 - accuracy: 0.6807 - val_loss: 0.5790 - val_accuracy: 0.6896\nEpoch 16/20\n146/146 [==============================] - 5s 32ms/step - loss: 0.5974 - accuracy: 0.6828 - val_loss: 0.5784 - val_accuracy: 0.6965\nEpoch 17/20\n146/146 [==============================] - 5s 32ms/step - loss: 0.5859 - accuracy: 0.6897 - val_loss: 0.5819 - val_accuracy: 0.6960\nEpoch 18/20\n146/146 [==============================] - 5s 33ms/step - loss: 0.5858 - accuracy: 0.6923 - val_loss: 0.5620 - val_accuracy: 0.7085\nEpoch 19/20\n146/146 [==============================] - 5s 32ms/step - loss: 0.5868 - accuracy: 0.6925 - val_loss: 0.5531 - val_accuracy: 0.7154\nEpoch 20/20\n146/146 [==============================] - 5s 33ms/step - loss: 0.5718 - accuracy: 0.7038 - val_loss: 0.5589 - val_accuracy: 0.7158\n\n\nWe now will train our newly created model2 on our training dataset. We again want to save our training information so that we can analyze it afterwards. Similar to last time, we initiate training with the fit() method, passing train_ds as an argument representing the dataset to train over, as well as the number of epochs we wish to run, and the validation dataset to test on after each dataset, however this time we pass it to model2. We can note that our validation accuracy settled between 68% and 72% by the final epochs. This represents an improvement over our first model which we calculated to have an accuracy of between 59% and 63%. With regards to overfitting, there is not nearly as large of an issue as there was in our first model. Taking a look at the graph depicting training and validation accuracy, we can see that they have a similar trend and remain relatively close to each other throughout the epochs. Note that the graph being referred to is displayed just below.\n\n# Visualize training history for model2\nplt.plot(history2.history['accuracy'], label = \"training\")\nplt.plot(history2.history['val_accuracy'], label = \"validation\")\nplt.gca().set(xlabel=\"epoch\", ylabel=\"accuracy\")\nplt.legend()\n\n\n\n\n\n\n\n\nWe run the same code we did last time to plot our training and validation accuracy, except this time calling history2 instead of history. As we can observe, our validation and training accuracies move much more in unison than they did in the previous model."
  },
  {
    "objectID": "posts/HW5/index.html#third-model",
    "href": "posts/HW5/index.html#third-model",
    "title": "HW 5: Image Classification with Keras",
    "section": "Third Model",
    "text": "Third Model\n\ni = keras.Input(shape=(150, 150, 3))\n# The pixel values have the range of (0, 255), but many models will work better if rescaled to (-1, 1.)\n# outputs: `(inputs * scale) + offset`\nscale_layer = keras.layers.Rescaling(scale=1 / 127.5, offset=-1)\nx = scale_layer(i)\npreprocessor = keras.Model(inputs = i, outputs = x)\n\nHere we are defining a preprocessor that modifies the data before running it through the rest of the model. In this case scenario, we are changing the pixel values from a scale of (0,255), to a scale of (-1,1). Often, models work better when working with this kind of scale. We can see how this is achieved as each shape is rescaled by dividing by 127.5, which puts all pixels in the range of (0,2), and then we offset by -1 to shift the range to (-1,1).\n\n# Create a sequential model\nmodel3 = keras.Sequential()\nmodel3.add(layers.Input((150,150,3)))\n\n# Augmentation Layers\nmodel3.add(layers.RandomFlip('vertical'))\nmodel3.add(layers.RandomRotation(factor=0.2))\n\n# Adding preprocessor layer defined above\nmodel3.add(preprocessor)\n\n# Convolutional layers\nmodel3.add(layers.Conv2D(32, (3, 3), activation='relu'))\nmodel3.add(layers.MaxPooling2D(pool_size=(3, 3)))\n\nmodel3.add(layers.Conv2D(32, (3, 3), activation='relu'))\nmodel3.add(layers.MaxPooling2D(pool_size=(3, 3)))\n\nmodel3.add(layers.Conv2D(32, (3, 3), activation='relu'))\nmodel3.add(layers.MaxPooling2D(pool_size=(3, 3)))\n\nmodel3.add(layers.Conv2D(32, (3,3), activation = 'relu'))\n\n# Dropout layer to prevent overfitting\nmodel3.add(layers.Dropout(0.1))\n\n# Flatten layer to convert 2D feature maps to a vector\nmodel3.add(layers.Flatten())\n\n# Fully connected layers\nmodel3.add(layers.Dense(64, activation='relu'))\n\n\n# Output layer with binary classification (sigmoid activation for binary classification)\nmodel3.add(layers.Dense(2))\n\n# Compile the model\nmodel3.compile(optimizer='adam', loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])\n\n# Print the model summary\nmodel3.summary()\n\n\nModel: \"sequential_4\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n random_flip_2 (RandomFlip)  (None, 150, 150, 3)       0         \n                                                                 \n random_rotation_2 (RandomR  (None, 150, 150, 3)       0         \n otation)                                                        \n                                                                 \n model (Functional)          (None, 150, 150, 3)       0         \n                                                                 \n conv2d_6 (Conv2D)           (None, 148, 148, 32)      896       \n                                                                 \n max_pooling2d_5 (MaxPoolin  (None, 49, 49, 32)        0         \n g2D)                                                            \n                                                                 \n conv2d_7 (Conv2D)           (None, 47, 47, 32)        9248      \n                                                                 \n max_pooling2d_6 (MaxPoolin  (None, 15, 15, 32)        0         \n g2D)                                                            \n                                                                 \n conv2d_8 (Conv2D)           (None, 13, 13, 32)        9248      \n                                                                 \n max_pooling2d_7 (MaxPoolin  (None, 4, 4, 32)          0         \n g2D)                                                            \n                                                                 \n conv2d_9 (Conv2D)           (None, 2, 2, 32)          9248      \n                                                                 \n dropout_2 (Dropout)         (None, 2, 2, 32)          0         \n                                                                 \n flatten_2 (Flatten)         (None, 128)               0         \n                                                                 \n dense_4 (Dense)             (None, 64)                8256      \n                                                                 \n dense_5 (Dense)             (None, 2)                 130       \n                                                                 \n=================================================================\nTotal params: 37026 (144.63 KB)\nTrainable params: 37026 (144.63 KB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\n\n\nThe main difference between this model and model2 is that we are including the preprocessor we defined above. It is important to note that we are still including the data augmentation layers and have placed those before the preprocessor. This is because the job that the augmentation play is to slightly vary the data, which should then be passed to the preprocessor so it can take the varied data and apply the scale change to it. Looking closely we can see that we have applied the ‘vertical’ argument to the RandomFlip() layer, which we learned earlier allows RandomFlip() to flip layers vertically but not horizontally. Other than that, all of the key elements have remained the same from the previous model, so we are counting on the preprocessor to improve the results of our model.\n\n# Train model3 here\nhistory3 = model3.fit(train_ds,\n                     epochs=20,\n                     validation_data=validation_ds)\n\nEpoch 1/20\n146/146 [==============================] - 7s 34ms/step - loss: 0.6631 - accuracy: 0.6002 - val_loss: 0.6185 - val_accuracy: 0.6612\nEpoch 2/20\n146/146 [==============================] - 5s 34ms/step - loss: 0.6265 - accuracy: 0.6460 - val_loss: 0.5861 - val_accuracy: 0.7068\nEpoch 3/20\n146/146 [==============================] - 5s 32ms/step - loss: 0.5859 - accuracy: 0.6903 - val_loss: 0.5503 - val_accuracy: 0.7283\nEpoch 4/20\n146/146 [==============================] - 5s 33ms/step - loss: 0.5621 - accuracy: 0.7122 - val_loss: 0.5401 - val_accuracy: 0.7322\nEpoch 5/20\n146/146 [==============================] - 5s 33ms/step - loss: 0.5364 - accuracy: 0.7367 - val_loss: 0.5216 - val_accuracy: 0.7472\nEpoch 6/20\n146/146 [==============================] - 5s 32ms/step - loss: 0.5275 - accuracy: 0.7348 - val_loss: 0.5263 - val_accuracy: 0.7459\nEpoch 7/20\n146/146 [==============================] - 5s 33ms/step - loss: 0.5058 - accuracy: 0.7543 - val_loss: 0.4944 - val_accuracy: 0.7670\nEpoch 8/20\n146/146 [==============================] - 5s 32ms/step - loss: 0.5059 - accuracy: 0.7562 - val_loss: 0.4947 - val_accuracy: 0.7653\nEpoch 9/20\n146/146 [==============================] - 5s 32ms/step - loss: 0.4926 - accuracy: 0.7639 - val_loss: 0.4986 - val_accuracy: 0.7605\nEpoch 10/20\n146/146 [==============================] - 5s 33ms/step - loss: 0.4827 - accuracy: 0.7673 - val_loss: 0.4855 - val_accuracy: 0.7691\nEpoch 11/20\n146/146 [==============================] - 5s 31ms/step - loss: 0.4756 - accuracy: 0.7702 - val_loss: 0.4638 - val_accuracy: 0.7743\nEpoch 12/20\n146/146 [==============================] - 5s 33ms/step - loss: 0.4727 - accuracy: 0.7761 - val_loss: 0.4475 - val_accuracy: 0.7919\nEpoch 13/20\n146/146 [==============================] - 5s 31ms/step - loss: 0.4586 - accuracy: 0.7836 - val_loss: 0.4548 - val_accuracy: 0.7876\nEpoch 14/20\n146/146 [==============================] - 5s 31ms/step - loss: 0.4562 - accuracy: 0.7881 - val_loss: 0.4528 - val_accuracy: 0.7923\nEpoch 15/20\n146/146 [==============================] - 5s 33ms/step - loss: 0.4468 - accuracy: 0.7903 - val_loss: 0.4528 - val_accuracy: 0.7928\nEpoch 16/20\n146/146 [==============================] - 5s 31ms/step - loss: 0.4433 - accuracy: 0.7881 - val_loss: 0.4387 - val_accuracy: 0.8035\nEpoch 17/20\n146/146 [==============================] - 5s 33ms/step - loss: 0.4356 - accuracy: 0.8008 - val_loss: 0.4383 - val_accuracy: 0.7984\nEpoch 18/20\n146/146 [==============================] - 5s 31ms/step - loss: 0.4286 - accuracy: 0.8014 - val_loss: 0.4258 - val_accuracy: 0.8031\nEpoch 19/20\n146/146 [==============================] - 5s 32ms/step - loss: 0.4287 - accuracy: 0.8054 - val_loss: 0.4267 - val_accuracy: 0.8100\nEpoch 20/20\n146/146 [==============================] - 5s 31ms/step - loss: 0.4171 - accuracy: 0.8059 - val_loss: 0.4229 - val_accuracy: 0.7962\n\n\nAfter training model3, we can see that our preprocessor did in fact make a difference. After fitting the model on our training dataset as we did previously and saving the information to history3, we see an improvement in accuracy. For this model, our validation accuracy settled between 80% and 82% by the final epochs. This represents an even more drastic improvement over our first model which we calculated to have an accuracy of between 57% and 60%. With regards to overfitting, there is not nearly as large of an issue as there was in our first model. When looking at the graph depicting training and validation accuracy, we note that they remain very close to each other throughout the training, even closer than our second model was. Note that the graph being referred to is displayed just below.\n\n# Visualize training history for model2\nplt.plot(history3.history['accuracy'], label=\"training\")\nplt.plot(history3.history['val_accuracy'], label=\"validation\")\nplt.gca().set(xlabel=\"epoch\", ylabel=\"accuracy\")\nplt.legend()\n\n\n\n\n\n\n\n\nAgain we are using saved information in history3 to visualize our training history. We can see that in this case our validation and training accuracy appear to be even closer than they have been before,"
  },
  {
    "objectID": "posts/HW5/index.html#fourth-model",
    "href": "posts/HW5/index.html#fourth-model",
    "title": "HW 5: Image Classification with Keras",
    "section": "Fourth Model",
    "text": "Fourth Model\n\nIMG_SHAPE = (150, 150, 3)\nbase_model = keras.applications.MobileNetV3Large(input_shape=IMG_SHAPE,\n                                               include_top=False,\n                                               weights='imagenet')\nbase_model.trainable = False\n\ni = keras.Input(shape=IMG_SHAPE)\nx = base_model(i, training = False)\nbase_model_layer = keras.Model(inputs = i, outputs = x)\n\nWARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not 224. Weights for input shape (224, 224) will be loaded as the default.\n\n\nDownloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v3/weights_mobilenet_v3_large_224_1.0_float_no_top_v2.h5\n12683000/12683000 [==============================] - 1s 0us/step\n\n\nIn this model we plan on implementing a pre-existing model to see if it can improve our accuracy. Above we are reading in the model, titled base_model, from keras, and passing it the shape of our images via the variable IMG_SHAPE. We will then implement this base layer into our model.\n\n\n\n# Create a sequential model\nmodel4 = keras.Sequential()\nmodel4.add(layers.Input((150,150,3)))\n\n\n# Augmentation Layers\nmodel4.add(layers.RandomFlip())\nmodel4.add(layers.RandomRotation(factor=10.0))\n\n# Adding layer defined above\nmodel4.add(base_model_layer)\n\n#Flatten\nmodel4.add(layers.Flatten())\n\n# Output layer with binary classification (sigmoid activation for binary classification)\nmodel4.add(layers.Dense(2))\n\n# Compile the model\nmodel4.compile(optimizer=Adam(learning_rate=0.0001), loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])\n\n# Print the model summary\nmodel4.summary()\n\n\nModel: \"sequential_5\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n random_flip_3 (RandomFlip)  (None, 150, 150, 3)       0         \n                                                                 \n random_rotation_3 (RandomR  (None, 150, 150, 3)       0         \n otation)                                                        \n                                                                 \n model_1 (Functional)        (None, 5, 5, 960)         2996352   \n                                                                 \n flatten_3 (Flatten)         (None, 24000)             0         \n                                                                 \n dense_6 (Dense)             (None, 2)                 48002     \n                                                                 \n=================================================================\nTotal params: 3044354 (11.61 MB)\nTrainable params: 48002 (187.51 KB)\nNon-trainable params: 2996352 (11.43 MB)\n_________________________________________________________________\n\n\nAs we can observem, this model appears to be much simpler than the previous models we created. Just by looking at the summary we would assume so as it is simply much shorter than the other ones. However we must take into account the fact that we have an entirely separate model built into this one we just created. While we have not included any Conv2D or Pooling2D layers in this model, it is important to note that we have still included our augmentation layers, as well as a Flatten layer followed by a Dense layer to give us our output. You may be wondering why we decided not to include our preprocessor that we introduced in the previous model. The reason is that the base_model_layer that we just added includes its own preprocessing layers, eliminating the need for us to introduce more. Now that we have created our model4, we can train it and see what kind of improvements this new model will experience.\n\n# Train model4 here\nhistory4 = model4.fit(train_ds,\n                     epochs=20,\n                     validation_data=validation_ds)\n\nEpoch 1/20\n146/146 [==============================] - 16s 66ms/step - loss: 0.5881 - accuracy: 0.8287 - val_loss: 0.1966 - val_accuracy: 0.9415\nEpoch 2/20\n146/146 [==============================] - 7s 45ms/step - loss: 0.3310 - accuracy: 0.9026 - val_loss: 0.1872 - val_accuracy: 0.9441\nEpoch 3/20\n146/146 [==============================] - 7s 46ms/step - loss: 0.2897 - accuracy: 0.9104 - val_loss: 0.1750 - val_accuracy: 0.9531\nEpoch 4/20\n146/146 [==============================] - 6s 44ms/step - loss: 0.2409 - accuracy: 0.9208 - val_loss: 0.1569 - val_accuracy: 0.9540\nEpoch 5/20\n146/146 [==============================] - 7s 45ms/step - loss: 0.2417 - accuracy: 0.9260 - val_loss: 0.1402 - val_accuracy: 0.9570\nEpoch 6/20\n146/146 [==============================] - 6s 44ms/step - loss: 0.2258 - accuracy: 0.9289 - val_loss: 0.1669 - val_accuracy: 0.9544\nEpoch 7/20\n146/146 [==============================] - 7s 45ms/step - loss: 0.2182 - accuracy: 0.9290 - val_loss: 0.1505 - val_accuracy: 0.9557\nEpoch 8/20\n146/146 [==============================] - 7s 48ms/step - loss: 0.2030 - accuracy: 0.9349 - val_loss: 0.1472 - val_accuracy: 0.9549\nEpoch 9/20\n146/146 [==============================] - 7s 45ms/step - loss: 0.1871 - accuracy: 0.9391 - val_loss: 0.1539 - val_accuracy: 0.9592\nEpoch 10/20\n146/146 [==============================] - 7s 46ms/step - loss: 0.1862 - accuracy: 0.9392 - val_loss: 0.1547 - val_accuracy: 0.9531\nEpoch 11/20\n146/146 [==============================] - 7s 45ms/step - loss: 0.1853 - accuracy: 0.9372 - val_loss: 0.1480 - val_accuracy: 0.9587\nEpoch 12/20\n146/146 [==============================] - 7s 45ms/step - loss: 0.1834 - accuracy: 0.9396 - val_loss: 0.1209 - val_accuracy: 0.9617\nEpoch 13/20\n146/146 [==============================] - 7s 45ms/step - loss: 0.1783 - accuracy: 0.9434 - val_loss: 0.1206 - val_accuracy: 0.9609\nEpoch 14/20\n146/146 [==============================] - 6s 44ms/step - loss: 0.1604 - accuracy: 0.9467 - val_loss: 0.1468 - val_accuracy: 0.9549\nEpoch 15/20\n146/146 [==============================] - 7s 46ms/step - loss: 0.1656 - accuracy: 0.9412 - val_loss: 0.1350 - val_accuracy: 0.9622\nEpoch 16/20\n146/146 [==============================] - 7s 46ms/step - loss: 0.1714 - accuracy: 0.9436 - val_loss: 0.1314 - val_accuracy: 0.9596\nEpoch 17/20\n146/146 [==============================] - 7s 46ms/step - loss: 0.1507 - accuracy: 0.9470 - val_loss: 0.1344 - val_accuracy: 0.9635\nEpoch 18/20\n146/146 [==============================] - 7s 45ms/step - loss: 0.1532 - accuracy: 0.9485 - val_loss: 0.1214 - val_accuracy: 0.9622\nEpoch 19/20\n146/146 [==============================] - 7s 45ms/step - loss: 0.1498 - accuracy: 0.9473 - val_loss: 0.1260 - val_accuracy: 0.9596\nEpoch 20/20\n146/146 [==============================] - 7s 45ms/step - loss: 0.1484 - accuracy: 0.9495 - val_loss: 0.1409 - val_accuracy: 0.9630\n\n\nAfter again fitting the model on our training dataset as we did previously and saving the information, we see a drastic improvement in our accuracy, largely due to the new model that we built off of. For this model, our validation accuracy settled between 94% and 96% by the final epochs. This represents a massive drastic improvement over our first model which we calculated to have an accuracy of between 57% and 60%. In fact, this is also a massive improvement over our second model, as we jumped up from around 80%. With regards to overfitting, we can see that the validation in fact experiences much higher accuracy rates than the training sets did. This is not typically a sign of overfitting, as for an overfit model normally the opposite is true. Note that the graph being referred to is displayed just below.\n\n# Visualize training history for model4\nplt.plot(history4.history['accuracy'], label=\"training\")\nplt.plot(history4.history['val_accuracy'], label=\"validation\")\nplt.gca().set(xlabel=\"epoch\", ylabel=\"accuracy\")\nplt.legend()\n\n\n\n\n\n\n\n\nAfter again graphing the training and validation accuracies, we can see that this most recent model has had by far the most success in predicting dogs and cats, peaking at around 94% success for training and 96% for validation. It is somewhat curious that the validation has a higher accuracy rate than the training does, as typically it is the other way around."
  },
  {
    "objectID": "posts/HW5/index.html#final-model",
    "href": "posts/HW5/index.html#final-model",
    "title": "HW 5: Image Classification with Keras",
    "section": "Final Model",
    "text": "Final Model\n\n#Create model and test against test dataset\nfinalModel = keras.Sequential()\n\nfinalModel.add(layers.Input((150,150,3)))\n\n\n# Augmentation Layers\nfinalModel.add(layers.RandomFlip())\nfinalModel.add(layers.RandomRotation(factor=0.2))\n\n# Adding layer defined above\nfinalModel.add(base_model_layer)\n\n#Adding convolution layer\nfinalModel.add(layers.GlobalMaxPooling2D())\n#finalModel.add(layers.Dropout(0.1))\n\n#Flatten\nfinalModel.add(layers.Flatten())\n\n# Output layer with binary classification (sigmoid activation for binary classification)\nfinalModel.add(layers.Dense(2))\n\n# Compile the model\nfinalModel.compile(optimizer=Adam(learning_rate=0.0001), loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])\n\n# Print the model summary\nfinalModel.summary()\n\nModel: \"sequential_6\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n random_flip_4 (RandomFlip)  (None, 150, 150, 3)       0         \n                                                                 \n random_rotation_4 (RandomR  (None, 150, 150, 3)       0         \n otation)                                                        \n                                                                 \n model_1 (Functional)        (None, 5, 5, 960)         2996352   \n                                                                 \n global_max_pooling2d (Glob  (None, 960)               0         \n alMaxPooling2D)                                                 \n                                                                 \n flatten_4 (Flatten)         (None, 960)               0         \n                                                                 \n dense_7 (Dense)             (None, 2)                 1922      \n                                                                 \n=================================================================\nTotal params: 2998274 (11.44 MB)\nTrainable params: 1922 (7.51 KB)\nNon-trainable params: 2996352 (11.43 MB)\n_________________________________________________________________\n\n\nFor our final model we are trying to get as accurate as possible. Seeing as our most recent model had an accuracy of around 95%, it does not make sense to deviate too much from that model. Hence, this model is very reminiscent of our previous one, with a couple of tweaks. After our base_model_layer, we have added a convolution layer, GlobalMaxPooling2D, and then from there it remains the same as before. Time to see how well our new model performs.\n\n# Train finalModel here\nfinalHistory = finalModel.fit(train_ds,\n                     epochs=20,\n                     validation_data=validation_ds)\n\nEpoch 1/20\n146/146 [==============================] - 12s 56ms/step - loss: 3.8295 - accuracy: 0.6091 - val_loss: 2.4163 - val_accuracy: 0.6896\nEpoch 2/20\n146/146 [==============================] - 7s 46ms/step - loss: 1.8335 - accuracy: 0.7530 - val_loss: 1.2446 - val_accuracy: 0.8328\nEpoch 3/20\n146/146 [==============================] - 6s 44ms/step - loss: 1.2931 - accuracy: 0.8121 - val_loss: 0.8283 - val_accuracy: 0.8831\nEpoch 4/20\n146/146 [==============================] - 6s 44ms/step - loss: 1.0553 - accuracy: 0.8497 - val_loss: 0.6857 - val_accuracy: 0.9003\nEpoch 5/20\n146/146 [==============================] - 7s 45ms/step - loss: 0.9403 - accuracy: 0.8609 - val_loss: 0.5691 - val_accuracy: 0.9132\nEpoch 6/20\n146/146 [==============================] - 6s 44ms/step - loss: 0.8372 - accuracy: 0.8725 - val_loss: 0.5087 - val_accuracy: 0.9187\nEpoch 7/20\n146/146 [==============================] - 6s 44ms/step - loss: 0.8271 - accuracy: 0.8728 - val_loss: 0.4742 - val_accuracy: 0.9218\nEpoch 8/20\n146/146 [==============================] - 7s 45ms/step - loss: 0.7453 - accuracy: 0.8826 - val_loss: 0.4405 - val_accuracy: 0.9256\nEpoch 9/20\n146/146 [==============================] - 7s 46ms/step - loss: 0.6927 - accuracy: 0.8883 - val_loss: 0.3828 - val_accuracy: 0.9342\nEpoch 10/20\n146/146 [==============================] - 6s 44ms/step - loss: 0.6712 - accuracy: 0.8879 - val_loss: 0.3686 - val_accuracy: 0.9342\nEpoch 11/20\n146/146 [==============================] - 7s 45ms/step - loss: 0.6137 - accuracy: 0.8960 - val_loss: 0.3487 - val_accuracy: 0.9372\nEpoch 12/20\n146/146 [==============================] - 7s 46ms/step - loss: 0.6280 - accuracy: 0.8953 - val_loss: 0.3209 - val_accuracy: 0.9407\nEpoch 13/20\n146/146 [==============================] - 6s 45ms/step - loss: 0.6124 - accuracy: 0.8970 - val_loss: 0.3180 - val_accuracy: 0.9381\nEpoch 14/20\n146/146 [==============================] - 7s 46ms/step - loss: 0.5795 - accuracy: 0.8993 - val_loss: 0.2963 - val_accuracy: 0.9420\nEpoch 15/20\n146/146 [==============================] - 6s 44ms/step - loss: 0.4950 - accuracy: 0.9066 - val_loss: 0.2871 - val_accuracy: 0.9441\nEpoch 16/20\n146/146 [==============================] - 7s 45ms/step - loss: 0.5141 - accuracy: 0.9017 - val_loss: 0.2713 - val_accuracy: 0.9458\nEpoch 17/20\n146/146 [==============================] - 7s 46ms/step - loss: 0.4813 - accuracy: 0.9088 - val_loss: 0.2583 - val_accuracy: 0.9480\nEpoch 18/20\n146/146 [==============================] - 6s 44ms/step - loss: 0.4964 - accuracy: 0.9070 - val_loss: 0.2568 - val_accuracy: 0.9450\nEpoch 19/20\n146/146 [==============================] - 6s 44ms/step - loss: 0.4581 - accuracy: 0.9128 - val_loss: 0.2860 - val_accuracy: 0.9411\nEpoch 20/20\n146/146 [==============================] - 7s 45ms/step - loss: 0.4521 - accuracy: 0.9111 - val_loss: 0.2522 - val_accuracy: 0.9428\n\n\nWe can see that this model again experienced exceptional accuracy, settling at around 95% for the validation sets. However this time we are going to go a step further and test it on our test_ds dataset that we are yet to use.\n\n# Visualize training history for finalModel\nplt.plot(finalHistory.history['accuracy'], label=\"training\")\nplt.plot(finalHistory.history['val_accuracy'], label=\"validation\")\nplt.gca().set(xlabel=\"epoch\", ylabel=\"accuracy\")\nplt.legend()\n\n\n\n\n\n\n\n\nAfter again plotting validatin and training accuracy throughout the training process, it makes sense that this graph appears similar to that of our previous model, as the two models are very similar in nature.\n\nfinalModel.evaluate(test_ds)\n\n37/37 [==============================] - 3s 77ms/step - loss: 0.3496 - accuracy: 0.9450\n\n\n[0.349590927362442, 0.944969892501831]\n\n\nTo test our model on our test dataset, we call then method evaluate() and pass it test_ds, so that it knows what to test our model on. We can see that we achieve an accuracy of just over 94%, a very respectable result indeed."
  },
  {
    "objectID": "posts/HW4/index.html",
    "href": "posts/HW4/index.html",
    "title": "HW 4: Heat diffusion with Jax",
    "section": "",
    "text": "Today we will be simulating two dimensional heat diffusion using tools such as numpy and jax. The goal of this project is to get us familiar with some jax functions, as well as the benefits and drawbacks of using jax. Let us begin!"
  },
  {
    "objectID": "posts/HW4/index.html#introduction",
    "href": "posts/HW4/index.html#introduction",
    "title": "HW 4: Heat diffusion with Jax",
    "section": "",
    "text": "Today we will be simulating two dimensional heat diffusion using tools such as numpy and jax. The goal of this project is to get us familiar with some jax functions, as well as the benefits and drawbacks of using jax. Let us begin!"
  },
  {
    "objectID": "posts/HW4/index.html#imports",
    "href": "posts/HW4/index.html#imports",
    "title": "HW 4: Heat diffusion with Jax",
    "section": "Imports",
    "text": "Imports\n\nimport jax\nimport numpy as np\nimport time\nfrom matplotlib import pyplot as plt\nfrom jax.experimental import sparse\nfrom jax import jit\nimport jax.numpy as jnp\n#importing the equations from our heat_equations file\n%run heat_equations.ipynb\n#setting our values to be used in this example\nN = 101\nepsilon = 0.2\n\nThis block contains the necessary imports needed to run our project. We can see we are calling for heat_equations.ipynb to run. This will allow us to access the functions we define within that file so that we can test their functionality within this notebook. Note that in this blog post, all functions have been placed within the same notebook to make the post easier to understand. We then define both N and epsilon to values that we will be using throughout this example, in this instance N = 101 and epsilon = 0.2. Epsilon is used in determining the magnitude of change between each timestep and N represents the dimensions of our grid state."
  },
  {
    "objectID": "posts/HW4/index.html#getting-the-matrix-a",
    "href": "posts/HW4/index.html#getting-the-matrix-a",
    "title": "HW 4: Heat diffusion with Jax",
    "section": "Getting the Matrix A",
    "text": "Getting the Matrix A\n\ndef get_A(N):\n    '''Defines our transition operator A given value N\n    Args:\n        N: Size of our vector u0 to be used.\n    Returns:\n        A: The NxN transition matrix that would be used for \n        a vector of size N\n    '''\n    \n    #define n to be N*N to be used as length of A\n    n = N * N\n    #Setting the diagonals to be -4 and 1 where necessary\n    diagonals = [-4 * np.ones(n), np.ones(n-1), np.ones(n-1), np.ones(n-N), np.ones(n-N)]\n    #setting all other values to zero\n    diagonals[1][(N-1)::N] = 0\n    diagonals[2][(N-1)::N] = 0\n    #combining diagonals to create matrix A\n    A = np.diag(diagonals[0]) + np.diag(diagonals[1], 1) + np.diag(diagonals[2], -1) + np.diag(diagonals[3], N) + np.diag(diagonals[4], -N)\n    return A\n\nThe purpose of this function is to create our transition operator, A, given the desired dimensions N. We know that our matrix A is going to be of size (N2, N2), so in order to make our syntax clearer, we can define n = N * N. The transition matrix follows a very distinct format for our model, which makes it easier to define for arbitrary sizes. It is made up of a diagonal full of -4, along with four other diagonals filled with ones. In order to implement this into our matrix, we are going to create arrays titled diagonals that will be filled with our desired values, as seen in the diagonals list above. As we can see, we are creating numpy arrays full of ones, with the exception of the first being full of -4 due to the scalar multiplication. As we know that all other values are going to be equal to zero, we can use these diagonal arrays, and call np.diag to input these arrays as a diagonal portion of our matrix A. After that, our matrix A is completed and ready to be returned by this function."
  },
  {
    "objectID": "posts/HW4/index.html#advance-time-using-matrix-multiplication",
    "href": "posts/HW4/index.html#advance-time-using-matrix-multiplication",
    "title": "HW 4: Heat diffusion with Jax",
    "section": "Advance Time Using Matrix Multiplication",
    "text": "Advance Time Using Matrix Multiplication\n\ndef advance_time_matvecmul(A, u, epsilon):\n    \"\"\"Advances the simulation by one timestep, via matrix-vector multiplication\n    Args:\n        A: The 2d finite difference matrix\n        u: N x N grid state at timestep k\n        epsilon: stability constant\n\n    Returns:\n        N x N Grid state at timestep k+1\n    \"\"\"\n    \n    #update our u to the next time step using matrix multiplication\n    u = u + epsilon * (A @ u.flatten()).reshape((u.shape[0], u.shape[0]))\n    return u\n\nThis function utilizes matrix multiplication to advance our gridstate one timestep. As we can see, it is passed in three parameters, those being, A, u, and epsilon. The parameter A refers to the transition matrix used to change the current gridstate, represented by u, when moving to the next timestep. The scalar value epsilon plays a role in dictating the size of the change between timesteps. We can see that A is multiplied with u within our definition of our new u, however, u must be flattened before the multiplication takes place, hence the use of the flatten attribute. This is necessary for these arrays to be multiplied together within python. However, we would like for our returned gridstate to be the same shape as the one received, so after multiplicative process, we will use the dimensions of the u that was passed in to reshape our product back to the original dimensions. We then multiply by the scalar epsilon to determine the size of the change to be used, before adding the whole term to our initial u. As the initial u is included as part of a sum within the equation, it is intuitive that epsilon(Au) represents the change between timesteps in the diffusion model. Now that we have defined our u for the next timestep, we can return this value, concluding the functionality of this function."
  },
  {
    "objectID": "posts/HW4/index.html#testing-our-first-function",
    "href": "posts/HW4/index.html#testing-our-first-function",
    "title": "HW 4: Heat diffusion with Jax",
    "section": "Testing our first function",
    "text": "Testing our first function\n\n#defining our matrix A and creating a list to append our results to\nA = get_A(N)\nlistShow = []\n\nHere we are calling the function we defined get_A() in order to set our variable A to an A matrix of size (N,N). We also initialize an empty list in which we can store the progression of our heat map. This will allow us to visualize the progression of our heat diffusion later.\n\n%%timeit -r 1 -n 1\n#setting u0 to initial condition\nu0 = np.zeros((N,N))\nu0[int(N/2), int(N/2)] = 1.0\n#using for loop to update our timestep 2700 times\nfor i in range(1, 2701):\n    #calling function to advance forward one timestep\n    u0 = advance_time_matvecmul(A, u0, epsilon)\n    if i%300 == 0:\n        #appending state every 300 steps and printing timestep for debugging\n        listShow.append([u0,i])\n        print(i)\n\n300\n600\n900\n1200\n1500\n1800\n2100\n2400\n2700\n1min 39s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n\n\nLots of things are happening here. First off, you can see that we are timing this block of code using the timeit function. We set both r and n to 1 as we only want to run and time the block of code once. Next, we initialize our u0 vector to be used within this example. As we have seen before, our initial condition is composed of a singular point of heat in the middle of our grid state. Therefore, our initial condition vector u0 will be entirely zeros with the exception of a 1 in the very middle. This is accomplished by creating an (N,N) numpy array filled with zeros, before setting the middle index of each dimension, found at N/2, and setting it to 1. Now we are ready to run our model. We chose to run it for 2700 timesteps, therefore we will make a loop that runs 2700 times, accomplished above by having our for loop running i from 1 to 2700. Within each loop, we will update the timestep by setting our u0 equal to the advance_time_matvemul function that we pass in our matrix A, our current u0, and our epsilon. While this updates our timestep, every 300 steps we will append our u0 and the i value, which represents the timestep, to the list we initialized in the previous block. This will allow us to visualize the diffusion in the next step. Additionally, every 300 steps I also chose to print out i in order to be able to see the progression of the loop as this loop ends up taking a long time, 1 min and 39 secs to be precise.\n\n#setting dimensions of image display\nrows=3\ncols = 3\n#defining variable to be used in display loop\nimg_count = 0\n#defining our display figure\nfig, axes = plt.subplots(nrows=rows, ncols=cols, figsize=(8,8))\n#looping through each element in our figure, setting each to a different timestep of our model saved in list listShow\nfor i in range(rows):\n    for j in range(cols):        \n        if img_count &lt; len(listShow):\n            #setting each box in display equal to unique image from list\n            axes[i, j].imshow(listShow[img_count][0])\n            #incrementing img_count to let us traverse along the list\n            img_count+=1\n\n\n\n\n\n\n\n\nHere we use the listShow list we appended our vectors to in order to visualize our data. We will start by determining the layout of our model. As we appended information every 300 steps out of 2700 total steps, we can calculate that we have a total of nine timesteps saved, making a (3,3) grid a very reasonable option. Hence, we define our rows and columns to be equal to three. We also define an img_count variable to be equal to zero. This variable will be used to keep track of how many images we have already plotted, helping us to traverse through our list. Now we define our figure itself by creating a plot with three rows and three columns using the row and columns variables we just defined. Now we are ready to fill the plot with our images. In order to traverse through our (3,3) grid, we will create a nested for loop, with the outer loop running along the rows and the inner loop running through each column. Now as long as our variable img_count is less than the length of listShow, we will place the image from the list indexed at img_count into the plot at location (i,j), with i and j being given by the two loops. As the index of the element of the list we wish to be accessing is given by img_show, it is critical that we increment it within the inner for loop. As you can see, the resulting figures show an expanding heat map, as the heat is diffused from the singular initial point outwards, expanding further as we traverse through more timesteps."
  },
  {
    "objectID": "posts/HW4/index.html#getting-sparse-matrix",
    "href": "posts/HW4/index.html#getting-sparse-matrix",
    "title": "HW 4: Heat diffusion with Jax",
    "section": "Getting Sparse Matrix",
    "text": "Getting Sparse Matrix\n\ndef get_sparse_A(N):\n    '''Returns a sparse form of our transition matrix A\n    Args:\n        N: Size of our vector u0 to be used\n    Returns:\n        A_sp_matrix: The sparse form of the NxN transition matrix \n        that would be used for a vector of size N \n    '''\n    \n    #get the matrix A corressponding to the passed value N\n    A = get_A(N)\n    #find sparse version of matrix A and set it to our return value \n    A_sp_matrix = sparse.BCOO.fromdense(A)\n    return A_sp_matrix\n\nThis function is relatively simple, as we have done most of the heavy lifting in our previous function get_A(). Our goal is to take a dimension N and use that to create the sparse version of the transition matrix corresponding to that dimension. A good place to start is by creating the actual transition matrix itself, which we can now easily do by calling our get_A() function, passing in our size N. Now we have to convert this matrix to a sparse matrix. Luckily, the jax.expiremental library has a way to do this via the sparse library. To save our A sparse matrix into A_sp_matrix, we call sparse.BCOO.fromdense() and pass in our matrix A. This method will convert whatever matrix it receives into its sparse form, meaning it only contains information regarding the nonzero elements. Now that we have created our desired sparse matrix, we can return this value and conclude our function."
  },
  {
    "objectID": "posts/HW4/index.html#testing-sparse-matrix",
    "href": "posts/HW4/index.html#testing-sparse-matrix",
    "title": "HW 4: Heat diffusion with Jax",
    "section": "Testing Sparse Matrix",
    "text": "Testing Sparse Matrix\n\n#creating a list in which to append our new results and getting the sparse version of matrix A\nlistShow1=[]\nsp_A = get_sparse_A(N)\n\nHere we are defining another list we will append data to. Additionally, we call the get_sparse_A() in order to get a sparse version of our matrix A that will be passed to our advance_time_matvecmul() function.\n\n%%timeit -r 1 -n 1\n#resetting u0 to initial condition\nu0 = np.zeros((N,N))\nu0[int(N/2), int(N/2)] = 1.0\n#looping to increase timestep a total of 2700 times\nfor t in range(1, 2701):\n    #calling jitted version of function to advance timestep\n    #note that the matrix passed in is a sparse matrix\n    u0 = jit(advance_time_matvecmul)(sp_A, u0, epsilon)\n    if t%300 == 0:\n        #appending timestep information once every 300 steps, printing for debugging purposes\n        print(t)\n        listShow1.append([u0,t])\n\n300\n600\n900\n1200\n1500\n1800\n2100\n2400\n2700\n1.7 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n\n\nThis block of code is nearly identical to the one we used to test our previous function. We are again timing the code block using timeit, again are creating our initial condition within u0, however the loop is slightly different. We are still running for 2700 timesteps, appending data every 300 steps, but we can see that the function we call is slightly different. We have placed the command jit before the function. This creates a jitted version of our advance_time_matvecmul() function, which should in theory make this function run faster. This is accomplished by using just-in-time compilation, meaning it is compiled during runtime as opposed to before execution. Additionally, we can see that instead of passing the A matrix, we are passing the sparse verision of matrix A, denoted sp_A. This should also speed up the process as A was predominantly filled with zeros. Now that it is a sparse matrix, it can ignore multiplication with the zeros, drastically decreasing the time required to run the matrix multiplication within the advance_time_matvecmul() function. As we can see, these changes resulted in a drastic increase in speed, as we are now at 1.7 secs, significantly faster than the previous example.\n\n#defining dimensions of our display figure\nrows=3\ncols = 3\n#resetting variable to be used in display loop\nimg_count = 0\n#defining our display figure\nfig, axes = plt.subplots(nrows=rows, ncols=cols, figsize=(8,8))\n#looping through each element in our figure, setting each to a different timestep of our model, saved in list listShow1\nfor i in range(rows):\n    for j in range(cols):        \n        if img_count &lt; len(listShow1):\n            #sets each box in display to different image in listShow1\n            axes[i, j].imshow(listShow1[img_count][0])\n            #increment img_count to traverse along the list\n            img_count+=1\n\n\n\n\n\n\n\n\nThis visualization process is the same as our previous one, except that we are pulling from listShow1 instead of listShow. However it is important that our visualizations resemble one another, as otherwise it would mean one of the two methods had some mistake. Upon inspection, we can see that these images appear very similar, if not identical, to our previous visualizations."
  },
  {
    "objectID": "posts/HW4/index.html#advancing-time-using-numpy",
    "href": "posts/HW4/index.html#advancing-time-using-numpy",
    "title": "HW 4: Heat diffusion with Jax",
    "section": "Advancing Time Using Numpy",
    "text": "Advancing Time Using Numpy\n\ndef advance_time_numpy(u, epsilon):\n    \"\"\"Advances simulation by one timestep using functions from numpy\n    Args:\n        u: NxN grid state at timestep k\n        epsilon: stability constant\n        \n    Returns: N x N Grid state at timestep k+1\n    \"\"\"\n    #get N so we know the size of matrix we are working with \n    m,N = u.shape\n    #create new matrix of size with two extra rows and columns\n    newU = np.zeros((N+2,N+2,),dtype='float64')\n    #set the center of new matrix equal to u, meaning ring of zeros around passed matrix u\n    newU[1:-1,1:-1] = u\n    #implement changes in x direction\n    u_xx = np.roll(newU, shift=1, axis=0) - 2 * newU + np.roll(newU, shift=-1, axis=0)\n    #implement changes in y direction\n    u_yy = np.roll(newU, shift=1, axis=1) - 2 * newU + np.roll(newU, shift=-1, axis=1)\n    #apply changes and set it equal to a new matrix\n    finU = newU + epsilon * (u_xx + u_yy)\n    #eliminate outer ring of values, allowing the heat to escape, setting equal to return value\n    u_new = finU[1:-1,1:-1]\n    return u_new\n\nThis method will give us a way to advance a timestep without the use of matrix multiplication. As matrix multiplication is expensive from a time standpoint, the hope is that by eliminating the need for it we will speed up our advancement process. The first thing to note about this function is that we are only passed in two parameters, those being our gridstate u and epsilon. Unlike our previous functions, we no longer have the variable N representing the size of our dimensions. Fortunately, we know that our gridstate is of size (N,N), so our first step is to extract size N from the shape of u, shown in the first line of code. We can now create an empty matrix that will be used to adjust our current u, titled newU. You may notice that the new matrix is of different dimensions than our current gridstate, two greater in each dimension in fact. This allows us to have some padding that later in the function we will take advantage of by using it to let heat escape from our model. To use this extra space as padding, we need to place our current gridstate information into the middle of our new matrix. We accomplish this by setting the indexed area in the middle of our new matrix equal to our current gridstate u. Now is when the math comes in. Using the update equation in discrete time, we can see that for a given point, the change is defined by adding heat from the points both to the left and right, as well as the points above and below, followed by a loss of four times the heat from the point itself. In order to implement this change, it will help us to do it in two parts: one part along the x-axis and another along the y-axis. The numpy roll() function will work well here, as it shifts all the elements along a given axis. It is important to note that the numpy roll function will send the last elements back to the beginning of the array, which could cause issues, however we have accounted for this possible issue via the padding we introduced earlier. Now, to define the change on the x-axis, we add a roll left and a roll right of our padded matrix, along with a subtraction of two of the original matrix itself. The roll left accounts for the added heat from the point directly to the left, while the roll right accounts for the head passed from the point to the right. The subtraction of two of the original matrix makes up half of the loss of heat at the given point. Note that we define the roll along the x-axis by setting axis=0 and the shift left or right by defining shift=1 for right shift and shift=-1 for left shift. We now do the exact same thing for the y-axis, the only difference being that we set our axis=1. Again, roll upwards accounts for the heat from below, roll downwards gives each point the heat from the point above, and the subratction of two original matrices represents the other half of the loss from each point as it is passed along to other points. We can now implement our changes into a new matrix by adding the matrix newU to an espilon scaled factor of the changes and assigning it to the matrix finU. We now have an updated version of our matrix set for the next timestep, however as a final step we need to remove the padding layer. This simultaneously changes our matrix back to the same size and to allows heat to escape from the model by eliminating all heat that reached beyond the edge of our original gridstate. We can create our desired matrix by simply indexing the middle of our finU matrix and setting it equal to our return value, u_new. Now, we have created our new u matrix to be used in the next timestep, allowing us to return u_new, concluding this function."
  },
  {
    "objectID": "posts/HW4/index.html#testing-numpy-function",
    "href": "posts/HW4/index.html#testing-numpy-function",
    "title": "HW 4: Heat diffusion with Jax",
    "section": "Testing Numpy Function",
    "text": "Testing Numpy Function\n\n#creating new list in which we will append our results\nnewList = []\n\nAgain, we are creating a list that we will append our data to, called newList. Note that there is no matrix definition next to the list definition this time as we are not planning on using any matrix multiplication.\n\n%%timeit -r 1 -n 1\n#resetting u0 to initial conditions\nu0 = np.zeros((N,N))\nu0[int(N/2), int(N/2)] = 1.0\n#looping to increase timestep a total of 2700 times \nfor t in range(1, 2701):\n    #calling numpy function to advance timestep\n    u0 = advance_time_numpy(u0, epsilon)\n    if t%300==0:\n        #appending timestate information once every 300 steps, printing for debugging purposes\n        newList.append([u0,t])\n        print(t)\n\n300\n600\n900\n1200\n1500\n1800\n2100\n2400\n2700\n520 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n\n\nThis testing block of code is a gain very similar to the previous ones, with one main exception. In this block we are no longer calling advance_time_matvecmul(), but are calling advance_time_numpy() instead. Note that in this new function, we no longer need to pass an instance of A, as we are no longer utilizing matrix multiplication to advance our timestep. Apart from that, everything else remains identical to the previous tests, and we can see the improvement our new function yields as the loop only took 520 ms, less than a third of the time of our previous attempt.\n\n#setting the dimensions for our display figure\nrows=3\ncols = 3\n#resetting variable used in display loop\nimg_count = 0\n#defining our display figure\nfig, axes = plt.subplots(nrows=rows, ncols=cols, figsize=(8,8))\n#looping to fill our display figure\nfor i in range(rows):\n    for j in range(cols):        \n        if img_count &lt; len(newList):\n            #setting each box in display equal to unique image from list newList\n            axes[i, j].imshow(newList[img_count][0])\n            #incrementing img_count to traverse to next list element\n            img_count+=1\n\n\n\n\n\n\n\n\nWe run through our visualization process, and can see that the heat plots yielded by the advance_time_numpy() function are the same as our previous visualizations, making us confident that it is accurate."
  },
  {
    "objectID": "posts/HW4/index.html#advancing-time-using-jax",
    "href": "posts/HW4/index.html#advancing-time-using-jax",
    "title": "HW 4: Heat diffusion with Jax",
    "section": "Advancing Time Using Jax",
    "text": "Advancing Time Using Jax\n\ndef advance_time_jax(u, epsilon):\n    \"\"\"Advances simulation by one timestep using jax without using\n        matrix multiplication routines.\n    Args:\n        u: N x N grid state at timestep k\n        epsilon: stability constant\n        \n    Returns: N x N Grid state at timestep k+1\n    \"\"\"\n    #get N so that we know what size of matrix we are working with\n    m,N = u.shape\n    #create new matrix with two extra rows and columns\n    newU = jnp.zeros((N+2,N+2))\n    #set the center of the new matrix equal to the passed matrix u\n    newU = newU.at[1:-1,1:-1].add(u)\n    #define the changes to be implemented in the x direction\n    u_xx = jnp.roll(newU, shift=1, axis=0) - 2 * newU + jnp.roll(newU, shift=-1, axis=0)\n    #define the changes to be implemented in the y direction\n    u_yy = jnp.roll(newU, shift=1, axis=1) - 2 * newU + jnp.roll(newU, shift=-1, axis=1)\n    #implement the changes into a new matrix\n    finU = newU + epsilon * (u_xx + u_yy)\n    #setting the return value to the matrix with only desired values\n    u_new = finU[1:-1,1:-1]\n\n    return u_new\n\nThis method is very similar to our numpy function, with a couple key differences. Similar to the numpy function above, we want this function to advance our grid state through one timestep without using matrix multiplication. However, we plan on jitting this function which means that we will not be able to change specific indexes within arrays. Luckily, the jax numpy library has an add() method that will be useful in implementing similar strategies as we did in the previous function. We will again retrieve the dimension N from the size of u, and then create a new matrix with padding, titled newU. However we cannot directly access and change the indices of newU to place our current u in the center, so we add .add(u) to the indices to add it to the middle, creating our padded u that we can now work with. The changes we define in both the x and y directions are identical to the numpy function above, as well as the assignment of the new matrix finU, defined as the sum of newU and an epsilon scaled change. Now, to return, we create a new matrix, u_new, that is defined as the center of our finU matrix, again both resizing back to the same size as our original u and allowing heat to escape. Now, we can return u_new to conclude our function."
  },
  {
    "objectID": "posts/HW4/index.html#testing-jax-numpy-function",
    "href": "posts/HW4/index.html#testing-jax-numpy-function",
    "title": "HW 4: Heat diffusion with Jax",
    "section": "Testing Jax Numpy Function",
    "text": "Testing Jax Numpy Function\n\n#creating new list to be appended to\nnewList1 = []\n\nInitializing our final output list that we will append to in the following block of code.\n\n%%timeit -r 1 -n 1\n#resetting u0 to initial conditions\nu0 = np.zeros((N,N))\nu0[int(N/2), int(N/2)] = 1.0\n#looping to increase timestep a total of 2700 times\nfor t in range(1, 2701):\n    #using jax function to advance timestep by one\n    u0 = jit(advance_time_jax)(u0, epsilon)\n    if t%300 == 0:\n        #once every 300 steps append timestep information to list, printing step for debugging purposes\n        newList1.append([u0,t])\n        print(t)\n\n300\n600\n900\n1200\n1500\n1800\n2100\n2400\n2700\n291 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n\n\nThis block is only slightly altered from our previous loop, as in this case we are both calling a different function, advance_time_jax() and jitting it. You may be wondering why we did not jit our advance_time_numpy() function. This is because after jitting a function, there are certain rules that must be followed, for example you cannot change specific indices in an array. This rule was not followed by advance_time_numpy(), however we created advance_time_jax() with the idea of jitting it in mind, meaning it satisfies all the rules. We observe another significant improvement as this loop only took 291 ms to run, just over half the time of the previous iteration.\n\n#defining dimensions of display\nrows=3\ncols = 3\n#resetting variable used in display loop\nimg_count = 0\n#defining our figure\nfig, axes = plt.subplots(nrows=rows, ncols=cols, figsize=(8,8))\nfor i in range(rows):\n    for j in range(cols):        \n        if img_count &lt; len(newList1):\n            #filling each box with image from information in newList1\n            axes[i, j].imshow(newList1[img_count][0])\n            #incrementing to allow for traversal along list\n            img_count+=1\n\n\n\n\n\n\n\n\nAs our visualizations yielded by jax_advance_time() match up with the other three sets, we are confident that all functions are running as intended."
  },
  {
    "objectID": "posts/HW4/index.html#comparison",
    "href": "posts/HW4/index.html#comparison",
    "title": "HW 4: Heat diffusion with Jax",
    "section": "Comparison:",
    "text": "Comparison:\nWhen looking at the four methods that we have implemented, it is clear that the advance_time_matvecmul() method is by far the slowest at 1 minute and 39 seconds per loop while our final method, advance_time_jax() is the fastest at a mere 291 ms per loop. The method advance_time_numpy() and advance_time_jax() may seem relatively similar in speeds as there is less than a second separating their time to progress 2700 time steps, however in reality the jax method is a just less than twice as fast as the numpy method, at times of 291 ms and 520 ms respectively. That being said, they are both significantly faster than the sparse_advance_time_matvecmul() method at 1.7 seconds per loop.\nWhen it comes to writing the functions, taking into account that I did not write the first method, I found the get_sparse_A() method the easiest to write, as I was simply building on the get_A() method I had previously written. Similarly, the get_A()function was relatively straightforward to write as I simply had to create a matrix and then edit three of the diagonals. I initially had some trouble when writing the advance_time_numpy() function as I was unsure of how to implement the boundary conditions that were to allow heat to escape from the model. Before successfuly completing the boundary condition, my results yielded in a heat map that began to take on an almost plus like shape as the heat began to reach the edges and simply wrap around to the other side. I solved this probliem by creating a matrix of dimensions (N+2)x(N+2), which allowed the heat to go to an outer ring, that was effectively deleted before returning the function, simulating the idea that the heat escaped outside of our model. Similar to the get_A_sparse() function, after writing the advance_time_numpy() function the advance_time_jax() function was relatively easyto implement. Conceptually it follows the same pattern as the numpy function, with the only catches being that jax does not allow for index assingment. To get around this, I found a function .at(), helping me to navigate around the matrices, and more importantly, I created new matrices at each step, meaning I was not altering any pre-existing matrices, but rather creating new ones which is allowed by jax."
  },
  {
    "objectID": "posts/project/index.html",
    "href": "posts/project/index.html",
    "title": "Final Project: Flight Site",
    "section": "",
    "text": "from google.colab import drive\ndrive.mount('/content/drive/')\n\nMounted at /content/drive/"
  },
  {
    "objectID": "posts/project/index.html#objective",
    "href": "posts/project/index.html#objective",
    "title": "Final Project: Flight Site",
    "section": "Objective",
    "text": "Objective\nOur objective was to use predictive analytics to enhance travel planning. Using historical flight data, our platform gives travelers insight to potential delays on their upcoming flights, minimizing the inconvenience of delays. The ultimate goal is to transform how travelers approach flying, shifting from reactive to proactive planning."
  },
  {
    "objectID": "posts/project/index.html#overview",
    "href": "posts/project/index.html#overview",
    "title": "Final Project: Flight Site",
    "section": "Overview",
    "text": "Overview\nThe Delayed Flight Site is a web-based application designed to predict flight departure delays within the United States. It aims to assist frequent flyers in efficiently organizing their flight schedules and travel plans by allowing them to anticipate and plan for potential delays. Our project will follow this path: data acquisition -&gt; SQL database creation -&gt; model creation -&gt; web app creation -&gt; complex visuals. We first acquire data about flights in the US, storing this data in a SQL database. We then create a predictive model that determines whether a given flight will be delayed. Then, we create an interactive web app that allows users to inputs their flight or itenerary details and outputs information about their flight and a predicted delay. This web app will have the following features: Itinerary to Save User-Inputted Flights, Flight Delay Predictions Based on User input, and Interactive Visualizations for Additional Insight. Then, we create interactive complex visualizations that will display on the applicationa and will change with user input. This procedure is displayed in the flow chart here:\n\n# @title Flowchart\nfrom IPython.display import Image, display\n\n# Correct path with no spaces in folder names\nimage_path0 = '/content/drive/MyDrive/PIC16B_Datasets/flowchart.jpeg'\n\n# Display the image\ndisplay(Image(filename=image_path0))"
  },
  {
    "objectID": "posts/project/index.html#data-acquisition-and-sql-database-creation",
    "href": "posts/project/index.html#data-acquisition-and-sql-database-creation",
    "title": "Final Project: Flight Site",
    "section": "1. Data Acquisition and SQL Database Creation",
    "text": "1. Data Acquisition and SQL Database Creation\nWe started by importing flights data from the US Bureau of Transportation. This data was obtained through the following link: https://www.transtats.bts.gov/DL_SelectFields.aspx?gnoyr_VQ=FGK&QO_fu146_anzr=b0-gvzr The data that we got was organized by month, so we imported data from November of 2022 to November of 2023. The data contains all flights that have departure and arrival location in the United States. Each of the monthly datasets are loaded in below. We could not put these datasets into our GitHub because they were too large, so this process will only work on computers where the data is already uploaded. You can manually load in the data using the above link. We started with a separate pandas dataframe for each month, and concatenated it into a single dataframe containing all the data we collected. As we can see, the data has 7,818,349 rows and 30 columns, and was about 3GB when downloaded as a csv file. Each column contains different information about the flight, and a brief summary of each column is given in the FlightDelayModel.ibynp in the GitHub.\n\nimport pandas as pd\nimport os\n\nos.chdir('/content/drive/MyDrive/PIC16B_Datasets')\n\nnov22 = pd.read_csv(\"nov22.csv\")\ndec22 = pd.read_csv(\"dec22.csv\")\njan23 = pd.read_csv(\"jan23.csv\")\nfeb23 = pd.read_csv(\"feb23.csv\")\nmar23 = pd.read_csv(\"mar23.csv\")\napr23 = pd.read_csv(\"apr23.csv\")\nmay23 = pd.read_csv(\"may23.csv\")\njun23 = pd.read_csv(\"jun23.csv\")\njul23 = pd.read_csv(\"jul23.csv\")\naug23 = pd.read_csv(\"aug23.csv\")\nsep23 = pd.read_csv(\"sep23.csv\")\noct23 = pd.read_csv(\"oct23.csv\")\nnov23 = pd.read_csv(\"nov23.csv\")\nall_data = pd.concat([nov22, dec22, jan23, feb23, mar23, apr23, may23, jun23, jul23, aug23, sep23, oct23, nov23],\n                     ignore_index = True, axis = 0)\nall_data.shape\n\nDtypeWarning: Columns (39) have mixed types. Specify dtype option on import or set low_memory=False.\n  jan23 = pd.read_csv(\"jan23.csv\")\n&lt;ipython-input-3-d6c0b08a5729&gt;:14: DtypeWarning: Columns (39) have mixed types. Specify dtype option on import or set low_memory=False.\n  jul23 = pd.read_csv(\"jul23.csv\")\n\n\n(7818349, 50)\n\n\nAfter getting data about each flight, we also wanted to get longitude and latitude data for each airport to be able to eaily plot our observations. This was done through an API called airportsdata. We will create a pandas dataframe using the airports data that contains the longitude and latitude of every airport in the United States. This process is outlined below.\n\n!pip install -U airportsdata\nimport airportsdata\nairports = airportsdata.load('IATA')  # key is the ICAO identifier (the default)\nairports_df = pd.DataFrame([(airport, dic[\"lat\"], dic[\"lon\"]) for airport, dic in airports.items()])\nairports_df.rename(columns = {0:\"AIRPORT_ID\", 1:\"LATITUDE\", 2:\"LONGITUDE\"}, inplace = True)\nairports_df.head()\n\nRequirement already satisfied: airportsdata in /usr/local/lib/python3.10/dist-packages (20240316.1)\n\n\n\n  \n    \n\n\n\n\n\n\nAIRPORT_ID\nLATITUDE\nLONGITUDE\n\n\n\n\n0\nOCA\n25.324317\n-80.275729\n\n\n1\nCYT\n60.080849\n-142.495494\n\n\n2\nFWL\n62.509183\n-153.890626\n\n\n3\nCSE\n38.851937\n-106.932821\n\n\n4\nCUS\n31.823711\n-107.626967\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\nNow that we have two large datasets, we will create a SQL database to store the data. The creation of a SQL database will allow us to easily look at subsets of the data without having to load in the whole dataframe. This will significantly speed up the data loading for the rest of our project. We will use this SQL database in every aspect of our project from now on. This SQL database is created below.\n\nimport sqlite3\n\nconn = sqlite3.connect(\"flights.db\")\n\nall_data.to_sql(\"flights\", conn, if_exists = \"replace\", index = False)\nairports_df.to_sql(\"airports\", conn, if_exists = \"replace\", index = False)\n\nconn.close()\n\nWe will now perform a couple small tests to ensure that the adtabase is working as intended. First, we will try to select the origin, destination, flight number, and departure delay for all United Airlines flights on Noverber 1, 2022 that were delayed over 15 minutes.\n\nwith sqlite3.connect(\"flights.db\") as conn:\n    cmd = \\\n    f\"\"\"\n    SELECT\n        ORIGIN, DEST, OP_CARRIER_FL_NUM, DEP_DELAY\n    FROM\n        flights\n    WHERE\n        YEAR = \"2022\"\n        AND\n        MONTH = 11\n        AND\n        DAY_OF_MONTH = 1\n        AND\n        OP_UNIQUE_CARRIER = \"9E\"\n        AND\n        DEP_DEL15 = 1\n    \"\"\"\n    df_test = pd.read_sql_query(cmd, conn)\n\nprint(df_test.shape)\ndf_test.head()\n\n(31, 4)\n\n\n\n  \n    \n\n\n\n\n\n\nORIGIN\nDEST\nOP_CARRIER_FL_NUM\nDEP_DELAY\n\n\n\n\n0\nLGA\nCVG\n4635\n22.0\n\n\n1\nCAE\nATL\n4675\n22.0\n\n\n2\nIND\nJFK\n4694\n20.0\n\n\n3\nBNA\nLGA\n4737\n105.0\n\n\n4\nLGA\nMCI\n4743\n146.0\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\nWe will not preform a small test to make sure the table is functional in our database. We will output a dataframe of the airport id, latitude, and longitude of all airports lower than -100 longitude.\n\nwith sqlite3.connect(\"flights.db\") as conn:\n    cmd = \\\n    \"\"\"\n    SELECT\n        AIRPORT_ID, LATITUDE, LONGITUDE\n    FROM\n        airports\n    WHERE\n        LONGITUDE &lt; -100\n    \"\"\"\n    df_airports_test = pd.read_sql_query(cmd, conn)\n\nprint(df_airports_test.shape)\ndf_airports_test.head()\n\n(1114, 3)\n\n\n\n  \n    \n\n\n\n\n\n\nAIRPORT_ID\nLATITUDE\nLONGITUDE\n\n\n\n\n0\nCYT\n60.080849\n-142.495494\n\n\n1\nFWL\n62.509183\n-153.890626\n\n\n2\nCSE\n38.851937\n-106.932821\n\n\n3\nCUS\n31.823711\n-107.626967\n\n\n4\nICY\n59.969019\n-141.661770\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\nWe can see that the database works as intended. This database will be very important for our project because it will allow us to quickly subset our dataset that contains about 8 million observations. The general structure of the database is displayed in the visualization below.\n\n\n\nsql_db.png"
  },
  {
    "objectID": "posts/project/index.html#data-preprocessing-and-building-a-model",
    "href": "posts/project/index.html#data-preprocessing-and-building-a-model",
    "title": "Final Project: Flight Site",
    "section": "2. Data Preprocessing and Building a Model",
    "text": "2. Data Preprocessing and Building a Model\nNow that we have all the data that we need to proceed with our project, our goal for this section is to create a model that predicts whether a flight will be delayed by over 15 minutes. This is a binary classification problem. Thiw will correspond to the binary varuable DEP_DEL15 in the flights table of our SQL database. To predict this, we plan to use the flight departure date and time, arrival date and time, departure airport, arrival airport, carrier, and distance. This should give us enough insight because most delays are destination or airline specific. These predictors correspond to the columns YEAR, MONTH, DAY_OF_MONTH, CARRIER, DEP_TIME, ARR_TIME, DISTANCE in the flights table and the columns LATITUDE and LONGITUDE in the airports table. We will create a function to output these variables from the SQL database. This function will select the desired predictors from the flights table while joining the airports table by airport ID. The code for the function is shown and it is tested below.\n\nimport sqlite3\nimport pandas as pd\nimport os\n\nos.chdir('/content/drive/MyDrive/PIC16B_Datasets')\n\ndef get_flight_model_data():\n    '''\n    Returns a pandas dataframe of all the necessary predictors to predict flight delay.\n    (note: must have the flights.db database)\n    Args:\n        none\n    Returns:\n        a pandas dataframe with all necessary predictors\n    '''\n    with sqlite3.connect(\"flights.db\") as conn:\n        cmd = \\\n        \"\"\"\n        SELECT\n            flights.YEAR, flights.MONTH, flights.DAY_OF_MONTH, flights.DEP_TIME, flights.ARR_TIME,\n            flights.OP_UNIQUE_CARRIER, flights.ORIGIN, flights.DEST, flights.DISTANCE, flights.DEP_DEL15,\n            airports.LATITUDE \"ORIGIN_LATITUDE\", airports.LONGITUDE \"ORIGIN_LONGITUDE\"\n        FROM\n            flights\n        INNER JOIN\n            airports ON flights.ORIGIN = airports.AIRPORT_ID\n        \"\"\"\n        df = pd.read_sql_query(cmd, conn)\n    with sqlite3.connect(\"flights.db\") as conn:\n        cmd = \\\n        \"\"\"\n        SELECT\n            flights.DEST, airports.LATITUDE \"DEST_LATITUDE\", airports.LONGITUDE \"DEST_LONGITUDE\"\n        FROM\n            flights\n        INNER JOIN\n            airports ON flights.DEST = airports.AIRPORT_ID\n        \"\"\"\n        df_dest = pd.read_sql_query(cmd, conn)\n    df[\"DEST_LATITUDE\"] = df_dest[\"DEST_LATITUDE\"]\n    df[\"DEST_LONGITUDE\"] = df_dest[\"DEST_LONGITUDE\"]\n    return df\n\ndf = get_flight_model_data()\nprint(df.shape)\ndf.head()\n\n(7818349, 14)\n\n\n\n  \n    \n\n\n\n\n\n\nYEAR\nMONTH\nDAY_OF_MONTH\nDEP_TIME\nARR_TIME\nOP_UNIQUE_CARRIER\nORIGIN\nDEST\nDISTANCE\nDEP_DEL15\nORIGIN_LATITUDE\nORIGIN_LONGITUDE\nDEST_LATITUDE\nDEST_LONGITUDE\n\n\n\n\n0\n2022\n11\n1\n1355.0\n1747.0\n9E\nXNA\nLGA\n1147.0\n0.0\n36.281579\n-94.307766\n40.777250\n-73.872611\n\n\n1\n2022\n11\n1\n1412.0\n1609.0\n9E\nLGA\nCVG\n585.0\n1.0\n40.777250\n-73.872611\n39.048837\n-84.667821\n\n\n2\n2022\n11\n1\n1345.0\n1550.0\n9E\nLGA\nXNA\n1147.0\n0.0\n40.777250\n-73.872611\n36.281579\n-94.307766\n\n\n3\n2022\n11\n1\n1550.0\n1648.0\n9E\nLSE\nMSP\n119.0\n0.0\n43.879266\n-91.256634\n44.881972\n-93.221778\n\n\n4\n2022\n11\n1\n1418.0\n1506.0\n9E\nMSP\nLSE\n119.0\n0.0\n44.881972\n-93.221778\n43.879266\n-91.256634\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\nNow that we have our data, we must proceed to making our data ready to be inputted into our model. There is only one categorical predictor, OP_UNIQUE_CARRIER. We need to see if there are any similarities between carriers to encode the predictor. We will first look at the proportion of delayed flights with the same carrier to see if there is any grouping we can make. This could potentially give some feedback in the final model, but after the model was tested this was the most efficient way to encode these variables. We will also look at the average distance of flights with the same carrier to see if there is any grouping we can make. As shown in the scatterplot below, there are four main groups, which we will use to encode the carriers. There is one group in the bottom left, one group in the top right, one group in the bottom middle and one group in the top middle. We will encode the flight carriers PT, YX, 9E, QX, OH, OO, C5, G7, and MQ as 0, HA, ZW, YV, and WN as 1, DL, AA, G4, UA, and AS as 2, and B6, F9, and NK as 3.\n\nimport matplotlib.pyplot as plt\n\n# get proportion of delays for each carrier\ncarrier_groups = df[[\"OP_UNIQUE_CARRIER\", \"DEP_DEL15\"]].groupby(\"OP_UNIQUE_CARRIER\")\ncarrier_delays = carrier_groups.mean()\n\n# get median distance of flight for each carrier\ncarrier_groups = df[[\"OP_UNIQUE_CARRIER\", \"DISTANCE\"]].groupby(\"OP_UNIQUE_CARRIER\")\ncarrier_distances = carrier_groups.median()\n\n# create a plot of proportion of delays vs average diatance\nfig, ax = plt.subplots()\nax.scatter(x = carrier_delays, y = carrier_distances)\nax.set_title(\"Flight Delays and Distance for Carriers\")\nax.set_xlabel(\"Proportion of Delayed FLights\")\nax.set_ylabel(\"Average Distance of Flights\")\n\nfor i, txt in enumerate(carrier_delays.index):\n    ax.annotate(txt, (carrier_delays[\"DEP_DEL15\"][i], carrier_distances[\"DISTANCE\"][i]))\n\n# encode the OP_UNIQUE_CARRIER column\ndf[\"OP_UNIQUE_CARRIER\"].replace({\"PT\":0, \"YX\":0, \"9E\":0, \"QX\":0, \"OH\":0, \"OO\":0, \"C5\":0, \"G7\":0, \"MQ\":0, \"HA\":1, \"ZW\":1, \"YV\":1, \"WN\":1, \"DL\":2, \"AA\":2, \"G4\":2, \"UA\":2, \"AS\":2, \"B6\":3, \"F9\":3, \"NK\":3}, inplace = True)\ndf[\"OP_UNIQUE_CARRIER\"].value_counts()\n\n2    3244790\n0    2013241\n1    1786664\n3     773654\nName: OP_UNIQUE_CARRIER, dtype: int64\n\n\n\n\n\n\n\n\n\nNow that we have all of our variables prepped to use in our model, we must figure out how to deal with NA values in our data. There were a small number in proportion to the size of the data, as shown below. We can see that about 0.35% of the data is NA values. This is an extremely small amount, so we will deal with the NAs by simply removing any rows that contain NA values, which is also done below.\n\nprint(\"Proportion of NAs: \", (df.isna().sum().sum()) / df.size)\nprint(\"Shape before dropping NAs: \", df.shape)\ndf = df.dropna()\nprint(\"Shape after dropping NAs: \", df.shape)\n\nProportion of NAs:  0.0034782736282119335\nShape before dropping NAs:  (7818349, 14)\nShape after dropping NAs:  (7687386, 14)\n\n\nWe will now check the rate at which our model should perform. This will be done by looking at the proportion of flights which are delayed, done below. As we can see, the data is extremely imbalanced. There are way more non-delayed flights than theer are delayed flights. The base rate is about 0.8, which is going to be very high for the model that we plan to create because of outside variability. Seeing that we have over 7 million observations, we can certainly even out this category in the data to make the model more accurate. We will remove some of the data of the non-delayed flights to make the number of flights in each category equal. This is done below.\n\nprint(\"Proportion of delayed flights: \", df[\"DEP_DEL15\"].mean())\nprint(\"Proportion of non-delayed flights: \", 1 - df[\"DEP_DEL15\"].mean())\n\n# make the number of delayed and not delayed flights equal\ng = df.groupby('DEP_DEL15')\ndf = g.apply(lambda x: x.sample(g.size().min()).reset_index(drop=True))\nprint(\"New data shape: \", df.shape)\n\nprint(\"New proportion of delayed flights: \", df[\"DEP_DEL15\"].mean())\nprint(\"New proportion of non-delayed flights: \", 1 - df[\"DEP_DEL15\"].mean())\n\nProportion of delayed flights:  0.20882612112882065\nProportion of non-delayed flights:  0.7911738788711793\nNew data shape:  (3210654, 14)\nNew proportion of delayed flights:  0.5\nNew proportion of non-delayed flights:  0.5\n\n\nWe will now create a training and testing dataset for our model, done below. We will set aside 20% for testing and keep 80% for training.\n\nfrom sklearn.model_selection import train_test_split\n\nmodel_X = df[['YEAR', 'MONTH', 'DAY_OF_MONTH', 'DEP_TIME', 'ARR_TIME', 'OP_UNIQUE_CARRIER', 'DISTANCE', 'ORIGIN_LATITUDE', 'ORIGIN_LONGITUDE', 'DEST_LATITUDE', 'DEST_LONGITUDE']]\nmodel_y = df[['DEP_DEL15']]\nX_train, X_test, y_train, y_test = train_test_split(model_X, model_y, test_size = 0.2, random_state = 50)\nprint(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n\n(2568523, 11) (642131, 11) (2568523, 1) (642131, 1)\n\n\nWe can now move on to model fitting. We started with training many deep neural network models, which took very long to train and were ultimately not as successful as we hoped. Our best model ended up being a random forest classifier, as shown below. we tested different values for n_estimators, max_depth, min_samples_split, and min_samples leaf, but the best accuracy with the default values used by RandomForestClassifier(). As we can see, the model predicts whether a flight will be delayed with 83% accuracy. This is very good, expecially considering the amount of variabiliy that goes into flight delays.\n\nfrom sklearn.ensemble import RandomForestClassifier\nimport numpy as np\n\nrf = RandomForestClassifier(n_estimators=100, criterion=\"entropy\", random_state=0)\nrf.fit(X_train, np.array(y_train).flatten())\nrf.score(X_test, np.array(y_test).flatten())\n\nThe above code was not able to be run due to the size of the data, but the model has 83% test accuracy."
  },
  {
    "objectID": "posts/project/index.html#making-a-flask-web-app",
    "href": "posts/project/index.html#making-a-flask-web-app",
    "title": "Final Project: Flight Site",
    "section": "3. Making a Flask Web App",
    "text": "3. Making a Flask Web App\nOur Flask web app is designed to grant a user intuitive access to the features that our project provides. Primarily, checking the delay status of individual flights as well as creating itineraries that can be visualized using the Plotly Dash apps described in the following sections. Additionally, we added the option for a user to create an account in order to save flights and itineraries to be viewed at a later time.\n\nSite Database Creation\nThe first step taken in the web app was to create a database that stored the information for each user. This database contained three tables: user, itineraryCounter, and itineraries. The user table stored the usernames as well as the hashed passwords of each user, a table called when logging in or creating a new account. The itinerary counter table served as a way to help index our final table, itineraries. This itinerary page contained several columns: itin_id, author_id, origin, destination, airline, depDate, and arrTime. Each row in this table represents a single flight, with the itin_id and author_id letting us know which itinerary that flight belongs to. The code for The creation of our database is contained below within our get_db() function, which simultaneously opens a connection with the database, as well as creating the tables if they have not already been created.\n\n'''\nOpens up connection with the database, creating the necessary tables if they have not already been created.\n'''\ndef get_db():\n    if 'db' not in g:\n        #connecting to database\n        g.db = sqlite3.connect(\"webProj.sqlite\")\n        #creating cursor so we can interact with database\n        cursor = g.db.cursor()\n        #execute creation of tables using (CREATE TABLE IF NOT EXISTS)\n        #creatin table for user login information\n        cursor.execute(\"CREATE TABLE IF NOT EXISTS user ( id INTEGER PRIMARY KEY AUTOINCREMENT, username TEXT UNIQUE NOT NULL, password TEXT NOT NULL)\")\n        #creating table for itinerary identification\n        cursor.execute(\"CREATE TABLE IF NOT EXISTS itineraryCounter ( id INTEGER PRIMARY KEY AUTOINCREMENT, counter TEXT)\")\n        #creating table holding information to be stored in itineraries (ex. flight info), to be tied back to each itinerary in previous table via itin_id value\n        cursor.execute(\"CREATE TABLE IF NOT EXISTS itineraries ( id INTEGER PRIMARY KEY AUTOINCREMENT, itin_id INTEGER NOT NULL, author_id TEXT, origin TEXT, destination TEXT, airline TEXT, depTime TEXT, arrTime TEXT)\")\n        #allowing us to access columns by their name\n        g.db.row_factory = sqlite3.Row\n        #commiting cursor changes\n        g.db.commit()\n    return g.db\n\n\n\nRegistration and Login\nNow that we have our database set up, we can begin to register users and save itineraries. In order to register, we must take in the input from the user, ensuring that their requested username is not already in use by another user, and hash their password for security purposes. Additionally, after their successful registration, we reroute them to the login page where they can test that their new login information is working correctly. The functions are pictured below.\n\n'''\nPage used for registering new users into the database. Takes in user input for\nusername and password, ensuring username is not already taken, before rerouting to\nlogin page.\n'''\n@auth_bp.route('/register', methods=('GET', 'POST'))\ndef register():\n    if request.method == 'POST':\n        #getting user input for their desired username and password\n        username = request.form.get('username')\n        password = request.form.get('password')\n        #opening connection with the database\n        db = get_db()\n        error = None\n\n        #checks to make sure username and password are filled out\n        if not username:\n            error = 'Username is required.'\n        elif not password:\n            error = 'Password is required.'\n\n\n        if error is None:\n            #inserts user information into database\n            try:\n                db.execute(\"INSERT INTO user (username, password) VALUES (?,?)\", (username, generate_password_hash(password)),\n                )\n                db.commit()\n            #checks if username already exists\n            except db.IntegrityError:\n                error = f\"Username {username} is already registered.\"\n            else:\n                #sends user to login page where they can login with their newly created account\n                return redirect(url_for(\"auth.login\"))\n        #error shown to user if there is one\n        flash(error)\n    return render_template('auth/register.html')\n\nWe can see that we first gather user input by assigning our username and password variables as the input from request forms displayed on the resgister page. We then open up a connection to the database using the get_db() function we creating previously, which will allow us to check that the username is not already in use. To accomplish this, we use a try statement in which we insert the username and the hashed password into the user table, excepting an IntegrityError, which would imply that the username is already in use. It that is the case, we send a message to the user that their chosen username is already in use, prompting them to choose a new one. If not, the username and password combination will be placed in the user table, and the user will be redirected to the login page, as shown by the redirect near the end of the code block. Below is an example of the error message shown when trying to register a username that is already taken. \n\n'''\nPage where users are able to log back in.\n'''\n@auth_bp.route('/login', methods = ('GET', 'POST'))\ndef login():\n    if request.method == 'POST':\n        #user enters their username and password\n        username = request.form.get('username')\n        password = request.form.get('password')\n        #opens connection with database\n        db = get_db()\n        error = None\n        #checks for username in database\n        user = db.execute('SELECT * FROM user WHERE username = ?', (username,)).fetchone()\n        #gives error if username not found\n        if user is None:\n            error = 'Incorrect username.'\n        #checks username password against inputted password\n        elif not check_password_hash(user['password'],password):\n            error = 'Incorrect password.'\n        #resets session and sets user id to user who just logged in\n        if error is None:\n            session.clear()\n            session['user_id'] = user['id']\n            #sends us back to main page, now as a logged in user\n            return redirect(url_for('index'))\n        #flashes error if one exists\n        flash(error)\n    return render_template('auth/login.html')\n\nSimilar to the register page, we gather user information using request.form.get() commands, and then open a connection with the database using get_db(). We then check to ensure that the inputted username is contained within the user table, using the cursor db that is returned by the get_db() function. If nothing is returned, that means the username is not contained within the table, which means that username is not associated with any existing user. Therefore, we send an incorrect user message, and prompt them to enter a different username. If the username is contained within the table, we can then move on to checking that the inputted password is correct. However, as we hashed the password for security reasons on the register page, we cannot simply check that user[‘password’]==password, as it would almost always return false. Instead, we call the function check_password_hash(), and pass in the hashed user password contained within the database, as well as the inputted password. This will now check that the hashed versions of the passwords match up, which if true means that the user has successfully inputted their login information and should now be fully logged in. In order to accomplish this, we set the session[‘user_id’] to be the inputted username. Note that we clear the session ahead of time just in case. The session stores variables that will be able to be used across all pages contained within the app, and as the information of the logged in user will be used across most pages, it makes sense to store it as a session variable.\n\n\nHandling User Input\nNow that we have finished the registration and login functionality of our site, we can begin to create the ability for the user to input flights and itineraries. Let us first look at taking in the input for an individual flight, which is displayed on our ‘flights’ page. We plan on taking in information regarding the origin, destination, airline, departure time, and arrival time of each flight. Therefore we will need to initialize five variables for this function, assigning them the values inputted by the user by again using request.form.get(). Now that we have obtained the input from the user, we can perform a bit of data processing by formatting the date into a more visually appealing format, accomplished by use of the strptime and strftime functions. After checking that each form has been filled out, we can begin to utilize this data. We first create a dictionary containing all of the flight data called flightDict, and append it to an empty list flightList. While it appears that creating a list with a single dictionary within it seems somewhat redundant, this helps to fit the format of when we have multiple flights from an itinerary. We then use the inputs we have received and use them to run the model we created, described in section two of the post, which will tell us whether or not we expect the flight to be delayed. Finally, we utilize a redirect to the ‘flightDisp’ page which will display all the user inputs, as well as the results of the model for our user to see. It is critical that we also pass the flight information as well as the result of the model that we just ran to the next page as well. You may be confused as to way we are passing each variable individually as opposed to the list we created, and it is due simply to the fact that when passing values this way, it is much simpler to obtain them on the next page when they are passed one by one.\n\n'''\nPage where user enters a single flight to check the estimate of it being delayed. Page is reached via the link in the navbar and will redirect to /flightDisp page where flight information is to be displayed.\n'''\n@server.route('/flights', methods = ['GET', 'POST'])\ndef flights():\n    if request.method == 'POST':\n\n        # Obtaining flight input from user\n        origin = request.form.get('origin')\n        destination = request.form.get('destination')\n        airline = request.form.get('airline')\n        date = request.form.get('date')\n        arrivalTime = request.form.get('arrivalTime')\n\n        # Altering format of date to make it more readable\n        date = dt.strptime(date, '%Y-%m-%dT%H:%M')\n        date = date.strftime('%d/%m/%Y %H:%M')\n\n        # Altering formate of arrivalTime to make it more readable\n        arrivalTime = dt.strptime(arrivalTime, '%Y-%m-%dT%H:%M')\n        arrivalTime = arrivalTime.strftime('%d/%m/%Y %H:%M')\n\n        # Initializing error to be none\n        error = None\n\n        # Checking all fields have been filled out, yielding an error if not\n        if not origin:\n            error = 'Please enter origin.'\n        elif not destination:\n            error = 'Please enter destination.'\n        elif not airline:\n            error = 'Please select an airline.'\n        elif not date:\n            error = 'Please input a departure date and time.'\n        elif not arrivalTime:\n            error = 'Please enter your estimated arrival time.'\n        if error is None:\n            # Creating flightList that will be passed to our layout function\n            flightList = []\n\n            # Initializing flightDict that contains all of the entered flight information\n            flightDict = {'origin': origin, 'destination': destination, 'airline': airline, 'depDate':date, 'arrDate':arrivalTime}\n\n            # Placing flight information into flightList\n            flightList.append(flightDict)\n\n            # Calling layoutDash to pass the flight information to Dash app, passed in name of app and flightList\n            #layoutDash(dash1, flightList)\n\n            # get airline abbreviation\n            airline_abr = airlineDict[airline]\n\n            # get day, month, year, and time as integers\n            dep_date_split = date.replace('/', ' ').replace(':', ' ').split()\n            day_of_month = int(dep_date_split[0])\n            month = int(dep_date_split[1])\n            year = int(dep_date_split[2])\n            dep_time = int(dep_date_split[3] + dep_date_split[4])\n\n            # get arrival time\n            arr_date_split = arrivalTime.replace('/', ' ').replace(':', ' ').split()\n            arr_time = int(arr_date_split[3] + arr_date_split[4])\n\n            # get origin longitude and latitude\n            origin_lon = airport_coords_df.loc[airport_coords_df[\"ORIGIN\"] == origin, \"lon\"].tolist()[0]\n            origin_lat = airport_coords_df.loc[airport_coords_df[\"ORIGIN\"] == origin, \"lat\"].tolist()[0]\n\n            # get destination longitude and latitude\n            dest_lon = airport_coords_df.loc[airport_coords_df[\"ORIGIN\"] == destination, \"lon\"].tolist()[0]\n            dest_lat = airport_coords_df.loc[airport_coords_df[\"ORIGIN\"] == destination, \"lat\"].tolist()[0]\n\n            distance = distances.loc[(distances[\"ORIGIN\"] == origin) & (distances[\"DEST\"] == destination), \"AVG_DISTANCE\"].tolist()[0]\n\n            if airline in ['PT', 'YX', '9E', 'QX', 'OH', 'OO', 'C5', 'G7', 'MQ']:\n                carrier = 0\n            elif airline in ['HA', 'ZW', 'YV', 'WN']:\n                carrier = 1\n            elif airline in ['DL', 'AA', 'G4', 'UA', 'AS']:\n                carrier = 2\n            else:\n                carrier = 3\n\n            X_new = pd.DataFrame({'YEAR':year, 'MONTH':month, 'DAY_OF_MONTH':day_of_month,\n                                  'DEP_TIME':dep_time, 'ARR_TIME':arr_time,\n                                  'OP_UNIQUE_CARRIER':carrier, 'DISTANCE':distance,\n                                  'ORIGIN_LATITUDE':origin_lat, 'ORIGIN_LONGITUDE':origin_lon,\n                                  'DEST_LATITUDE':dest_lat, 'DEST_LONGITUDE':dest_lon}, index = [0])\n\n            pred = rf_model.predict(X_new).tolist()[0]\n\n            if pred == 0:\n                delay = \"our model predicts no delays for your flight.\"\n            elif pred == 1:\n                delay = \"our model predicts that your flight will be delayed at least fifteen minutes.\"\n            else:\n                delay = \"our model returned inconclusive results.\"\n\n            # Send user to Dash app for visualization\n            #return redirect('/dashFlight/')\n            return redirect(url_for(\"flightDisp\", origin=origin, destination=destination, airline=airline, date=date, arrivalTime=arrivalTime, delay=delay, pred = pred))\n\n        # Flash error if one was present\n        flash(error)\n\n    #Rendering template, passing in airlineDict and flightInputDict to provide options in the searchable dropdown menus\n    return render_template('blog/flights.html', airlineDict = airlineDict, flightInputDict = flightInputDict)\n\nNote that our itinerary flights page is very similar to the above page, with a couple key exceptions. As our user is able to select how many flights they wish to place into their itinerary, we must run a for loop in order to ensure we take in all of their inputs. Additionally, we do not run the model as there are multiple flights in the itinerary, and it makes it much simpler and more efficient to do it in this manner.\n\n'''\nPage where user is able to input the information for the number of flights they specified on the /itinNum page. Page will forward to /itinDisp page where the complete itinerary is to be displayed.\n'''\n@server.route('/itinFlights', methods = ('GET', 'POST'))\ndef itinFlights():\n    # Get the number of flights passed from previous page\n    numFlight = int(request.args.get('numFlight'))\n\n    if request.method=='POST':\n        # Instructions if user selects the 'See itinerary' button\n        if request.form.get('action') == \"See itinerary\":\n            # Initializing flightList that will contain the data for all of our flights\n            flightList=[]\n\n            # Run through loop for number of flights requested by user\n            for i in range(0,numFlight):\n\n                # Taking in user inputs for each flight\n                origin = request.form.get(f'origin{i}')\n                destination = request.form.get(f'destination{i}')\n                airline = request.form.get(f'airline{i}')\n                date = request.form.get(f'date{i}')\n                arrivalTime = request.form.get(f'arrivalTime{i}')\n\n                # Creating a dictionary for each flight, containing user's information\n                flightDict = {'origin': origin, 'destination': destination, 'airline': airline, 'depDate':date, 'arrTime':arrivalTime}\n\n                # Adding information from each flight to flightList\n                flightList.append(flightDict)\n\n\n            # Converting first depDate to a datetime object that we will be able to extract the hour value\n            sesDate = dt.strptime(flightList[0]['depDate'], '%Y-%m-%dT%H:%M')\n\n            # Saving variables to session so that the Dash app will be able to access them\n            session['dateDash'] = sesDate.hour\n            session['dashboard_id'] = flightList\n            session['numFlight'] = numFlight\n\n            # Sending user to Dash app for visualization\n            return redirect(url_for('/dashFlight/', flightList = flightList))\n\n        # Instructions if user clicks 'Save itinerary' button\n        elif request.form.get('action') == \"Save itinerary\":\n\n            # Initializing flightList that will contain the data for all of our flights\n            flightList=[]\n\n            # Run through loop for number of flights requested by user\n            for i in range(0,numFlight):\n\n                # Taking in user inputs for each flight\n                origin = request.form.get(f'origin{i}')\n                destination = request.form.get(f'destination{i}')\n                airline = request.form.get(f'airline{i}')\n                date = request.form.get(f'date{i}')\n                arrivalTime = request.form.get(f'arrivalTime{i}')\n\n                # Creating a dictionary for each flight, containing user's information\n                flightDict = {'origin': origin, 'destination': destination, 'airline': airline, 'depDate':date, 'arrTime':arrivalTime}\n\n                # Adding information from each flight to flightList\n                flightList.append(flightDict)\n\n\n            # Converting sesDate to a datetime object\n            sesDate = dt.strptime(flightList[0]['depDate'], '%Y-%m-%dT%H:%M')\n\n            # Saving certain variables to session so they can be used by next page\n            session['dateDash'] = sesDate.hour\n            session['dashboard_id'] = flightList\n            session['numFlight'] = numFlight\n\n            # Redirecting to save page\n            return redirect(url_for('saveItin'))\n\n    # Rendering template, passing in numFlight for iterative purposes, as well as two dictionaries that the searchable dropdowns will access for their options\n    return render_template('blog/itinFlights.html', numFlight = numFlight, airlineDict=airlineDict, flightInputDict = flightInputDict)\n\n\n\nSaving Itineraries\nSaving a flight or itinerary is relatively straightforward. The trickiest part of this process is ensuring that each flight in an itinerary is given the same itin_id so that they can be accessed all together when we wish to display them. The first thing we do is open a connection to the database as we will need to place new information into our database in order to save it. In order to increment our itin_id by one each time, we check the maximum itin_id from itineraries and then increment it by one to ensure we are not accidentally placing our new itinerary into an already existing itinerary. We then loop through each flight within our flightList, that we access using the session variable ‘dashboard_id’, and reformat our departure and arrival dates and times into a readable format. Note that we know how many flights are contained within the list by simply checking the length of the list, using that as the range of our for loop. Finally, we run another for loop within the same range, inserting a new row for each flight within our list, passing in the itin_id we found earlier as our itin_id, session[‘user_id’] as the author_id, and then the corressponding value from each flight to its respective column in the database. To finish it all off, we commit the changes to the database using db.commit() and then close our connection to the databse before redirecting to our ‘itinDisp’ page where the user will see their saved itineraries.\n\n'''\nSaves a created itinerary and then reroutes to /allItins page. Only possible to be called if user is logged in.\nStill need to write code and finish HTML file.\nNOTE: May or may not be implemented\n'''\n@server.route('/saveItin')\n@login_required\ndef saveItin():\n    # Opening database connection\n    db = get_db()\n    # Using dummy variable to help assign itinerary ids\n    dumVar = 'textCount'\n    # Adding dummy variable to itineraryCounter table so we can figure out how many itineraries we have\n    db.execute(\n        'INSERT INTO itineraryCounter (counter) VALUES (?)',\n        (dumVar,))\n    # Commiting insertion into itineraryCounter\n    db.commit()\n    # Finding max itin_id from table and incrementing our variable by one for new itinerary\n    itin_id = db.execute('SELECT MAX(itin_id) FROM itineraries').fetchone()[0]\n    if itin_id is None:\n        itin_id = 1\n    else:\n        itin_id += 1\n\n    # Changing format of time for disply\n    for i in range(0, len(session['dashboard_id'])):\n        session['dashboard_id'][i]['depDate'] = dt.strptime(session['dashboard_id'][i]['depDate'], '%Y-%m-%dT%H:%M')\n        session['dashboard_id'][i]['depDate'] = session['dashboard_id'][i]['depDate'].strftime(\"%d/%m/%Y %H:%M\")\n        session['dashboard_id'][i]['arrTime'] = dt.strptime(session['dashboard_id'][i]['arrTime'], '%Y-%m-%dT%H:%M')\n        session['dashboard_id'][i]['arrTime'] = session['dashboard_id'][i]['arrTime'].strftime(\"%d/%m/%Y %H:%M\")\n\n    # Placing information into database for each flight in itinerary, all with same itin_id\n    for i in range (0, len(session['dashboard_id'])):\n        db.execute(\n            'INSERT INTO itineraries (itin_id, author_id, origin, destination, airline, depTime, arrTime) VALUES (?,?,?,?,?,?,?)',\n            (itin_id, session['user_id'], session['dashboard_id'][i]['origin'], session['dashboard_id'][i]['destination'], session['dashboard_id'][i]['airline'], session['dashboard_id'][i]['depDate'], session['dashboard_id'][i]['arrTime']))\n    # Commit and close the database\n    db.commit()\n    db.close()\n    # Sending user to itinerary page where they can see their newly saved itinerary\n    return redirect(url_for('itinsDisp'))\n\n\n\nDisplaying Saved Information\nNow that we have allowed the user to save their itineraries, we need a way for them to be able to see and access these itineraries. This is accomplished on our ‘dispItins’ page. In order to access a user’s itineraries, we need to access the database, hence we will again use the get_db() function to open up a connection with the database. We now need to create a list of all the flights saved under a certain user in our itineraries table. To accomplish this, we use our cursor from get_db() to execute a command that selects all of the columns from each flight, given that the author_id of each flight is the same as the user that is currently logged in. Remember that when a user is logged in, we set the session variable ‘user_id’ to be the username of the user, so we again use this session variable to select the flights to be included. We then utilize the command fetchall() to ensure that we are returned a list of all of the flights in itineraries saved by our desired user. Then, for ease of use when sending information from this page to other pages, we loop through our flights list and create a dictionary for each flight, similar to what we did in the previous pages, and append all of these dictionaries to list titled ‘flights_list’. Additionally, we create a list of all the unique itin_ids contained within the flight list. This helps us to determine how many itineraries are going to be displayed. This is critical as for each itinerary there is a button that allows the user to see a visualization of that itinerary, so we now know how many buttons we will need to consider. With this information, we can now check what button the user inputted by using a for loop, with the button identification being an f-string, utilizing the i value. Now, we use the button value as a way to index our list containing the itinerary ids to find which itinerary the user would like to see. We then run through the flights_list and append any flight with the correct itin_id to a list titled dashList that will contain all the flights we wish to visualize. Now that we have selected all the flights in the itinerary that we plan on visualizing, we set the session variable ‘dashboard_id’ equal to our dashList, and obtain the hour value for the departure time of our first flight to be ‘dateDash’, allowing the Dash app to access and visualize this itinerary. Finally, we reroute the user to the Dash app where they will be shown the visualization for their itinerary.\n\n'''\nPage displays all the itineraries for the logged in user.\n'''\n@server.route('/dispItins', methods =('GET', 'POST'))\n@login_required\ndef itinsDisp():\n    # Opening connection with database\n    db = get_db()\n\n    # Getting list of all flights in itineraries created by current user\n    flights = db.execute(\n        'SELECT f.id, f.itin_id, author_id, origin, destination, airline, depTime, arrTime'\n        ' FROM itineraries f WHERE author_id = ?', (session['user_id'],)\n    ).fetchall()\n\n    # Initializing empty list that will contain flight information\n    flights_list = []\n    # Converting information for flights from database into a dictionary for each flight\n    for flight in flights:\n        flight_dict = {\n            'id': flight['id'],\n            'itin_id': flight['itin_id'],\n            'author_id': flight['author_id'],\n            'origin': flight['origin'],\n            'destination': flight['destination'],\n            'airline': flight['airline'],\n            'depTime': flight['depTime'],\n            'arrTime': flight['arrTime']\n        }\n\n        # Adding flight dictionaries to flights_list\n        flights_list.append(flight_dict)\n\n    # Obtaining all unique itinerary ids in user's itineraries\n    itin_ids=[]\n    for flight in flights:\n        if flight['itin_id'] not in itin_ids:\n            itin_ids.append(flight['itin_id'])\n\n\n    if request.method=='POST':\n        dashList = []\n        # Run through number of buttons\n        for i in range(0,len(itin_ids)+1):\n\n            if request.form.get('action') == f'See Itinerary {i}':\n                # Get itinID for selected itinerary\n                retItinID = itin_ids[i-1]\n                # Compile all flights in that itinerary\n                for el in flights_list:\n                    if el['itin_id']==retItinID:\n                        dashList.append(el)\n\n                # Assign session variables to be used by the dash app\n                sesDate = dt.strptime(dashList[0]['depTime'], '%d/%m/%Y %H:%M')\n                session['dashboard_id'] = dashList\n                session['dateDash'] = sesDate.hour\n                # Redirecting to Dash app\n                return redirect(url_for('/dashFlight/'))\n\n    return render_template('blog/itinsDisp.html', flights = flights)\n\nBelow is a screenshot of a user’s itinerary page. We can see that there are multiple ‘See itinerary’ buttons, which are handled within the code block above. \n\n\n\nitinDispScreenshot.png"
  },
  {
    "objectID": "posts/project/index.html#adding-interactive-visualizations-with-plotly-dash",
    "href": "posts/project/index.html#adding-interactive-visualizations-with-plotly-dash",
    "title": "Final Project: Flight Site",
    "section": "4. Adding Interactive Visualizations with Plotly Dash",
    "text": "4. Adding Interactive Visualizations with Plotly Dash\nThis Plotly Dash app is an interactive tool designed to give users insights into flight delays and travel patterns across U.S. airports.\nWe pulled the data from the database we created as described in part 2, and used this data to create our visualizations.\nFor faster processing time, we decided to process the data first and writing them out to separate CSV files that were suited for each visualization instead of processing the data within our Dash app.\nIt features three core visualizations:\n\nFlight Routes Visualization: Users can input up to ten pairs of departure and arrival airport codes to plot the routes on a map. The routes are color-coded by average delay proportions, helping users identify which routes typically experience more delays.\nHeatmap: This displays a heatmap overlay on a U.S. map, showing the volume of flights departing from each airport (denoted by the size of the circles) and the proportion of those flights that are delayed (indicated by the color intensity).\nRush Hour Analysis: By entering an airport code and a specific hour, users can generate a bar chart that reveals the busiest times for departures at that airport, providing insights into peak travel hours and potential rush times.\n\nThe app’s layout includes a markdown block at the top that explains the functionalities and how to use the app. A RadioItems selection lets users choose between the flight routes, heatmap, or rush hour visualizations, dynamically updating the display content based on their choice.\nCallbacks are set up to respond to user interactions, such as entering airport codes and clicking the “Plot Routes” button, which generates the visualizations accordingly. For the heatmap and rush hour charts, the app processes flight count and delay data, providing a detailed analysis of travel patterns for better planning and decision-making. The app is equipped to handle data transformations and plotting, making it a comprehensive tool for travelers looking to optimize their itineraries.\n\nHere is a more detailed look at each visualization\n\n\nFlight Routes Visualization:\nThis app allows you to visualize flight routes between airports and the average proportion of delays. Enter the airport codes for departures and arrivals, and press “Plot Routes” to see the routes on the map.\nEach number in the legend is a group number that represents the proportion of delayed flights on average for each route.\nHere is what each number in the legend means:\n    - 0: delay proportion &lt;= 0.1\n    - 1: 0.1 &lt; delay proportion &lt;= 0.15\n    - 2: 0.15 &lt; delay proportion &lt;= 0.2\n    - 3: 0.2 &lt; delay proportion &lt;= 0.25\n    - 4: 0.25 &lt; delay proportion &lt;= 0.3\n    - 5: delay proportion &gt; 0.3\n\n# @title Flight Routes Visualization\nfrom IPython.display import Image, display\n\n# Correct path with no spaces in folder names\nimage_path = '/content/drive/MyDrive/PIC16B_Datasets/routes.png'\n\n# Display the image\ndisplay(Image(filename=image_path))\n\n\n\n\n\n\n\n\nCallback: update_map\nThis callback listens for user interaction with the ‘Plot Routes’ button or changes in the visualization mode selector. Upon activation, it processes user input to plot flight routes between selected departure and arrival airports. The callback assembles a list of route data based on the input fields for up to 10 routes.\n\n# Callback to update the map based on the inputs\n@app.callback(\n    #Output('content-route', 'children'),\n    Output('map', 'figure'),\n    [Input('vis-mode-selector', 'value'),\n     Input('plot-button', 'n_clicks')\n    ],\n    [State({'type': 'departure-input', 'index': ALL}, 'value'),\n     State({'type': 'arrival-input', 'index': ALL}, 'value'),\n    ]\n)\n\nFunction: update_map\nThis function is the core of the update_map callback. It transforms user input into uppercase to match the database format, constructs a key for each route combining departure and arrival airport codes, and retrieves relevant data like group classification, flight count, and delay proportion. It then checks for the existence of coordinates for the given airports and, if found, prepares a structured dictionary of route information to be plotted.\n\ndef update_map(n_clicks, vis_mode, departures, arrivals):\n    \"\"\"\n    Responds to user inputs to generate and update a flight route map.\n\n    Parameters:\n    - n_clicks (int): Number of times the plot button has been clicked.\n    - vis_mode (str): The visualization mode selected by the user.\n    - departures (list): List of user-input departure airport codes.\n    - arrivals (list): List of user-input arrival airport codes.\n\n    Returns:\n    - Plotly Figure: A figure object that contains the updated flight routes map.\n    \"\"\"\n    routes = []\n\n    departures = [dep.upper() for dep in departures if dep is not None]\n    arrivals = [arr.upper() for arr in arrivals if arr is not None]\n\n    # loop through each pair of depature and arrival inputs\n    for dep, arr in zip(departures, arrivals):\n        if dep and arr:  # Ensure both inputs are provided\n            # Generate the composite key for the current route\n            route_key = f\"{dep}_{arr}\"\n            # Look up the group for the current route\n            group = route_dict.get(route_key)\n            count = count_dict.get(route_key)\n            delay_proportion = dep_del_dict.get(route_key)\n            dep_coords = airport_coordinates.get(dep)\n            arr_coords = airport_coordinates.get(arr)\n\n            # Proceed only if coordinates for both airports are found\n            if dep_coords and arr_coords:\n                # Construct the route data structure\n                route = {\n                    \"departure_airport\": dep,\n                    \"arrival_airport\": arr,\n                    \"departure_lat\": dep_coords['lat'],\n                    \"departure_lon\": dep_coords['lon'],\n                    \"arrival_lat\": arr_coords['lat'],\n                    \"arrival_lon\": arr_coords['lon'],\n                    \"delay_proportion\": delay_proportion,\n                    \"group\": group,\n                    \"flight_count\": count\n                }\n                routes.append(route)\n\n    fig = create_figure_with_routes(routes)\n\n    return fig\n\nFunction: create_figure_with_routes\nThis function takes the list of routes created by update_map and visualizes them on a map. Each route is represented as a line between its departure and arrival coordinates with markers at each airport. The lines and markers are color-coded by delay proportion group. The function sets up the map’s appearance, including its geographic projection (set to the United States) and styling details. The resulting figure is then returned to the update_map callback to update the ‘map’ component in the app’s layout.\n\ndef create_figure_with_routes(routes):\n    \"\"\"\n    Creates a Plotly map visualization for the given flight routes.\n\n    Parameters:\n    - routes (list): A list of dictionaries, each containing data for a flight route.\n\n    Returns:\n    - Plotly Figure: A figure object that visualizes the flight routes on a map.\n    \"\"\"\n    fig = go.Figure()\n    # Define a color scheme for the different groups\n    group_colors = {\n        0: \"#1f77b4\",  # Muted blue\n        1: \"#ff7f0e\",  # Safety orange\n        2: \"#2ca02c\",  # Cooked asparagus green\n        3: \"#d62728\",  # Brick red\n        4: \"#9467bd\",  # Muted purple\n        5: \"#8c564b\",  # Chestnut brown\n    }\n    # Loop through each route and add it to the figure with the respective color\n    for route in routes:\n        # Get the color for the current group or default to black if not found\n        route_color = group_colors.get(route[\"group\"])\n\n        fig.add_trace(\n            go.Scattergeo(\n                lon = [route[\"departure_lon\"], route[\"arrival_lon\"]],\n                lat = [route[\"departure_lat\"], route[\"arrival_lat\"]],\n                text = [f\"{route['departure_airport']}\", f\"{route['arrival_airport']}\"],\n                hoverinfo='text',\n                mode = \"lines+markers\",\n                line = dict(width = 2, color = route_color),\n                marker = dict(size = 4, color = route_color),\n                name = route[\"group\"],\n            )\n        )\n        # Update layout of the map\n    fig.update_layout(\n        title_text = \"Flight Routes and Delay Proportions\",\n        showlegend = True,\n        geo = dict(\n            projection_type = \"albers usa\",\n            showland = True,\n            landcolor = \"rgb(200, 200, 200)\",\n            countrycolor = \"rgb(204, 204, 204)\",\n            showsubunits=True,  # Show state lines and other subunits\n            subunitwidth=1  # Width of the subunit lines (state lines)\n        ),\n    )\n    return fig\n\n\n\nHeatmap Visualization:\nThe heatmap illustrates U.S. airport departures, highlighting flight volume and delay frequency.\nLarger circles denote more flights; color intensity reflects higher delay percentages.\n\n# @title Heatmap\n\n# Correct path with no spaces in folder names\nimage_path1 = '/content/drive/MyDrive/PIC16B_Datasets/heatmap.png'\n\n# Display the image\ndisplay(Image(filename=image_path1))\n\n\n\n\n\n\n\n\nCallback: create_composite_map\nThis callback updates the content of the ‘content-heatmap’ division based on the visualization mode selected by the user. When the ‘Heatmap’ mode is selected, it triggers the create_composite_map function to generate and display a heatmap.\n\n@app.callback(\n    Output('content-heatmap', 'children'),\n    Input('vis-mode-selector', 'value')\n)\n\nFunction: create_composite_map\nThe create_composite_map function constructs a heatmap visualization of U.S. flight departures. It uses the Scattergeo trace of Plotly to plot the longitude and latitude of origin airports as points on a map.\nEach point’s size represents the delay proportion, offering a visual representation of the flight volume and delay frequency. The color intensity corresponds to higher delay percentages.\nThe hovertemplate enriches the points with interactive data display on hover, showing the flight count and delay proportion for each airport. The function returns a Plotly figure object configured with a title and a stylized geographical layout, ready to be rendered in the app.\n\ndef create_composite_map():\n    \"\"\"\n    Generates a heatmap Plotly figure displaying U.S. airport flight delays.\n\n    Size of points reflects flight count; color indicates delay proportions.\n\n    Returns:\n    - Plotly Figure: A map with scaled markers for visualizing flight delays.\n    \"\"\"\n\n    fig = go.Figure(data=go.Scattergeo(\n        lon = dep_delay['ORIGIN_LONGITUDE'],\n        lat = dep_delay['ORIGIN_LATITUDE'],\n        text = dep_delay['ORIGIN'],\n        customdata = dep_delay[['flight_count', 'DEP_DEL15']],  # Add flight count and delay proportions to the custom data\n        hovertemplate = (\n            \"&lt;b&gt;%{text}&lt;/b&gt;&lt;br&gt;\"\n            \"Flight Count: %{customdata[0]}&lt;br&gt;\"\n            \"Delay Proportion: %{customdata[1]:.2f}&lt;extra&gt;&lt;/extra&gt;\"  # Format delay proportion to show two decimal places\n        ),\n        marker = dict(\n            size = dep_delay['DEP_DEL15'] * 50,  # Scale the points based on delay proportion\n            color = dep_delay['DEP_DEL15'],\n            colorscale = 'Viridis',\n            showscale = True,\n            colorbar_title = 'Delay Proportion'\n        )\n    ))\n\n    fig.update_layout(\n        title = 'Heatmap of Flight Delay Proportions',\n        geo = dict(\n            scope = 'usa',\n            projection_type = 'albers usa',\n            showland = True,\n            landcolor = 'rgb(217, 217, 217)',\n            subunitcolor = \"rgb(255, 255, 255)\"\n        )\n    )\n\n\n\n    return fig\n\n\n\nRush Hour:\nThis app offers insights into the frequency and peak hours of flight departures from specific airports.\nBy inputting an airport code and a flight’s departure time, users can generate a bar chart that reveals the airport’s busiest periods, aiding in understanding rush hour trends.\n\n# @title Rush Hour\nfrom IPython.display import Image, display\n\n# Correct path with no spaces in folder names\nimage_path2 = '/content/drive/MyDrive/PIC16B_Datasets/rush_hour.png'\n\n# Display the image\ndisplay(Image(filename=image_path2))\n\n\n\n\n\n\n\n\nCallback: update_hourly_activity\nThis callback updates the histogram visualization for hourly flight activity based on user interaction. It triggers when the user selects a visualization mode and clicks the ‘Update’ button. The callback receives the airport code and hour input by the user and passes this information to the update_hourly_activity function to refresh the histogram display.\n\n@app.callback(\n    Output('hist', 'figure'),\n    [Input('vis-mode-selector', 'value'),\n     Input('update-button', 'n_clicks')],\n    [State('origin-input', 'value'), State('hour-input', 'value')]\n)\n\nFunction: update_hourly_activity\nThis function creates a histogram to display the number of flights for each hour from a specified airport. It takes user inputs for the airport and hour, converts the airport code to uppercase, filters the dataset for the selected airport, and constructs a bar chart. If an hour is provided, it highlights that hour on the chart. The function returns a Plotly figure object with the updated histogram for rendering in the app.\n\ndef update_hourly_activity(n_clicks, vis_mode, selected_origin, selected_hour):\n    \"\"\"\n    Generates a histogram figure of flight counts for each hour of the day based on user-selected origin and hour.\n\n    This function filters the data for the specified airport origin and hour, then produces a bar plot showing the\n    number of flights departing at each hour of the day. If an hour is selected, it highlights that hour on the histogram.\n\n    Parameters:\n    - n_clicks (int): Number of clicks received. This parameter can be used to trigger the function in a callback.\n    - vis_mode (str): The visualization mode. Currently unused in the function but can be utilized for future modes.\n    - selected_origin (str): The airport origin code selected by the user.\n    - selected_hour (int/str): The hour selected by the user for highlighting in the histogram.\n\n    Returns:\n    - fig (plotly.graph_objs._figure.Figure): A Plotly figure object containing the histogram of hourly flight activity.\n    \"\"\"\n\n    if selected_origin is not None:\n        selected_origin = selected_origin.upper()\n\n    # Otherwise, generate the histogram for the selected origin and hour\n    filtered_df = dep_count[dep_count['ORIGIN'] == selected_origin]\n\n    # Create a barplot of flights by hour\n    # First, we create the text that will be displayed on each bar\n    filtered_df['text'] = 'Airport: ' + filtered_df['ORIGIN'] \\\n                      + '&lt;br&gt;Hour: ' + filtered_df['dep_hour'].astype(str) \\\n                      + '&lt;br&gt;Flights: ' + filtered_df['dep_count'].astype(str)\n\n    # Now we can create the bar plot\n    fig = px.bar(filtered_df, x='dep_hour', y='dep_count', title='Hourly Flight Activity')\n\n    fig.update_layout(xaxis_title='Departure Hour', yaxis_title='Numer of Flights')\n    # To add hover text, you can use the hover_data parameter\n    fig.update_traces(hovertemplate=filtered_df['text'])\n\n    # Highlight the selected hour if one is selected\n\n    try:\n        selected_hour = int(selected_hour)\n        if 0 &lt;= selected_hour &lt;= 23:\n            fig.add_vline(x=selected_hour, line_color=\"red\", annotation_text=\"Selected Hour\")\n    except (ValueError, TypeError):\n        pass\n\n    return fig"
  },
  {
    "objectID": "posts/project/index.html#conclusions",
    "href": "posts/project/index.html#conclusions",
    "title": "Final Project: Flight Site",
    "section": "6. Conclusions",
    "text": "6. Conclusions\nIn conclusion, we created a Flask web app that helps users predict whether or not their flight will be delayed. Our project aims to predict flight delays, offering substantial benefits to travelers and airports by providing more accurate departure times and enhancing planning efficiency. However, it faces challenges such as data quality and availability, computational limitations, and the complexity of developing predictive models. Mitigating these risks requires thorough data assessment, scalable processing solutions, and flexible project scope management. Ethically, while the project presents an opportunity to improve the travel experience for passengers and operational efficiency for airports, it may also highlight airlines’ operational shortcomings, potentially impacting their reputation. Despite these considerations, the project stands to make a positive impact by enabling better-informed travel decisions and streamlining airport traffic management, contributing to a more predictable and less stressful travel experience for all involved. As far as any ethical concerns, there are very few that we have been able to identify. The only such concern may be that the airlines would not be happy with their constant delays being exposed."
  },
  {
    "objectID": "posts/bruinPage/index.html",
    "href": "posts/bruinPage/index.html",
    "title": "Analysis and Recommendations of Web Scraped Movie Data",
    "section": "",
    "text": "In this blog post, I’m going to create a web scraper that will allow us to recommend new movies based on shared actors with your favorite movie.\nThe first thing we are going to do is navigate to the website called TMDB, found at the URL https://www.themoviedb.org/ . We can now pick our favorite movie that will be the basis of our recommendations, mine is Harry Potter and the Philosopher’s Stone. Next, I navigated to the main movie page found at: https://www.themoviedb.org/movie/671-harry-potter-and-the-philosopher-s-stone This URL is important to save for later as it will be the starting point for our spider. Now we can start to create our spider. The first step is to type the following into the terminal:\nconda activate PIC16B-24W\nscrapy startproject TMDB_scraper\ncd TMDB_scraper\nThis command will activate the PIC16B-24W environment that we installed previously, create a scrapy project with the name TMDB_scraper on your computer, and then change the directory of your terminal to your newly created project. As we navigate to the TMDB_scraper folder, we see that there are various files within the project, however first we want to navigate to the file titled ‘settings.py’, and add the following line:\n    CLOSESPIDER_PAGECOUNT = 20\nThis will limit the number of pages our spider will visit initially, limiting the amount of data it will acquire when we are constructing and debugging. Additionally, I wrote the line:\nUSER_AGENT = 'Mozilla/5.0 (iPad; CPU OS 12_2 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Mobile/15E148'\nThis line helps to minimize the chance that the website identifies your spider as a bot and tries to block the web scraping process. Finally, it is time to begin writing our spider. Inside of our project folder, navigate into the spiders directory and create a new file titled ‘tmdb_spider.py’, and add the following chunk of code to the file:\nimport scrapy\n\n\nclass TmdbSpider(scrapy.Spider):\n\n    name = 'tmdb_spider'\n    \n    def __init__(self, subdir=None, *args, **kwargs):\n    \n        self.start_urls = [f\"https://www.themoviedb.org/movie/{subdir}/\"]\nThis code chunk will allow us to run the spider for our movie of choice from the terminal by giving the subdirectory on the TMDB website as our extra argument. Now that we have a working spider, it is time to implement our three parsing methods: parse(self, response), parse_full_credits(self, response), and parse_actor_page(self, response).\nLet’s start with the parse(self, response) function. The role of this function is to navigate from our main movie page to the ‘Full Cast and Crew’ page. If we go back to our main movie page, and navigate to the ‘Full Cast and Crew’ page, we can see that the only change in our URL is there is now a ‘/cast’ at the end. That tells us, all we need to do to get to our cast page is to add ‘/cast’ to the end of our current URL, and send our spider to the page found by our new URL. As we can see in the function below, we define our new URL to be called ‘full_credits_url’ which is created by taking ‘response.url’, the URL of the page we’re on, and adding the string ‘/cast’ to the end. As we now have the URL for the page we want to navigate to, we yield a scrapy.Request to send our spider to the cast page, passing our new ‘full_credits_url’ as the URL parameter, and setting the callback to the parse_full_credits() function as we will now navigate to the full credits page.\ndef parse(self, response):\n\n        '''\n        This function takes us from the main movie page to the cast and credits page yielding the url of the full credit page to be parsed by the parse_full_credits\n        '''\n        \n        #modifies url in order to navigate to the cast page\n        \n        full_credits_url = response.url + '/cast'\n        \n        #sends spider to the full credit page\n        \n        yield scrapy.Request(url=full_credits_url, callback=self.parse_full_credits)\nNow that we have made it onto the full cast and credits page, we will work with the parse_full_credits(self, response) function that was called as we navigated to this page. The goal of this function is to find all the actors who played a role in the movie and send our spider to their individual pages to be parsed by our final function parse_actor_page(). Last time we were able to navigate to the cast page by simply adding ‘/cast’, however as each actor has their own distinct name and page, we will not be able to hardcode the new URL. In fact if you click on the first actor or actress in the list, in my case Daniel Radcliffe, we see that the URL for their individual page contains a seemingly arbitrary number followed by their name. In order to find the extension for each actor, let us navigate back to the full cast and credits page. By right clicking on the top actor or actress’s name, followed by clicking inspect, we can see where their name is stored in the HTML file. More importantly, just before where their name is stored in text, we see something similar to ‘ Daniel Radcliffe’. Should we navigate back to Mr. Radcliffe’s page, we can see that the the element defined as ‘href’ is the same as the URL extension for the page, after the ‘https://www.themoviedb.org/’ that all pages on the TMDB site begin with. Clearly this is critical information as it will allow us to navigate to the actor or actress’s page using their URL extension. Now, we simply need to find a way to access this information for all of the actors. As we go back to the cast and credits page and look at the line where the first name was stored, we can traverse backwards to the line starting with “&lt;li data-order=”0” data-credit-id=” and then up again to the line reading ‘&lt;ol class = “people credits”&gt;’. It is important to note the information of all the cast members, including their names and the ‘href’ URL extensions we are trying to extract are all located under this ol class. We want to access this class from the HTML file in order to use it in our function, which we will be able to accomplish using css selectors. By taking a look at the first line of code, we can see that in the selector, we start in the overarching ol class, before specifying to travel down through the li class and finally asking for the attribute ‘href’ located within a. One thing to note about this line is the ‘:not(.crew)’ following the ol class. This effectively tells the program that all we do not want the attributes of any of the crew members, but only the attributes of the actors. Additionally, the .extract() command at the end of the line effectively gathers the information from the location we specified in the selector, allowing us to assign it to our variable actor_selectors. This command returns a list of the URL extensions for every actor in the movie and stores it in the actor_selectors variable. Now that we have the URL extensions, we can finish this function in a similar manner to the initial parse() function by creating new URLs for the spider to follow. As we observed before, every actor page starts with ‘https://www.themoviedb.org/’, and is followed by a tail unique to each actor which we have stored in the actor_selectors list. Now, to send our spider to each actor page, we simply write a for loop that iterates through all of the actor extensions in our actor_selectors list, add the extension to the generic URL started mentioned above, and again send the spider to the new page addressed by our new URL saved as actor_page_url. Our yield scrapy.Request() is slightly different than the one we used in the previous function as instead of calling parse_full_credits(), we are going to call parse_actor_page() as the spider is now traveling to an individual actor page.\n def parse_full_credits(self, response):\n \n        '''\n        This function navigates from the response's full credits page to each actor's individual page to be parsed in the parse_actor_page() function.\n        '''\n        \n        #creates list of cast members, excluding the crew members\n        \n        actor_selectors = response.css('ol.people.credits:not(.crew) li div.info a::attr(href)').extract()\n        \n        #for loop iterates over actor list sending spider to each actor page\n        \n        for actor_ in actor_selectors:\n        \n            actor_page_url = 'https://www.themoviedb.org'+actor_\n            \n            yield scrapy.Request(url=actor_page_url, callback=self.parse_actor_page)\nWe have now reached our third and final parsing function, parse_actor_page(self, response). The goal of this function is to identify all the movies in which the individual played an acting role and yield dictionaries that contain the individual’s name with the title of the movie they played a role in. The first thing we must do is identify the name of the actor or actress whose page we are on. This will be a similar process to finding the actor page references in the previous function, however this time there is only one piece of data we wish to extract, making it even simpler. Let us continue with our Daniel Radcliffe example from earlier by navigating back to his actor page. By highlighting his name at the top of the page and then clicking on the inspect option, we can again see where his name is stored in the HTML file associated with this page. We can see it is located as an attribute under a class titled ‘h2 class=”title”&gt;’ which we will again be able to access via css selectors. As we can see from the first line of code below, in the css selector we start at the h2 title class and then traverse to the ‘a’ section in which the name is located. Different from last time, we add the command ‘::text’ afterwards to tell the program we want the text stored at this location as opposed to the href attribute which is what we were looking for in the previous function. It is also important to note that here we are using .extract_first() instead of .extract() as we want the name itself as a string instead of a list containing the name, which is accomplished by .extract_first() which extracts the first element of the list to be returned while .extract() would grab the entire list. The .strip() at the end of the line simply ensures that there is no whitespace before or after the name to keep everything neat and consistent. We now have the name of the individual who the page belongs to stored under the variable actor_name, meaning that now we only need to find the names of the movies in which they played an acting role. We will now again implement a similar method to extract the titles of the movies the actor played a role in. If we highlight the title of the first movie or show under the ‘Acting’ section and check where it is located in the HTML file, we find it is under one of many sections labeled ’\n\n’ which are all under a table class titled ‘credits list’. While we could try and simply use the css selectors to try and locate this class as before, we must notice that there are two other classes with the same title, which would clearly make it somewhat tricky for the program to decipher exactly which one we are trying to access. Taking a closer look at these three classes, we can observe an h3 line above each one which are identical except for one word in each, the different words being ‘Acting’, ‘Production’, and ‘Crew’. Clearly we want to access the one referring to the acting roles, as we are not concerned with production and crew roles, however we need to find a way to access the correct table under acting. By taking another glance at the HTML file, we can see that the three classes of interest are all located within a div class titled ‘credits_list’. We can take advantage of this by creating a list of all the h3 terms under this list via the command defining datCred. Now that we have the list, we can simply search for which h3 contains the word ‘Acting’ to find exactly which table we want by using a for loop to iterate through each h3 element in datCred. We check for acting by checking if ‘Acting’ is in the text of the h3 element, as found in the condition of the if statement within the for loop. Now that we have identified which h3 term we want to follow, we will take advantage of xpath, another kind of selector, which has a useful function called ‘following-sibling’. This specifier tells the selector to follow a sibling class, a class with the same parent and on the same level as the initially specified node, and we have then included the xpath to ‘table[1]’ in which the tr classes containing the movie names are located, saving it to the variable txt. We then use the Selector() function to make it so that we can access just the data under the location we specified as opposed to the entire response that scrapy has found. It is worth noting that to run this command we must import the selector library from scrapy.Selector at the top of the page, with the line of code shown just below:\nfrom scrapy.selector import Selector\nFinally, we use the subset of the response to extract the movie titles for the actor. This process is shown in the final for loop. We call all elements in ‘tablePick.css(‘table.credit_group tr’)’ to access all of the tr classes individually, and then set the movOrTvName to the text under the class ‘role’ and within the ‘tooltip’ class and as the bdi attribute in the HTML file. Finally, we yield a dictionary with the actor name that we found in the beginning of the function, and the movie name that we just found. This will loop through all of the movies that the actor played a character in, resulting in all of their movies being represented in their own dictionaries to be saved in a .csv file we will create next.\ndef parse_actor_page(self, response):\n\n        '''\n        This function parses the actor page, extracting both the actor name and the movies/shows they were in, yielding the dictionaries with the actor name and the movies they appear in.\n        '''\n        \n        #extracts actor name from page\n        \n        actor_name = response.css('h2.title a::text').extract_first().strip()\n        \n        #list of data under actor acting appearances\n        \n        datCred = response.css('div.credits_list h3')\n        \n        #filtering to just acting roles\n        \n        for h3 in datCred:\n        \n            if 'Acting' in h3.xpath('./text()').get():\n            \n                txt = h3.xpath('following-sibling::table[1]').get()\n                \n                break\n                \n        #making tablePick a selector used to follow to movie data\n        \n        tablePick = Selector(text = txt)\n        \n        #extract and yield movie/show name with the actor name\n        \n        for cred in tablePick.css('table.credit_group tr'):\n        \n            movOrTvName = cred.css('td.role a.tooltip bdi::text').get().strip()\n            \n            yield{'actor': actor_name, 'movie_or_TV_name' : movOrTvName}\nNow that we have finished writing our spider, we are ready to run it in order to scrape our desired data off of the TMDB website. The first thing we need to do is go back into the settings.py file and comment out the page count limit we implemented just before creating our spider to allow our spider to scrape all of the pages it is sent to. We can now go back to the terminal and run the following command:\nscrapy crawl tmdb_spider -o results.csv -a subdir=671-harry-potter-and-the-philosopher-s-stone\nNote that the command is all one on the same line. This command will tell our spider to crawl on the website for the first Harry Potter movie as given by the subdirectory, and will write all of the results in a file named ‘results.csv’. If we go back into the project folder, we can open the file we just created and we will see dictionaries represented by a table with one column containing the actor names and the other column containing the titles of the movies that respective actor appeared in. This is where we will access the results our spider was able to produce. We can now use this data to create our recommendations.\nOur first step in creating our recommendations is importing our necessary packages and reading in the data we just acquired and saved into results.csv, which can be done via the following code:\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n#reading in the data we scraped using our spider\ndf = pd.read_csv('/Users/jakebrowning/Desktop/PIC16B/TMDB_scraper/results.csv')\n#using for loop to create a counting function for how many actors are in each movie\nmovDict = {}\nfor el in df1:\n    if el in movDict.keys():\n        movDict[el] = movDict[el]+1\n    else:\n        movDict[el] = 1\n\n#adjusting my dictionaries for the pandas dataframe\nsortedMovDict = sorted(movDict.items(), key=lambda x:x[1], reverse=True)\nconvertedMovDict = dict(sortedMovDict)\n\n#putting the counted dictionary into a pandas dataframe and displaying the dataframe\ndfList = pd.DataFrame(list(convertedMovDict.items()),columns=['Movie Name','Number of Shared Actors'])\ndfList\nWe can see this yields a list of movies containing shared actors, ordered from most shared actors to least shared actors with our original movie.\n\n\n#creating lists of the top twenty movies and their numbert of actors\nmovN = []\nnumAct = []\ni=0\nwhile i &lt; 20:\n    movN.append(dfList['Movie Name'][i])\n    numAct.append(dfList['Number of Shared Actors'][i])\n    i = 1+i\n\n#changing order of list to be better displayed in the graph\nmovN.reverse()\nnumAct.reverse()\n\n#creating bar chart with top twenty movie recommendations, displaying how many shared actors in each movie\nmyChart = plt.barh(movN, numAct)\nplt.bar_label(myChart, labels=numAct, label_type=\"edge\")\nplt.title('Top Twenty Movie Recommendations')\nplt.xlabel('Number of Shared Actors')\nplt.ylabel('Movie Title')\nplt.show()\nThis leaves us with our top twenty recommendations, shown in decreasing order by the bar chart displayed below."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "16bhwblog",
    "section": "",
    "text": "Final Project: Flight Site\n\n\n\n\n\n\nweek 10\n\n\n\n\n\n\n\n\n\nMar 22, 2024\n\n\nJake Browning\n\n\n\n\n\n\n\n\n\n\n\n\nHW 6: Fake News Detector\n\n\n\n\n\n\nweek 10\n\n\n\n\n\n\n\n\n\nMar 10, 2024\n\n\nJake Browning\n\n\n\n\n\n\n\n\n\n\n\n\nHW 5: Image Classification with Keras\n\n\n\n\n\n\nweek 9\n\n\n\n\n\n\n\n\n\nMar 4, 2024\n\n\nJake Browning\n\n\n\n\n\n\n\n\n\n\n\n\nHW 4: Heat diffusion with Jax\n\n\n\n\n\n\nweek 6\n\n\n\n\n\n\n\n\n\nFeb 22, 2024\n\n\nJake Browning\n\n\n\n\n\n\n\n\n\n\n\n\nHomework 3: Flask\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nFeb 14, 2024\n\n\nJake Browning\n\n\n\n\n\n\n\n\n\n\n\n\nPost With Code\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\nFeb 11, 2024\n\n\nHarlow Malloc\n\n\n\n\n\n\n\n\n\n\n\n\nAnalysis and Recommendations of Web Scraped Movie Data\n\n\n\n\n\n\nweek 6\n\n\n\n\n\n\n\n\n\nFeb 11, 2024\n\n\nJake Browning\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nFeb 8, 2024\n\n\nTristan O’Malley\n\n\n\n\n\n\nNo matching items"
  }
]