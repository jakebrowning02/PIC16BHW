[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/index.html",
    "href": "posts/index.html",
    "title": "Analysis and Recommendations of Web Scraped Movie Data",
    "section": "",
    "text": "In this blog post, I’m going to create a web scraper that will allow us to recommend new movies based on shared actors with your favorite movie.\nThe first thing we are going to do is navigate to the website called TMDB, found at the URL https://www.themoviedb.org/ . We can now pick our favorite movie that will be the basis of our recommendations, mine is Harry Potter and the Philosopher’s Stone. Next, I navigated to the main movie page found at: https://www.themoviedb.org/movie/671-harry-potter-and-the-philosopher-s-stone This URL is important to save for later as it will be the starting point for our spider. Now we can start to create our spider. The first step is to type the following into the terminal:\nconda activate PIC16B-24W\nscrapy startproject TMDB_scraper\ncd TMDB_scraper\nThis command will activate the PIC16B-24W environment that we installed previously, create a scrapy project with the name TMDB_scraper on your computer, and then change the directory of your terminal to your newly created project. As we navigate to the TMDB_scraper folder, we see that there are various files within the project, however first we want to navigate to the file titled ‘settings.py’, and add the following line:\n    CLOSESPIDER_PAGECOUNT = 20\nThis will limit the number of pages our spider will visit initially, limiting the amount of data it will acquire when we are constructing and debugging. Additionally, I wrote the line:\nUSER_AGENT = ‘Mozilla/5.0 (iPad; CPU OS 12_2 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Mobile/15E148’\nThis line helps to minimize the chance that the website identifies your spider as a bot and tries to block the web scraping process. Finally, it is time to begin writing our spider. Inside of our project folder, navigate into the spiders directory and create a new file titled ‘tmdb_spider.py’, and add the following chunk of code to the file:\nimport scrapy\nclass TmdbSpider(scrapy.Spider):\n\n    name = 'tmdb_spider'\n    \n    def __init__(self, subdir=None, *args, **kwargs):\n    \n        self.start_urls = [f\"https://www.themoviedb.org/movie/{subdir}/\"]\nThis code chunk will allow us to run the spider for our movie of choice from the terminal by giving the subdirectory on the TMDB website as our extra argument. Now that we have a working spider, it is time to implement our three parsing methods: parse(self, response), parse_full_credits(self, response), and parse_actor_page(self, response).\nLet’s start with the parse(self, response) function. The role of this function is to navigate from our main movie page to the ‘Full Cast and Crew’ page. If we go back to our main movie page, and navigate to the ‘Full Cast and Crew’ page, we can see that the only change in our URL is there is now a ‘/cast’ at the end. That tells us, all we need to do to get to our cast page is to add ‘/cast’ to the end of our current URL, and send our spider to the page found by our new URL. As we can see in the function below, we define our new URL to be called ‘full_credits_url’ which is created by taking ‘response.url’, the URL of the page we’re on, and adding the string ‘/cast’ to the end. As we now have the URL for the page we want to navigate to, we yield a scrapy.Request to send our spider to the cast page, passing our new ‘full_credits_url’ as the URL parameter, and setting the callback to the parse_full_credits() function as we will now navigate to the full credits page.\ndef parse(self, response):\n\n        '''\n        This function takes us from the main movie page to the cast and credits page yielding the url of the full credit page to be parsed by the parse_full_credits\n        '''\n        \n        #modifies url in order to navigate to the cast page\n        \n        full_credits_url = response.url + '/cast'\n        \n        #sends spider to the full credit page\n        \n        yield scrapy.Request(url=full_credits_url, callback=self.parse_full_credits)\nNow that we have made it onto the full cast and credits page, we will work with the parse_full_credits(self, response) function that was called as we navigated to this page. The goal of this function is to find all the actors who played a role in the movie and send our spider to their individual pages to be parsed by our final function parse_actor_page(). Last time we were able to navigate to the cast page by simply adding ‘/cast’, however as each actor has their own distinct name and page, we will not be able to hardcode the new URL. In fact if you click on the first actor or actress in the list, in my case Daniel Radcliffe, we see that the URL for their individual page contains a seemingly arbitrary number followed by their name. In order to find the extension for each actor, let us navigate back to the full cast and credits page. By right clicking on the top actor or actress’s name, followed by clicking inspect, we can see where their name is stored in the HTML file. More importantly, just before where their name is stored in text, we see something similar to ‘ Daniel Radcliffe’. Should we navigate back to Mr. Radcliffe’s page, we can see that the the element defined as ‘href’ is the same as the URL extension for the page, after the ‘https://www.themoviedb.org/’ that all pages on the TMDB site begin with. Clearly this is critical information as it will allow us to navigate to the actor or actress’s page using their URL extension. Now, we simply need to find a way to access this information for all of the actors. As we go back to the cast and credits page and look at the line where the first name was stored, we can traverse backwards to the line starting with “&lt;li data-order=”0” data-credit-id=” and then up again to the line reading ‘&lt;ol class = “people credits”&gt;’. It is important to note the information of all the cast members, including their names and the ‘href’ URL extensions we are trying to extract are all located under this ol class. We want to access this class from the HTML file in order to use it in our function, which we will be able to accomplish using css selectors. By taking a look at the first line of code, we can see that in the selector, we start in the overarching ol class, before specifying to travel down through the li class and finally asking for the attribute ‘href’ located within a. One thing to note about this line is the ‘:not(.crew)’ following the ol class. This effectively tells the program that all we do not want the attributes of any of the crew members, but only the attributes of the actors. Additionally, the .extract() command at the end of the line effectively gathers the information from the location we specified in the selector, allowing us to assign it to our variable actor_selectors. This command returns a list of the URL extensions for every actor in the movie and stores it in the actor_selectors variable. Now that we have the URL extensions, we can finish this function in a similar manner to the initial parse() function by creating new URLs for the spider to follow. As we observed before, every actor page starts with ‘https://www.themoviedb.org/’, and is followed by a tail unique to each actor which we have stored in the actor_selectors list. Now, to send our spider to each actor page, we simply write a for loop that iterates through all of the actor extensions in our actor_selectors list, add the extension to the generic URL started mentioned above, and again send the spider to the new page addressed by our new URL saved as actor_page_url. Our yield scrapy.Request() is slightly different than the one we used in the previous function as instead of calling parse_full_credits(), we are going to call parse_actor_page() as the spider is now traveling to an individual actor page.\n def parse_full_credits(self, response):\n \n        '''\n        This function navigates from the response's full credits page to each actor's individual page to be parsed in the parse_actor_page() function.\n        '''\n        \n        #creates list of cast members, excluding the crew members\n        \n        actor_selectors = response.css('ol.people.credits:not(.crew) li div.info a::attr(href)').extract()\n        \n        #for loop iterates over actor list sending spider to each actor page\n        \n        for actor_ in actor_selectors:\n        \n            actor_page_url = 'https://www.themoviedb.org'+actor_\n            \n            yield scrapy.Request(url=actor_page_url, callback=self.parse_actor_page)\nWe have now reached our third and final parsing function, parse_actor_page(self, response). The goal of this function is to identify all the movies in which the individual played an acting role and yield dictionaries that contain the individual’s name with the title of the movie they played a role in. The first thing we must do is identify the name of the actor or actress whose page we are on. This will be a similar process to finding the actor page references in the previous function, however this time there is only one piece of data we wish to extract, making it even simpler. Let us continue with our Daniel Radcliffe example from earlier by navigating back to his actor page. By highlighting his name at the top of the page and then clicking on the inspect option, we can again see where his name is stored in the HTML file associated with this page. We can see it is located as an attribute under a class titled ‘h2 class=”title”&gt;’ which we will again be able to access via css selectors. As we can see from the first line of code below, in the css selector we start at the h2 title class and then traverse to the ‘a’ section in which the name is located. Different from last time, we add the command ‘::text’ afterwards to tell the program we want the text stored at this location as opposed to the href attribute which is what we were looking for in the previous function. It is also important to note that here we are using .extract_first() instead of .extract() as we want the name itself as a string instead of a list containing the name, which is accomplished by .extract_first() which extracts the first element of the list to be returned while .extract() would grab the entire list. The .strip() at the end of the line simply ensures that there is no whitespace before or after the name to keep everything neat and consistent. We now have the name of the individual who the page belongs to stored under the variable actor_name, meaning that now we only need to find the names of the movies in which they played an acting role. We will now again implement a similar method to extract the titles of the movies the actor played a role in. If we highlight the title of the first movie or show under the ‘Acting’ section and check where it is located in the HTML file, we find it is under one of many sections labeled ’\n\n’ which are all under a table class titled ‘credits list’. While we could try and simply use the css selectors to try and locate this class as before, we must notice that there are two other classes with the same title, which would clearly make it somewhat tricky for the program to decipher exactly which one we are trying to access. Taking a closer look at these three classes, we can observe an h3 line above each one which are identical except for one word in each, the different words being ‘Acting’, ‘Production’, and ‘Crew’. Clearly we want to access the one referring to the acting roles, as we are not concerned with production and crew roles, however we need to find a way to access the correct table under acting. By taking another glance at the HTML file, we can see that the three classes of interest are all located within a div class titled ‘credits_list’. We can take advantage of this by creating a list of all the h3 terms under this list via the command defining datCred. Now that we have the list, we can simply search for which h3 contains the word ‘Acting’ to find exactly which table we want by using a for loop to iterate through each h3 element in datCred. We check for acting by checking if ‘Acting’ is in the text of the h3 element, as found in the condition of the if statement within the for loop. Now that we have identified which h3 term we want to follow, we will take advantage of xpath, another kind of selector, which has a useful function called ‘following-sibling’. This specifier tells the selector to follow a sibling class, a class with the same parent and on the same level as the initially specified node, and we have then included the xpath to ‘table[1]’ in which the tr classes containing the movie names are located, saving it to the variable txt. We then use the Selector() function to make it so that we can access just the data under the location we specified as opposed to the entire response that scrapy has found. It is worth noting that to run this command we must import the selector library from scrapy.Selector at the top of the page, with the line of code shown just below:\nfrom scrapy.selector import Selector\nFinally, we use the subset of the response to extract the movie titles for the actor. This process is shown in the final for loop. We call all elements in ‘tablePick.css(‘table.credit_group tr’)’ to access all of the tr classes individually, and then set the movOrTvName to the text under the class ‘role’ and within the ‘tooltip’ class and as the bdi attribute in the HTML file. Finally, we yield a dictionary with the actor name that we found in the beginning of the function, and the movie name that we just found. This will loop through all of the movies that the actor played a character in, resulting in all of their movies being represented in their own dictionaries to be saved in a .csv file we will create next.\ndef parse_actor_page(self, response):\n\n        '''\n        This function parses the actor page, extracting both the actor name and the movies/shows they were in, yielding the dictionaries with the actor name and the movies they appear in.\n        '''\n        \n        #extracts actor name from page\n        \n        actor_name = response.css('h2.title a::text').extract_first().strip()\n        \n        #list of data under actor acting appearances\n        \n        datCred = response.css('div.credits_list h3')\n        \n        #filtering to just acting roles\n        \n        for h3 in datCred:\n        \n            if 'Acting' in h3.xpath('./text()').get():\n            \n                txt = h3.xpath('following-sibling::table[1]').get()\n                \n                break\n                \n        #making tablePick a selector used to follow to movie data\n        \n        tablePick = Selector(text = txt)\n        \n        #extract and yield movie/show name with the actor name\n        \n        for cred in tablePick.css('table.credit_group tr'):\n        \n            movOrTvName = cred.css('td.role a.tooltip bdi::text').get().strip()\n            \n            yield{'actor': actor_name, 'movie_or_TV_name' : movOrTvName}\nNow that we have finished writing our spider, we are ready to run it in order to scrape our desired data off of the TMDB website. The first thing we need to do is go back into the settings.py file and comment out the page count limit we implemented just before creating our spider to allow our spider to scrape all of the pages it is sent to. We can now go back to the terminal and run the following command:\nscrapy crawl tmdb_spider -o results.csv -a subdir=671-harry-potter-and-the-philosopher-s-stone\nNote that the command is all one on the same line. This command will tell our spider to crawl on the website for the first Harry Potter movie as given by the subdirectory, and will write all of the results in a file named ‘results.csv’. If we go back into the project folder, we can open the file we just created and we will see dictionaries represented by a table with one column containing the actor names and the other column containing the titles of the movies that respective actor appeared in. This is where we will access the results our spider was able to produce. We can now use this data to create our recommendations.\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport os\n\n\n#checking directory\nnotebook_path = os.getcwd()\nprint(notebook_path)\n\n/Users/jakebrowning\n\n\n\n#reading in the data we scraped using our spider\ndf = pd.read_csv('/Users/jakebrowning/Desktop/PIC16B/TMDB_scraper/results.csv')\n\n\n#using for loop to create a counting function for how many actors are in each movie\nmovDict = {}\nfor el in df1:\n    if el in movDict.keys():\n        movDict[el] = movDict[el]+1\n    else:\n        movDict[el] = 1\n\n\n#adjusting my dictionaries for the pandas dataframe\nsortedMovDict = sorted(movDict.items(), key=lambda x:x[1], reverse=True)\nconvertedMovDict = dict(sortedMovDict)\n\n\n#putting the counted dictionary into a pandas dataframe and displaying the dataframe\ndfList = pd.DataFrame(list(convertedMovDict.items()),columns=['Movie Name','Number of Shared Actors'])\ndfList\n\n\n\n\n\n\n\n\nMovie Name\nNumber of Shared Actors\n\n\n\n\n0\nHarry Potter and the Philosopher's Stone\n63\n\n\n1\nHarry Potter and the Chamber of Secrets\n37\n\n\n2\nCreating the World of Harry Potter\n36\n\n\n3\nHarry Potter and the Prisoner of Azkaban\n26\n\n\n4\nHarry Potter and the Order of the Phoenix\n24\n\n\n...\n...\n...\n\n\n2269\nQueen Kong\n1\n\n\n2270\nFrankenstein: The True Story\n1\n\n\n2271\nThe Darwin Adventure\n1\n\n\n2272\nThe Canterbury Tales\n1\n\n\n2273\nThe Sooty Show\n1\n\n\n\n\n2274 rows × 2 columns\n\n\n\n\n#creating lists of the top twenty movies and their numbert of actors\nmovN = []\nnumAct = []\ni=0\nwhile i &lt; 20:\n    movN.append(dfList['Movie Name'][i])\n    numAct.append(dfList['Number of Shared Actors'][i])\n    i = 1+i\n\n\n#changing order of list to be better displayed in the graph\nmovN.reverse()\nnumAct.reverse()\n\n\n#creating bar chart with top twenty movie recommendations, displaying how many shared actors in each movie\nmyChart = plt.barh(movN, numAct)\nplt.bar_label(myChart, labels=numAct, label_type=\"edge\")\nplt.title('Top Twenty Movie Recommendations')\nplt.xlabel('Number of Shared Actors')\nplt.ylabel('Movie Title')\nplt.show()"
  },
  {
    "objectID": "posts/bruinPage/index.html",
    "href": "posts/bruinPage/index.html",
    "title": "Analysis and Recommendations of Web Scraped Movie Data",
    "section": "",
    "text": "In this blog post, I’m going to create a web scraper that will allow us to recommend new movies based on shared actors with your favorite movie.\nThe first thing we are going to do is navigate to the website called TMDB, found at the URL https://www.themoviedb.org/ . We can now pick our favorite movie that will be the basis of our recommendations, mine is Harry Potter and the Philosopher’s Stone. Next, I navigated to the main movie page found at: https://www.themoviedb.org/movie/671-harry-potter-and-the-philosopher-s-stone This URL is important to save for later as it will be the starting point for our spider. Now we can start to create our spider. The first step is to type the following into the terminal:\nconda activate PIC16B-24W\nscrapy startproject TMDB_scraper\ncd TMDB_scraper\nThis command will activate the PIC16B-24W environment that we installed previously, create a scrapy project with the name TMDB_scraper on your computer, and then change the directory of your terminal to your newly created project. As we navigate to the TMDB_scraper folder, we see that there are various files within the project, however first we want to navigate to the file titled ‘settings.py’, and add the following line:\n    CLOSESPIDER_PAGECOUNT = 20\nThis will limit the number of pages our spider will visit initially, limiting the amount of data it will acquire when we are constructing and debugging. Additionally, I wrote the line:\nUSER_AGENT = ‘Mozilla/5.0 (iPad; CPU OS 12_2 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Mobile/15E148’\nThis line helps to minimize the chance that the website identifies your spider as a bot and tries to block the web scraping process. Finally, it is time to begin writing our spider. Inside of our project folder, navigate into the spiders directory and create a new file titled ‘tmdb_spider.py’, and add the following chunk of code to the file:\nimport scrapy\nclass TmdbSpider(scrapy.Spider):\n\n    name = 'tmdb_spider'\n    \n    def __init__(self, subdir=None, *args, **kwargs):\n    \n        self.start_urls = [f\"https://www.themoviedb.org/movie/{subdir}/\"]\nThis code chunk will allow us to run the spider for our movie of choice from the terminal by giving the subdirectory on the TMDB website as our extra argument. Now that we have a working spider, it is time to implement our three parsing methods: parse(self, response), parse_full_credits(self, response), and parse_actor_page(self, response).\nLet’s start with the parse(self, response) function. The role of this function is to navigate from our main movie page to the ‘Full Cast and Crew’ page. If we go back to our main movie page, and navigate to the ‘Full Cast and Crew’ page, we can see that the only change in our URL is there is now a ‘/cast’ at the end. That tells us, all we need to do to get to our cast page is to add ‘/cast’ to the end of our current URL, and send our spider to the page found by our new URL. As we can see in the function below, we define our new URL to be called ‘full_credits_url’ which is created by taking ‘response.url’, the URL of the page we’re on, and adding the string ‘/cast’ to the end. As we now have the URL for the page we want to navigate to, we yield a scrapy.Request to send our spider to the cast page, passing our new ‘full_credits_url’ as the URL parameter, and setting the callback to the parse_full_credits() function as we will now navigate to the full credits page.\ndef parse(self, response):\n\n        '''\n        This function takes us from the main movie page to the cast and credits page yielding the url of the full credit page to be parsed by the parse_full_credits\n        '''\n        \n        #modifies url in order to navigate to the cast page\n        \n        full_credits_url = response.url + '/cast'\n        \n        #sends spider to the full credit page\n        \n        yield scrapy.Request(url=full_credits_url, callback=self.parse_full_credits)\nNow that we have made it onto the full cast and credits page, we will work with the parse_full_credits(self, response) function that was called as we navigated to this page. The goal of this function is to find all the actors who played a role in the movie and send our spider to their individual pages to be parsed by our final function parse_actor_page(). Last time we were able to navigate to the cast page by simply adding ‘/cast’, however as each actor has their own distinct name and page, we will not be able to hardcode the new URL. In fact if you click on the first actor or actress in the list, in my case Daniel Radcliffe, we see that the URL for their individual page contains a seemingly arbitrary number followed by their name. In order to find the extension for each actor, let us navigate back to the full cast and credits page. By right clicking on the top actor or actress’s name, followed by clicking inspect, we can see where their name is stored in the HTML file. More importantly, just before where their name is stored in text, we see something similar to ‘ Daniel Radcliffe’. Should we navigate back to Mr. Radcliffe’s page, we can see that the the element defined as ‘href’ is the same as the URL extension for the page, after the ‘https://www.themoviedb.org/’ that all pages on the TMDB site begin with. Clearly this is critical information as it will allow us to navigate to the actor or actress’s page using their URL extension. Now, we simply need to find a way to access this information for all of the actors. As we go back to the cast and credits page and look at the line where the first name was stored, we can traverse backwards to the line starting with “&lt;li data-order=”0” data-credit-id=” and then up again to the line reading ‘&lt;ol class = “people credits”&gt;’. It is important to note the information of all the cast members, including their names and the ‘href’ URL extensions we are trying to extract are all located under this ol class. We want to access this class from the HTML file in order to use it in our function, which we will be able to accomplish using css selectors. By taking a look at the first line of code, we can see that in the selector, we start in the overarching ol class, before specifying to travel down through the li class and finally asking for the attribute ‘href’ located within a. One thing to note about this line is the ‘:not(.crew)’ following the ol class. This effectively tells the program that all we do not want the attributes of any of the crew members, but only the attributes of the actors. Additionally, the .extract() command at the end of the line effectively gathers the information from the location we specified in the selector, allowing us to assign it to our variable actor_selectors. This command returns a list of the URL extensions for every actor in the movie and stores it in the actor_selectors variable. Now that we have the URL extensions, we can finish this function in a similar manner to the initial parse() function by creating new URLs for the spider to follow. As we observed before, every actor page starts with ‘https://www.themoviedb.org/’, and is followed by a tail unique to each actor which we have stored in the actor_selectors list. Now, to send our spider to each actor page, we simply write a for loop that iterates through all of the actor extensions in our actor_selectors list, add the extension to the generic URL started mentioned above, and again send the spider to the new page addressed by our new URL saved as actor_page_url. Our yield scrapy.Request() is slightly different than the one we used in the previous function as instead of calling parse_full_credits(), we are going to call parse_actor_page() as the spider is now traveling to an individual actor page.\n def parse_full_credits(self, response):\n \n        '''\n        This function navigates from the response's full credits page to each actor's individual page to be parsed in the parse_actor_page() function.\n        '''\n        \n        #creates list of cast members, excluding the crew members\n        \n        actor_selectors = response.css('ol.people.credits:not(.crew) li div.info a::attr(href)').extract()\n        \n        #for loop iterates over actor list sending spider to each actor page\n        \n        for actor_ in actor_selectors:\n        \n            actor_page_url = 'https://www.themoviedb.org'+actor_\n            \n            yield scrapy.Request(url=actor_page_url, callback=self.parse_actor_page)\nWe have now reached our third and final parsing function, parse_actor_page(self, response). The goal of this function is to identify all the movies in which the individual played an acting role and yield dictionaries that contain the individual’s name with the title of the movie they played a role in. The first thing we must do is identify the name of the actor or actress whose page we are on. This will be a similar process to finding the actor page references in the previous function, however this time there is only one piece of data we wish to extract, making it even simpler. Let us continue with our Daniel Radcliffe example from earlier by navigating back to his actor page. By highlighting his name at the top of the page and then clicking on the inspect option, we can again see where his name is stored in the HTML file associated with this page. We can see it is located as an attribute under a class titled ‘h2 class=”title”&gt;’ which we will again be able to access via css selectors. As we can see from the first line of code below, in the css selector we start at the h2 title class and then traverse to the ‘a’ section in which the name is located. Different from last time, we add the command ‘::text’ afterwards to tell the program we want the text stored at this location as opposed to the href attribute which is what we were looking for in the previous function. It is also important to note that here we are using .extract_first() instead of .extract() as we want the name itself as a string instead of a list containing the name, which is accomplished by .extract_first() which extracts the first element of the list to be returned while .extract() would grab the entire list. The .strip() at the end of the line simply ensures that there is no whitespace before or after the name to keep everything neat and consistent. We now have the name of the individual who the page belongs to stored under the variable actor_name, meaning that now we only need to find the names of the movies in which they played an acting role. We will now again implement a similar method to extract the titles of the movies the actor played a role in. If we highlight the title of the first movie or show under the ‘Acting’ section and check where it is located in the HTML file, we find it is under one of many sections labeled ’\n\n’ which are all under a table class titled ‘credits list’. While we could try and simply use the css selectors to try and locate this class as before, we must notice that there are two other classes with the same title, which would clearly make it somewhat tricky for the program to decipher exactly which one we are trying to access. Taking a closer look at these three classes, we can observe an h3 line above each one which are identical except for one word in each, the different words being ‘Acting’, ‘Production’, and ‘Crew’. Clearly we want to access the one referring to the acting roles, as we are not concerned with production and crew roles, however we need to find a way to access the correct table under acting. By taking another glance at the HTML file, we can see that the three classes of interest are all located within a div class titled ‘credits_list’. We can take advantage of this by creating a list of all the h3 terms under this list via the command defining datCred. Now that we have the list, we can simply search for which h3 contains the word ‘Acting’ to find exactly which table we want by using a for loop to iterate through each h3 element in datCred. We check for acting by checking if ‘Acting’ is in the text of the h3 element, as found in the condition of the if statement within the for loop. Now that we have identified which h3 term we want to follow, we will take advantage of xpath, another kind of selector, which has a useful function called ‘following-sibling’. This specifier tells the selector to follow a sibling class, a class with the same parent and on the same level as the initially specified node, and we have then included the xpath to ‘table[1]’ in which the tr classes containing the movie names are located, saving it to the variable txt. We then use the Selector() function to make it so that we can access just the data under the location we specified as opposed to the entire response that scrapy has found. It is worth noting that to run this command we must import the selector library from scrapy.Selector at the top of the page, with the line of code shown just below:\nfrom scrapy.selector import Selector\nFinally, we use the subset of the response to extract the movie titles for the actor. This process is shown in the final for loop. We call all elements in ‘tablePick.css(‘table.credit_group tr’)’ to access all of the tr classes individually, and then set the movOrTvName to the text under the class ‘role’ and within the ‘tooltip’ class and as the bdi attribute in the HTML file. Finally, we yield a dictionary with the actor name that we found in the beginning of the function, and the movie name that we just found. This will loop through all of the movies that the actor played a character in, resulting in all of their movies being represented in their own dictionaries to be saved in a .csv file we will create next.\ndef parse_actor_page(self, response):\n\n        '''\n        This function parses the actor page, extracting both the actor name and the movies/shows they were in, yielding the dictionaries with the actor name and the movies they appear in.\n        '''\n        \n        #extracts actor name from page\n        \n        actor_name = response.css('h2.title a::text').extract_first().strip()\n        \n        #list of data under actor acting appearances\n        \n        datCred = response.css('div.credits_list h3')\n        \n        #filtering to just acting roles\n        \n        for h3 in datCred:\n        \n            if 'Acting' in h3.xpath('./text()').get():\n            \n                txt = h3.xpath('following-sibling::table[1]').get()\n                \n                break\n                \n        #making tablePick a selector used to follow to movie data\n        \n        tablePick = Selector(text = txt)\n        \n        #extract and yield movie/show name with the actor name\n        \n        for cred in tablePick.css('table.credit_group tr'):\n        \n            movOrTvName = cred.css('td.role a.tooltip bdi::text').get().strip()\n            \n            yield{'actor': actor_name, 'movie_or_TV_name' : movOrTvName}\nNow that we have finished writing our spider, we are ready to run it in order to scrape our desired data off of the TMDB website. The first thing we need to do is go back into the settings.py file and comment out the page count limit we implemented just before creating our spider to allow our spider to scrape all of the pages it is sent to. We can now go back to the terminal and run the following command:\nscrapy crawl tmdb_spider -o results.csv -a subdir=671-harry-potter-and-the-philosopher-s-stone\nNote that the command is all one on the same line. This command will tell our spider to crawl on the website for the first Harry Potter movie as given by the subdirectory, and will write all of the results in a file named ‘results.csv’. If we go back into the project folder, we can open the file we just created and we will see dictionaries represented by a table with one column containing the actor names and the other column containing the titles of the movies that respective actor appeared in. This is where we will access the results our spider was able to produce. We can now use this data to create our recommendations.\nOur first step in creating our recommendations is importing our necessary packages and reading in the data we just acquired and saved into results.csv, which can be done via the following code:\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n#reading in the data we scraped using our spider\ndf = pd.read_csv('/Users/jakebrowning/Desktop/PIC16B/TMDB_scraper/results.csv')\n#using for loop to create a counting function for how many actors are in each movie\nmovDict = {}\nfor el in df1:\n    if el in movDict.keys():\n        movDict[el] = movDict[el]+1\n    else:\n        movDict[el] = 1\n\n#adjusting my dictionaries for the pandas dataframe\nsortedMovDict = sorted(movDict.items(), key=lambda x:x[1], reverse=True)\nconvertedMovDict = dict(sortedMovDict)\n\n#putting the counted dictionary into a pandas dataframe and displaying the dataframe\ndfList = pd.DataFrame(list(convertedMovDict.items()),columns=['Movie Name','Number of Shared Actors'])\ndfList\nWe can see this yields a list of movies containing shared actors, ordered from most shared actors to least shared actors with our original movie.\n\n```python #creating lists of the top twenty movies and their numbert of actors movN = [] numAct = [] i=0 while i &lt; 20: movN.append(dfList[‘Movie Name’][i]) numAct.append(dfList[‘Number of Shared Actors’][i]) i = 1+i\n#changing order of list to be better displayed in the graph movN.reverse() numAct.reverse()\n#creating bar chart with top twenty movie recommendations, displaying how many shared actors in each movie myChart = plt.barh(movN, numAct) plt.bar_label(myChart, labels=numAct, label_type=“edge”) plt.title(‘Top Twenty Movie Recommendations’) plt.xlabel(‘Number of Shared Actors’) plt.ylabel(‘Movie Title’) plt.show() ’’’\nThis leaves us with our top twenty reccommendations, shown in decreasing order by the bar chart displayed below."
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code."
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/flaskHW/index.html",
    "href": "posts/flaskHW/index.html",
    "title": "Homework 3: Flask",
    "section": "",
    "text": "https://github.com/jakebrowning02/PICHHW3 https://jakebrowning02.github.io/PIC16BHW/posts/flaskHW/\nWe will start the tutorial with the init.py file. The first function we see in this file is create_app(). The function effectively the creation of the app. As we can see, in the first line, we will set the app to be an instance of the Flask class that we imported at the top of the page. Next we begin to configure the app by defining where the database is to be located, by mapping the database to the ‘flaskBlog.sqlite’ file. If you are wishing to use a different file name for your database, that is fine, but just be sure to use that file name here instead of the one that I used. It is important to note, that for this we must import os at the top of the page. The function os.path.join shows the computer how to reach the file we have specified. Continuing downward, we see a hello() function. This function is not critical to this project, but is useful in checking that the app is working correctly and also happens to be an easier wsay to explain how these functions work. Above the function we see the command ‘@app.route(’/hello’)’ which is critical to this function. This ties the url route ‘/hello’ to this function, a practice we will use pervasively throughout this project. This page simply prints out ‘Hello, World!’, but other pages will have more complex applications. The following commands are all imports that we will revisit later after we have defined the functions that they are referring to. Let’s move over to the db.py file. This file is responsible for defining most of the basic commands necessary for setting up our database. The first thing we want to do is to import sqlite3. This allows us to make an sqlite3 database on this page. We will also import click for a later function, but more importantly, we import both current_app and g from the flask library. g is a special object that is able to be accessed by multiple functions, which is also unique for each request. Similarly, current_app is another special object that points to the Flask application handling the request. We will ensure that current_app will be able to be used by importing this file into the other .py files. The first function we see is get_db(). We first want to check that there is no database currently in our special instance g. If there is not, logically we want to begin to access the database. We define the database connection by creating an attribute db of g, and setting it to sqlite3.connect(), effectively making it a cursor we can use to search the database in later functions. We then return this attribute so that when we import it into other files, we can effectively use this as our cursor. The next function close_db() is rather self explanatory. We need a way to close the connection to the database if the database is open. We check if it is open by writing the if statement ‘if db is not None:’. This way, we only attempt to close the connection if it exists, which we do within the if statement by calling db.close(). We now come across our function that is called when we are intiating the database, init_db(). Using the function we just defined, we call get_db() to create a cursor, and using that we open the schematic that we write for the database, defining the tables, rows, columns, and titles to be held within the database. The next function gives us a way to call the function we just created. We call @click.command(‘init-db’) so that we can call the function using a command line command. Now, when we call this function via the command, we can see that it simply calls the init_db() function and recieve a message letting us know that we have successfully initialized the database. Finally, we come to the init_app() function. This function will be used to ensure that the cursor is closed when it needs to be. In order to ensure that the cursor is closed each time after returning a response, we will use the function teardown_appcontext(). This calls whatever function is within the parentheses whenever a response is returned, so we choose to call close_db() whenever a response is returned. Finally, we want to travel back to init.py and look at what we imported from this page. By importing db.init_app(app), we allow ourselves to initiate the database within the init.py file. Let us now take a look at our last .py file, blog.py. This file will contain most of what we see on the web app. We will import several useful methods from flask up top to be able to run our code. Taking a look at the first function index(), we see it appears to have a similar formate to the hello() function we saw before. Using the @bp.route(‘/’), we define the URL extension to be the backslash to access this page. First thing we do in this function is define db to be the cursor we get from the get_db() function we created in the previous page. Now, we will use this cursor to access the database. We now select both the author and body attributes from every post in the database, and set this list to be the list posts. This will be used in the html to show all of the posts that are contained within our database. This is useful for debugging and being able to view the database in a user friendly manner. Now we reach the return line of this function by calling render_template() we tell the computer to run the html file template we have created for this page called ‘index.html’. In my case, this file is contained within a blog folder, hence the ‘/blog’ before the ‘index.html’. We also choose to pass the posts list to the template as it is used to display the posts on this page. Continuing on, we get to the function used to insert new messages into the database. We see the function is associated with the URL extension ‘/submit’, however this time we also define the methods(‘GET’, ‘POST’), as any user request will be considered a ‘POST’. We check to make sure that this is the case via the if statement at the beginning of the function. Now we need to get input from the user so that we can input their name and their message. We do so by using the request.form[] function, allowing us to use the input to define the variables. We then check that the boxes have been filled, and send an error if they have not been. If everything is working well, we then use the get_db() function to access the cursor again and use it to insert the new message with its author into the database. We ensure to call db.commit() to save the new information into the database and call for a redirect. This function sends us back to the main page by calling the index function in the blog.py file, or in other words the function above this one. We also make sure to render the template for this page so that the html file we write will appear on this page. The random_messages function again uses the cursor we acquire from get_db(). We then use a special command for the cursor ‘RANDOM()’ to select the rows from the database in a random order. We use the variable n to determine how many rows are to be selected, and then set this to the list messages to be returned by this function. The view function is associated with its own URL and is the page used to determine how many random posts the user withes to see. We use request.form.get() to take in the user input. Then again, assuming no errors, we call random_messages function with the user inputted amount of function and pass it to our final function randomview(). This final function is associated with its own URL and gets random messages from the random_messages function. We call int(request.args.get(‘num_messages’)) to get the amount of messages desired by the user, a value passed by the previous page. Finally, we will take a look at our template for the new post page. First, we extend ‘base.html’ so that we can keep what we coded in that. We now define our header under the {%block header%} and set the block title to ‘New Messages’. Next, we get to the block content. We create a label titled ‘Author’ which is used for our input. This title is placed with the input text box that sends its input to the value request.form[‘author’] which we use to input the value into the database. We then do the same for the message, however this time we define a textarea to create a bigger box for the user to input their message. Finally, we create a submit button that says ‘Submit message’ for the user to click after they input their desired information."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "16bhwblog",
    "section": "",
    "text": "Homework 3: Flask\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nFeb 14, 2024\n\n\nJake Browning\n\n\n\n\n\n\n\n\n\n\n\n\nPost With Code\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\nFeb 11, 2024\n\n\nHarlow Malloc\n\n\n\n\n\n\n\n\n\n\n\n\nAnalysis and Recommendations of Web Scraped Movie Data\n\n\n\n\n\n\nweek 6\n\n\n\n\n\n\n\n\n\nFeb 11, 2024\n\n\nJake Browning\n\n\n\n\n\n\n\n\n\n\n\n\nAnalysis and Recommendations of Web Scraped Movie Data\n\n\n\n\n\n\nweek 6\n\n\n\n\n\n\n\n\n\nFeb 11, 2024\n\n\nJake Browning\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nFeb 8, 2024\n\n\nTristan O’Malley\n\n\n\n\n\n\nNo matching items"
  }
]